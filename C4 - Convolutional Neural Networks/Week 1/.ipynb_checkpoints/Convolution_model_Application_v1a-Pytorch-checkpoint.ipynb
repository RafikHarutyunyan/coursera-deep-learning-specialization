{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks: Application\n",
    "\n",
    "Welcome to Course 4's second assignment! In this notebook, you will:\n",
    "\n",
    "- Implement helper functions that you will use when implementing a TensorFlow model\n",
    "- Implement a fully functioning ConvNet using TensorFlow \n",
    "\n",
    "**After this assignment you will be able to:**\n",
    "\n",
    "- Build and train a ConvNet in TensorFlow for a classification problem \n",
    "\n",
    "We assume here that you are already familiar with TensorFlow. If you are not, please refer the *TensorFlow Tutorial* of the third week of Course 2 (\"*Improving deep neural networks*\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkblue'> Updates to Assignment <font>\n",
    "\n",
    "#### If you were working on a previous version\n",
    "* The current notebook filename is version \"1a\". \n",
    "* You can find your work in the file directory as version \"1\".\n",
    "* To view the file directory, go to the menu \"File->Open\", and this will open a new tab that shows the file directory.\n",
    "\n",
    "#### List of Updates\n",
    "* `initialize_parameters`: added details about tf.get_variable, `eval`. Clarified test case.\n",
    "* Added explanations for the kernel (filter) stride values, max pooling, and flatten functions.\n",
    "* Added details about softmax cross entropy with logits.\n",
    "* Added instructions for creating the Adam Optimizer.\n",
    "* Added explanation of how to evaluate tensors (optimizer and cost).\n",
    "* `forward_propagation`: clarified instructions, use \"F\" to store \"flatten\" layer.\n",
    "* Updated print statements and 'expected output' for easier visual comparisons.\n",
    "* Many thanks to Kevin P. Brown (mentor for the deep learning specialization) for his suggestions on the assignments in this course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 - TensorFlow model\n",
    "\n",
    "In the previous assignment, you built helper functions using numpy to understand the mechanics behind convolutional neural networks. Most practical applications of deep learning today are built using programming frameworks, which have many built-in functions you can simply call. \n",
    "\n",
    "As usual, we will start by loading in the packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from cnn_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to load the \"SIGNS\" dataset you are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (signs)\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the SIGNS dataset is a collection of 6 signs representing numbers from 0 to 5.\n",
    "\n",
    "<img src=\"images/SIGNS.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "The next cell will show you an example of a labelled image in the dataset. Feel free to change the value of `index` below and re-run to see different examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 6\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Course 2, you had built a fully-connected network for this dataset. But since this is an image dataset, it is more natural to apply a ConvNet to it.\n",
    "\n",
    "To get started, let's examine the shapes of your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (1080, 64, 64, 3)\n",
      "Y_train shape: (1080, 6)\n",
      "X_test shape: (120, 64, 64, 3)\n",
      "Y_test shape: (120, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6).T\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))\n",
    "conv_layers = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Create placeholders\n",
    "\n",
    "TensorFlow requires that you create placeholders for the input data that will be fed into the model when running the session.\n",
    "\n",
    "**Exercise**: Implement the function below to create placeholders for the input image X and the output Y. You should not define the number of training examples for the moment. To do so, you could use \"None\" as the batch size, it will give you the flexibility to choose it later. Hence X should be of dimension **[None, n_H0, n_W0, n_C0]** and Y should be of dimension **[None, n_y]**.  [Hint: search for the tf.placeholder documentation\"](https://www.tensorflow.org/api_docs/python/tf/placeholder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GRADED FUNCTION: create_placeholders\n",
    "\n",
    "# def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "#     \"\"\"\n",
    "#     Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "#     Arguments:\n",
    "#     n_H0 -- scalar, height of an input image\n",
    "#     n_W0 -- scalar, width of an input image\n",
    "#     n_C0 -- scalar, number of channels of the input\n",
    "#     n_y -- scalar, number of classes\n",
    "        \n",
    "#     Returns:\n",
    "#     X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n",
    "#     Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "#     \"\"\"\n",
    "\n",
    "#     ### START CODE HERE ### (â‰ˆ2 lines)\n",
    "#     X = tf.placeholder(shape=[None, n_H0, n_W0, n_C0], dtype=\"float\", name=\"X\")\n",
    "#     Y = tf.placeholder(shape=[None, n_y], dtype=\"float\", name=\"Y\")\n",
    "#     ### END CODE HERE ###\n",
    "    \n",
    "#     return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_placeholders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b900d089d9ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_placeholders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"X = \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Y = \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_placeholders' is not defined"
     ]
    }
   ],
   "source": [
    "X, Y = create_placeholders(64, 64, 3, 6)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "<table> \n",
    "<tr>\n",
    "<td>\n",
    "    X = Tensor(\"Placeholder:0\", shape=(?, 64, 64, 3), dtype=float32)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "    Y = Tensor(\"Placeholder_1:0\", shape=(?, 6), dtype=float32)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Initialize parameters\n",
    "\n",
    "You will initialize weights/filters $W1$ and $W2$ using `tf.contrib.layers.xavier_initializer(seed = 0)`. You don't need to worry about bias variables as you will soon see that TensorFlow functions take care of the bias. Note also that you will only initialize the weights/filters for the conv2d functions. TensorFlow initializes the layers for the fully connected part automatically. We will talk more about that later in this assignment.\n",
    "\n",
    "**Exercise:** Implement initialize_parameters(). The dimensions for each group of filters are provided below. Reminder - to initialize a parameter $W$ of shape [1,2,3,4] in Tensorflow, use:\n",
    "```python\n",
    "W = tf.get_variable(\"W\", [1,2,3,4], initializer = ...)\n",
    "```\n",
    "#### tf.get_variable()\n",
    "[Search for the tf.get_variable documentation](https://www.tensorflow.org/api_docs/python/tf/get_variable).  Notice that the documentation says:\n",
    "```\n",
    "Gets an existing variable with these parameters or create a new one.\n",
    "```\n",
    "So we can use this function to create a tensorflow variable with the specified name, but if the variables already exist, it will get the existing variable with that same name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [4, 4, 3, 8]\n",
    "                        W2 : [2, 2, 8, 16]\n",
    "    Note that we will hard code the shape values in the function to make the grading simpler.\n",
    "    Normally, functions should take values as inputs rather than hard coding.\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.random.set_seed(1)                              # so that your \"random\" numbers match ours\n",
    "        \n",
    "    ### START CODE HERE ### (approx. 2 lines of code)\n",
    "    \n",
    "    initializer = tf.initializers.GlorotUniform()\n",
    "    W1 = tf.Variable(initializer(shape=(4, 4, 3, 8)), )\n",
    "    W2 = tf.Variable(initializer(shape=(2, 2, 8, 16)))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1[1,1,1] = \n",
      "tf.Tensor(\n",
      "[ 0.13869302 -0.08964861  0.09414922  0.12346362 -0.0073135  -0.06831413\n",
      "  0.09115724 -0.12548038], shape=(8,), dtype=float32)\n",
      "W1.shape: (4, 4, 3, 8)\n",
      "\n",
      "\n",
      "W2[1,1,1] = \n",
      "tf.Tensor(\n",
      "[-0.1450851  -0.06108421  0.03728181 -0.11014593  0.15256226 -0.04861301\n",
      " -0.08096564 -0.23541981  0.14797115  0.07797277  0.15634406  0.17394447\n",
      " -0.17914408 -0.12279058 -0.14740777  0.11102837], shape=(16,), dtype=float32)\n",
      "W2.shape: (2, 2, 8, 16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters = initialize_parameters()\n",
    "\n",
    "print(\"W1[1,1,1] = \\n\" + str(parameters[\"W1\"][1,1,1]))\n",
    "print(\"W1.shape: \" + str(parameters[\"W1\"].shape))\n",
    "print(\"\\n\")\n",
    "print(\"W2[1,1,1] = \\n\" + str(parameters[\"W2\"][1,1,1]))\n",
    "print(\"W2.shape: \" + str(parameters[\"W2\"].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output:**\n",
    "\n",
    "```\n",
    "W1[1,1,1] = \n",
    "[ 0.00131723  0.14176141 -0.04434952  0.09197326  0.14984085 -0.03514394\n",
    " -0.06847463  0.05245192]\n",
    "W1.shape: (4, 4, 3, 8)\n",
    "\n",
    "\n",
    "W2[1,1,1] = \n",
    "[-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058\n",
    " -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228\n",
    " -0.22779644 -0.1601823  -0.16117483 -0.10286498]\n",
    "W2.shape: (2, 2, 8, 16)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Forward propagation\n",
    "\n",
    "In TensorFlow, there are built-in functions that implement the convolution steps for you.\n",
    "\n",
    "- **tf.nn.conv2d(X,W, strides = [1,s,s,1], padding = 'SAME'):** given an input $X$ and a group of filters $W$, this function convolves $W$'s filters on X. The third parameter ([1,s,s,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev). Normally, you'll choose a stride of 1 for the number of examples (the first value) and for the channels (the fourth value), which is why we wrote the value as `[1,s,s,1]`. You can read the full documentation on [conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d).\n",
    "\n",
    "- **tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = 'SAME'):** given an input A, this function uses a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window.  For max pooling, we usually operate on a single example at a time and a single channel at a time.  So the first and fourth value in `[1,f,f,1]` are both 1.  You can read the full documentation on [max_pool](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool).\n",
    "\n",
    "- **tf.nn.relu(Z):** computes the elementwise ReLU of Z (which can be any shape). You can read the full documentation on [relu](https://www.tensorflow.org/api_docs/python/tf/nn/relu).\n",
    "\n",
    "- **tf.contrib.layers.flatten(P)**: given a tensor \"P\", this function takes each training (or test) example in the batch and flattens it into a 1D vector.  \n",
    "    * If a tensor P has the shape (m,h,w,c), where m is the number of examples (the batch size), it returns a flattened tensor with shape (batch_size, k), where $k=h \\times w \\times c$.  \"k\" equals the product of all the dimension sizes other than the first dimension.\n",
    "    * For example, given a tensor with dimensions [100,2,3,4], it flattens the tensor to be of shape [100, 24], where 24 = 2 * 3 * 4.  You can read the full documentation on [flatten](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten).\n",
    "\n",
    "- **tf.contrib.layers.fully_connected(F, num_outputs):** given the flattened input F, it returns the output computed using a fully connected layer. You can read the full documentation on [full_connected](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected).\n",
    "\n",
    "In the last function above (`tf.contrib.layers.fully_connected`), the fully connected layer automatically initializes weights in the graph and keeps on training them as you train the model. Hence, you did not need to initialize those weights when initializing the parameters.\n",
    "\n",
    "\n",
    "#### Window, kernel, filter\n",
    "The words \"window\", \"kernel\", and \"filter\" are used to refer to the same thing.  This is why the parameter `ksize` refers to \"kernel size\", and we use `(f,f)` to refer to the filter size.  Both \"kernel\" and \"filter\" refer to the \"window.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Implement the `forward_propagation` function below to build the following model: `CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED`. You should use the functions above. \n",
    "\n",
    "In detail, we will use the following parameters for all the steps:\n",
    " - Conv2D: stride 1, padding is \"SAME\"\n",
    " - ReLU\n",
    " - Max pool: Use an 8 by 8 filter size and an 8 by 8 stride, padding is \"SAME\"\n",
    " - Conv2D: stride 1, padding is \"SAME\"\n",
    " - ReLU\n",
    " - Max pool: Use a 4 by 4 filter size and a 4 by 4 stride, padding is \"SAME\"\n",
    " - Flatten the previous output.\n",
    " - FULLYCONNECTED (FC) layer: Apply a fully connected layer without an non-linear activation function. Do not call the softmax here. This will result in 6 neurons in the output layer, which then get passed later to a softmax. In TensorFlow, the softmax and cost function are lumped together into a single function, which you'll call in a different function when computing the cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Note that for simplicity and grading purposes, we'll hard-code some values\n",
    "    such as the stride and kernel (filter) sizes. \n",
    "    Normally, functions should take these values as function parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # CONV2D: stride of 1, padding 'SAME'\n",
    "    \n",
    "    Z1 = tf.nn.conv2d(X, W1, strides = [1,1,1,1], padding = 'SAME')\n",
    "    \n",
    "    # RELU\n",
    "    \n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    \n",
    "    # MAXPOOL: window 8x8, stride 8, padding 'SAME'\n",
    "    \n",
    "    P1 = tf.nn.max_pool(A1, ksize = [1,8,8,1], strides = [1,8,8,1], padding = 'SAME')\n",
    "   \n",
    "    # CONV2D: filters W2, stride 1, padding 'SAME'\n",
    "    \n",
    "    Z2 = tf.nn.conv2d(P1, W2, strides = [1,1,1,1], padding = 'SAME')\n",
    "   \n",
    "    # RELU\n",
    "    \n",
    "    A2 = tf.nn.relu(Z2)\n",
    "  \n",
    "    # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n",
    "    \n",
    "    P2 = tf.nn.max_pool(A2, ksize = [1,4,4,1], strides = [1,4,4,1], padding = 'SAME')\n",
    "    \n",
    "    # FLATTEN\n",
    "    \n",
    "    Z3 = tf.compat.v1.layers.flatten(P2)\n",
    "   \n",
    "    # FULLY-CONNECTED without non-linear activation function (not not call softmax).\n",
    "    # 6 neurons in output layer. Hint: one of the arguments should be \"activation_fn=None\" \n",
    "    Z3 = tf.compat.v1.layers.dense(Z3, 6, activation = None)\n",
    "   \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z3 = \n",
      "tf.Tensor(\n",
      "[[ 0.23551293 -0.02307653 -0.15652823  1.9143677  -0.63153493 -2.7213254 ]\n",
      " [-0.08194128 -0.04931334  0.03292936  1.6985412  -0.2843245  -2.7015092 ]], shape=(2, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "X = np.random.randn(2,64,64,3)\n",
    "Y = np.random.randn(2,6)\n",
    "\n",
    "parameters = initialize_parameters()\n",
    "Z3 = forward_propagation(X, parameters)\n",
    "\n",
    "print(\"Z3 = \\n\" + str(Z3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```\n",
    "Z3 = \n",
    "[[-0.44670227 -1.57208765 -1.53049231 -2.31013036 -1.29104376  0.46852064]\n",
    " [-0.17601591 -1.57972014 -1.4737016  -2.61672091 -1.00810647  0.5747785 ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Compute cost\n",
    "\n",
    "Implement the compute cost function below. Remember that the cost function helps the neural network see how much the model's predictions differ from the correct labels.  By adjusting the weights of the network to reduce the cost, the neural network can improve its predictions.\n",
    "\n",
    "You might find these two functions helpful: \n",
    "\n",
    "- **tf.nn.softmax_cross_entropy_with_logits(logits = Z, labels = Y):** computes the softmax entropy loss. This function both computes the softmax activation function as well as the resulting loss. You can check the full documentation  [softmax_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits).\n",
    "- **tf.reduce_mean:** computes the mean of elements across dimensions of a tensor. Use this to calculate the sum of the losses over all the examples to get the overall cost. You can check the full documentation [reduce_mean](https://www.tensorflow.org/api_docs/python/tf/reduce_mean).\n",
    "\n",
    "#### Details on softmax_cross_entropy_with_logits (optional reading)\n",
    "* Softmax is used to format outputs so that they can be used for classification.  It assigns a value between 0 and 1 for each category, where the sum of all prediction values (across all possible categories) equals 1.\n",
    "* Cross Entropy is compares the model's predicted classifications with the actual labels and results in a numerical value representing the \"loss\" of the model's predictions.\n",
    "* \"Logits\" are the result of multiplying the weights and adding the biases.  Logits are passed through an activation function (such as a relu), and the result is called the \"activation.\"\n",
    "* The function is named `softmax_cross_entropy_with_logits` takes logits as input (and not activations); then uses the model to predict using softmax, and then compares the predictions with the true labels using cross entropy.  These are done with a single function to optimize the calculations.\n",
    "\n",
    "** Exercise**: Compute the cost below using the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost \n",
    "\n",
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (number of examples, 6)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = Y, logits = Z3))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = tf.Tensor(-0.13187718, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "X = np.random.randn(4,64,64,3)\n",
    "Y = np.random.randn(4,6)\n",
    "\n",
    "parameters = initialize_parameters()\n",
    "Z3 = forward_propagation(X, parameters)\n",
    "cost = compute_cost(Z3, Y)\n",
    "\n",
    "print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "```\n",
    "cost = 2.91034\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Model \n",
    "\n",
    "Finally you will merge the helper functions you implemented above to build a model. You will train it on the SIGNS dataset. \n",
    "\n",
    "**Exercise**: Complete the function below. \n",
    "\n",
    "The model below should:\n",
    "\n",
    "- create placeholders\n",
    "- initialize parameters\n",
    "- forward propagate\n",
    "- compute the cost\n",
    "- create an optimizer\n",
    "\n",
    "Finally you will create a session and run a for loop  for num_epochs, get the mini-batches, and then for each mini-batch you will optimize the function. [Hint for initializing the variables](https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam Optimizer\n",
    "You can use `tf.train.AdamOptimizer(learning_rate = ...)` to create the optimizer.  The optimizer has a `minimize(loss=...)` function that you'll call to set the cost function that the optimizer will minimize.\n",
    "\n",
    "For details, check out the documentation for [Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random mini batches\n",
    "If you took course 2 of the deep learning specialization, you implemented `random_mini_batches()` in the \"Optimization\" programming assignment. This function returns a list of mini-batches. It is already implemented in the `cnn_utils.py` file and imported here, so you can call it like this:\n",
    "```Python\n",
    "minibatches = random_mini_batches(X, Y, mini_batch_size = 64, seed = 0)\n",
    "```\n",
    "(You will want to choose the correct variable names when you use it in your code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the optimizer and cost\n",
    "\n",
    "Within a loop, for each mini-batch, you'll use the `tf.Session` object (named `sess`) to feed a mini-batch of inputs and labels into the neural network and evaluate the tensors for the optimizer as well as the cost.  Remember that we built a graph data structure and need to feed it inputs and labels and use `sess.run()` in order to get values for the optimizer and cost.\n",
    "\n",
    "You'll use this kind of syntax:\n",
    "```\n",
    "output_for_var1, output_for_var2 = sess.run(\n",
    "                                                fetches=[var1, var2],\n",
    "                                                feed_dict={var_inputs: the_batch_of_inputs,\n",
    "                                                           var_labels: the_batch_of_labels}\n",
    "                                                )\n",
    "```\n",
    "* Notice that `sess.run` takes its first argument `fetches` as a list of objects that you want it to evaluate (in this case, we want to evaluate the optimizer and the cost).  \n",
    "* It also takes a dictionary for the `feed_dict` parameter.  \n",
    "* The keys are the `tf.placeholder` variables that we created in the `create_placeholders` function above.  \n",
    "* The values are the variables holding the actual numpy arrays for each mini-batch.  \n",
    "* The sess.run outputs a tuple of the evaluated tensors, in the same order as the list given to `fetches`. \n",
    "\n",
    "For more information on how to use sess.run, see the documentation [tf.Sesssion#run](https://www.tensorflow.org/api_docs/python/tf/Session#run) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(inputs,targets,parameters):\n",
    "    with tf.GradientTape() as tape:\n",
    "    # calculate the loss\n",
    "        Z3 = forward_propagation(inputs, parameters)\n",
    "        loss_value = compute_cost(Z3, targets)\n",
    "  \n",
    "  # return gradient\n",
    "    return [tape.gradient(loss_value, list(parameters.values())), loss_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost \n",
    "\n",
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(inputs,targets,parameters):\n",
    "    with tf.GradientTape() as tape:\n",
    "    # calculate the loss\n",
    "        Z3=forward_propagation(inputs, parameters)\n",
    "        loss_value = compute_cost(Z3, targets)\n",
    "  \n",
    "  # return gradient\n",
    "    return [tape.gradient(loss_value, list(parameters.values())),loss_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.009,\n",
    "          num_epochs = 100, minibatch_size = 64, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer ConvNet in Tensorflow:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_train -- test set, of shape (None, n_y = 6)\n",
    "    X_test -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_test -- test set, of shape (None, n_y = 6)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    train_accuracy -- real number, accuracy on the train set (X_train)\n",
    "    test_accuracy -- real number, testing accuracy on the test set (X_test)\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.random.set_seed(1)                             # to keep results consistent (tensorflow seed)\n",
    "    seed = 3                                          # to keep results consistent (numpy seed)\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape             \n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    \n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Do the training loop\n",
    "#     for epoch in range(num_epochs):\n",
    "\n",
    "#         epoch_cost = 0.\n",
    "#         num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "#         seed = seed + 1\n",
    "#         minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "#         for minibatch in minibatches:\n",
    "\n",
    "#             # Select a minibatch\n",
    "#             (minibatch_X, minibatch_Y) = minibatch\n",
    "#             X = minibatch_X.astype(np.float32)\n",
    "#             Y = minibatch_Y.astype(np.float32)\n",
    "#             \"\"\"\n",
    "#             # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "#             # Run the session to execute the optimizer and the cost.\n",
    "#             # The feedict should contain a minibatch for (X,Y).\n",
    "#             \"\"\"\n",
    "#             ### START CODE HERE ### (1 line)\n",
    "            \n",
    "# #             Z3 = forward_propagation(X, parameters)\n",
    "# #             cost = compute_cost(Z3, Y)\n",
    "# #             opt_op = optimizer.minimize(cost, var_list=list(parameters.values()))\n",
    "# #             opt_op.run()\n",
    "            \n",
    "#             # Optimizer\n",
    "#             with tf.GradientTape() as tape:\n",
    "#             # calculate the loss\n",
    "#                 Z3 = forward_propagation(X, parameters)\n",
    "#                 loss_value = compute_cost(Z3, Y)\n",
    "  \n",
    "#                 grads = tape.gradient(loss_value, list(parameters.values()))\n",
    "#                 minibatch_cost = loss_value\n",
    "   \n",
    "#             optimizer.apply_gradients(zip(grads, list(parameters.values())))\n",
    "\n",
    "#             ### END CODE HERE ###\n",
    "\n",
    "#             epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "\n",
    "#         # Print the cost every epoch\n",
    "#         if print_cost == True and epoch % 5 == 0:\n",
    "#             print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "#         if print_cost == True and epoch % 1 == 0:\n",
    "#             costs.append(epoch_cost)\n",
    "        \n",
    "        \n",
    "#     # plot the cost\n",
    "#     plt.plot(np.squeeze(costs))\n",
    "#     plt.ylabel('cost')\n",
    "#     plt.xlabel('iterations (per tens)')\n",
    "#     plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "#     plt.show()\n",
    "\n",
    "#     # Calculate the correct predictions\n",
    "#     predict_op = tf.argmax(Z3, 1)\n",
    "#     correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "\n",
    "#     # Calculate accuracy on the test set\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "#     print(accuracy)\n",
    "\n",
    "#     Z3 = forward_propagation(X_train, parameters)\n",
    "#     predict_op = tf.argmax(Z3, 1)\n",
    "#     correct_prediction = tf.equal(predict_op, tf.argmax(Y_train, 1))\n",
    "#     train_accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "#     Z3 = forward_propagation(X_test, parameters)\n",
    "#     predict_op = tf.argmax(Z3, 1)\n",
    "#     correct_prediction = tf.equal(predict_op, tf.argmax(Y_test, 1))\n",
    "#     test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "#     print(\"Train Accuracy:\", train_accuracy)\n",
    "#     print(\"Test Accuracy:\", test_accuracy)\n",
    "                \n",
    "#     return train_accuracy, test_accuracy, parameters\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                X=minibatch_X.astype(np.float32)\n",
    "                Y=minibatch_Y.astype(np.float32)\n",
    "                grads, minibatch_cost = get_grad(X, Y, parameters)\n",
    "                optimizer.apply_gradients(zip(grads, list(parameters.values())))\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "              # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "                \n",
    "                Z3_train = forward_propagation(X_train, parameters)\n",
    "                correct_prediction = tf.equal(np.argmax(Z3_train, axis = 1), tf.argmax(Y_train, axis = 1))\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                print(\"Train: \", accuracy)\n",
    "                \n",
    "                Z3_test = forward_propagation(X_test, parameters)\n",
    "                correct_prediction = tf.equal(tf.argmax(Z3_test, axis = 1), tf.argmax(Y_test, axis = 1))\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                print(\"Test: \", accuracy)\n",
    "                \n",
    "                \n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "\n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per fives)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    # lets save the parameters in a variable\n",
    "    print (\"Parameters have been trained!\")\n",
    "\n",
    "    Z3_train = forward_propagation(X_train, parameters)\n",
    "    correct_prediction = tf.equal(tf.argmax(Z3_test), tf.argmax(Y_train))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Test: \", accuracy)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Z3_test = forward_propagation(X_test, parameters)\n",
    "    correct_prediction = tf.equal(tf.argmax(Z3_test), tf.argmax(Y_test))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Test: \", accuracy)\n",
    "\n",
    "#     print (\"Train Accuracy:\", accuracy.eval({Z3, Y_train}))\n",
    "#     print (\"Test Accuracy:\", accuracy.eval({X_test, Y_test}))\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to train your model for 100 epochs. Check if your cost after epoch 0 and 5 matches our output. If not, stop the cell and go back to your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 46.750107\n",
      "Train:  tf.Tensor(0.16666667, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.16666667, shape=(), dtype=float32)\n",
      "Cost after epoch 5: 46.817223\n",
      "Train:  tf.Tensor(0.16666667, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.18333334, shape=(), dtype=float32)\n",
      "Cost after epoch 10: 46.841007\n",
      "Train:  tf.Tensor(0.16666667, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.16666667, shape=(), dtype=float32)\n",
      "Cost after epoch 15: 46.688118\n",
      "Train:  tf.Tensor(0.16666667, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.16666667, shape=(), dtype=float32)\n",
      "Cost after epoch 20: 46.698021\n",
      "Train:  tf.Tensor(0.16666667, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.125, shape=(), dtype=float32)\n",
      "Cost after epoch 25: 46.716133\n",
      "Train:  tf.Tensor(0.11296296, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.108333334, shape=(), dtype=float32)\n",
      "Cost after epoch 30: 46.705353\n",
      "Train:  tf.Tensor(0.1537037, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.20833333, shape=(), dtype=float32)\n",
      "Cost after epoch 35: 46.707905\n",
      "Train:  tf.Tensor(0.10925926, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.1, shape=(), dtype=float32)\n",
      "Cost after epoch 40: 46.713158\n",
      "Train:  tf.Tensor(0.1425926, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.15, shape=(), dtype=float32)\n",
      "Cost after epoch 45: 46.710430\n",
      "Train:  tf.Tensor(0.12314815, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.15, shape=(), dtype=float32)\n",
      "Cost after epoch 50: 46.709194\n",
      "Train:  tf.Tensor(0.13425925, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.15833333, shape=(), dtype=float32)\n",
      "Cost after epoch 55: 46.710007\n",
      "Train:  tf.Tensor(0.14537036, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.15, shape=(), dtype=float32)\n",
      "Cost after epoch 60: 46.709446\n",
      "Train:  tf.Tensor(0.14444445, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.15, shape=(), dtype=float32)\n",
      "Cost after epoch 65: 46.709152\n",
      "Train:  tf.Tensor(0.15833333, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.14166667, shape=(), dtype=float32)\n",
      "Cost after epoch 70: 46.709724\n",
      "Train:  tf.Tensor(0.1675926, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.15833333, shape=(), dtype=float32)\n",
      "Cost after epoch 75: 46.709557\n",
      "Train:  tf.Tensor(0.15462963, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.15833333, shape=(), dtype=float32)\n",
      "Cost after epoch 80: 46.709766\n",
      "Train:  tf.Tensor(0.1574074, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.15833333, shape=(), dtype=float32)\n",
      "Cost after epoch 85: 46.709522\n",
      "Train:  tf.Tensor(0.1574074, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.16666667, shape=(), dtype=float32)\n",
      "Cost after epoch 90: 46.709427\n",
      "Train:  tf.Tensor(0.1574074, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.15833333, shape=(), dtype=float32)\n",
      "Cost after epoch 95: 46.709591\n",
      "Train:  tf.Tensor(0.15462963, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.16666667, shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Test:  tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "Test:  tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, X_test, Y_test, learning_rate = 0.009,\n",
    "          num_epochs = 100, minibatch_size = 64, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(inputs,targets,parameters):\n",
    "    with tf.GradientTape() as tape:\n",
    "    # calculate the loss\n",
    "        Z3=forward_propagation(inputs, parameters)\n",
    "        loss_value = compute_cost(Z3, targets)\n",
    "  \n",
    "  # return gradient\n",
    "    return [tape.gradient(loss_value, list(parameters.values())),loss_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads, loss = get_grad(X_train, Y_train, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=3>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.apply_gradients(zip(grads, list(parameters.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(4, 4, 3, 8) dtype=float32, numpy=\n",
       " array([[[[-1.23658627e-01,  1.75254926e-01,  4.83654141e-02,\n",
       "           -2.41704434e-02, -7.68316090e-02,  5.26223779e-02,\n",
       "            1.48695841e-01, -2.39661038e-02],\n",
       "          [ 5.91215342e-02,  6.57308251e-02,  5.04545271e-02,\n",
       "            4.22630161e-02,  1.43771455e-01,  4.71791476e-02,\n",
       "           -1.51913604e-02, -1.75044149e-01],\n",
       "          [-2.18333304e-02, -6.43371493e-02,  1.42622367e-01,\n",
       "            1.43017575e-01,  1.06085762e-01, -1.62646219e-01,\n",
       "           -1.85382515e-01, -7.07475618e-02]],\n",
       " \n",
       "         [[-9.18818861e-02,  1.77835554e-01, -1.05318427e-02,\n",
       "           -9.51305851e-02,  4.91149575e-02,  3.17690820e-02,\n",
       "            1.24407083e-01,  2.59022713e-02],\n",
       "          [-1.31662190e-03,  6.16760217e-02,  1.53001845e-02,\n",
       "           -2.10792124e-02, -7.66316205e-02,  8.63897949e-02,\n",
       "            1.27986372e-01,  6.22295886e-02],\n",
       "          [-1.04838662e-01, -8.87599438e-02, -3.42808217e-02,\n",
       "           -1.81069285e-01, -1.27109587e-02, -7.52631873e-02,\n",
       "            6.53646588e-02,  8.08598101e-03]],\n",
       " \n",
       "         [[ 5.06406426e-02,  5.06621860e-02,  2.61338651e-02,\n",
       "           -1.75214723e-01,  4.11807597e-02, -6.99200556e-02,\n",
       "           -3.11369896e-02, -5.00223041e-03],\n",
       "          [-1.63026512e-01,  1.73837915e-01, -6.05756864e-02,\n",
       "            1.54638544e-01,  5.53062409e-02,  1.58439979e-01,\n",
       "           -5.59359714e-02, -1.57574162e-01],\n",
       "          [ 9.77093428e-02,  3.45230773e-02, -1.24948621e-01,\n",
       "           -7.85065144e-02,  6.71739429e-02, -8.87942091e-02,\n",
       "           -1.77002370e-01, -1.59683824e-02]],\n",
       " \n",
       "         [[ 1.13024488e-01,  6.92025572e-02,  7.92694837e-02,\n",
       "            8.02269131e-02, -1.13886990e-01,  6.46219105e-02,\n",
       "            1.00560062e-01, -7.67127499e-02],\n",
       "          [-1.50510788e-01,  8.06650072e-02, -7.58177191e-02,\n",
       "            1.45543292e-01,  9.07062143e-02, -1.08764447e-01,\n",
       "            7.83076808e-02,  1.49426118e-01],\n",
       "          [-1.03878781e-01,  7.82113522e-02, -8.14339966e-02,\n",
       "           -1.50040329e-01, -1.36363417e-01, -1.25828415e-01,\n",
       "           -1.53908089e-01,  1.27230659e-01]]],\n",
       " \n",
       " \n",
       "        [[[ 2.08675563e-02,  3.83451395e-02, -1.23970121e-01,\n",
       "           -1.24406457e-01, -1.63279921e-02, -5.55873215e-02,\n",
       "            3.07854787e-02,  1.55403152e-01],\n",
       "          [-1.44970492e-01, -1.36569828e-01, -8.14419687e-02,\n",
       "           -1.60219252e-01, -1.39654562e-01, -1.55370682e-02,\n",
       "           -9.64163616e-02, -1.29094124e-02],\n",
       "          [ 5.65391779e-02,  8.46446231e-02, -1.76929384e-02,\n",
       "            1.46368250e-01, -6.27951771e-02, -1.03381343e-01,\n",
       "           -1.21049628e-01, -4.96765822e-02]],\n",
       " \n",
       "         [[-1.68520018e-01, -5.67592420e-02, -1.11121327e-01,\n",
       "           -6.08499795e-02, -8.15434754e-03, -8.69610459e-02,\n",
       "           -9.52252969e-02, -3.27294916e-02],\n",
       "          [ 1.38693020e-01, -6.26534224e-02,  9.41492170e-02,\n",
       "            1.23463616e-01, -7.31350482e-03, -6.83141276e-02,\n",
       "            6.41579926e-02, -1.25480384e-01],\n",
       "          [-1.49081796e-02,  1.09525330e-01, -9.34885591e-02,\n",
       "           -1.87750161e-02,  8.12485069e-02, -1.62501425e-01,\n",
       "            2.17728410e-03, -1.68449372e-01]],\n",
       " \n",
       "         [[ 1.31407365e-01, -1.47623643e-01, -6.52787909e-02,\n",
       "            1.23693898e-01,  7.86621422e-02, -6.04662076e-02,\n",
       "            9.34104621e-02, -1.71767980e-01],\n",
       "          [-1.65244907e-01,  8.08285847e-02, -1.43332779e-03,\n",
       "            1.71177790e-01,  9.04017240e-02,  1.02613792e-01,\n",
       "           -5.40596470e-02,  1.67233363e-01],\n",
       "          [ 1.59300312e-01,  1.36495054e-01, -1.42682537e-01,\n",
       "           -2.70159245e-02, -2.74126828e-02,  1.37590989e-01,\n",
       "            1.52320275e-02, -1.78916156e-02]],\n",
       " \n",
       "         [[-2.32855380e-02, -3.63177657e-02,  8.88221711e-02,\n",
       "            2.07915753e-02,  3.90654206e-02,  1.54213622e-01,\n",
       "            3.32528502e-02, -1.13749288e-01],\n",
       "          [-1.80674031e-01, -1.85423344e-02, -9.28968340e-02,\n",
       "            9.80359763e-02, -1.03885822e-01,  2.65919566e-02,\n",
       "            4.39704098e-02, -6.90252408e-02],\n",
       "          [ 8.24746937e-02, -6.83808327e-03, -1.73589915e-01,\n",
       "           -1.40066817e-01, -6.64445534e-02, -2.02597231e-02,\n",
       "           -1.70498222e-01, -4.57756221e-02]]],\n",
       " \n",
       " \n",
       "        [[[-9.91331041e-02,  1.87945306e-01, -2.22167969e-02,\n",
       "            2.96750516e-02, -1.46245956e-01, -4.53683734e-03,\n",
       "           -1.50366453e-02,  1.51778013e-02],\n",
       "          [ 1.72121540e-01,  8.58931690e-02, -6.01467863e-02,\n",
       "            3.78918648e-02, -2.01254040e-02, -8.71153846e-02,\n",
       "           -7.57838115e-02, -1.11003399e-01],\n",
       "          [ 1.89052820e-02, -1.45322114e-01, -9.56338793e-02,\n",
       "            8.86020511e-02, -5.25764674e-02,  7.05777258e-02,\n",
       "            2.42956262e-03, -8.48460197e-03]],\n",
       " \n",
       "         [[-1.76713154e-01,  2.89458744e-02, -2.87978500e-02,\n",
       "            1.75682887e-01,  5.23497164e-03, -1.01314075e-01,\n",
       "           -2.11479187e-01,  1.21660307e-01],\n",
       "          [ 1.76762506e-01,  1.97596356e-01, -1.73250198e-01,\n",
       "            1.06165186e-01, -8.60705450e-02, -3.37259918e-02,\n",
       "            1.34743348e-01,  3.95435393e-02],\n",
       "          [-1.20893419e-02,  9.50637534e-02, -7.15309158e-02,\n",
       "            1.68116435e-01,  6.07897192e-02, -3.16888243e-02,\n",
       "            2.53183991e-02, -1.38363242e-01]],\n",
       " \n",
       "         [[ 2.94848830e-02,  5.38828745e-02,  4.54851240e-02,\n",
       "           -1.14896215e-01,  1.41278759e-01, -1.77987739e-01,\n",
       "            6.72061648e-03,  1.39595568e-03],\n",
       "          [-1.40399754e-01,  1.25073493e-01, -1.38240471e-01,\n",
       "            1.75841376e-01, -4.05461043e-02,  4.57963347e-03,\n",
       "            3.94941792e-02, -1.27488226e-01],\n",
       "          [-7.50880241e-02,  1.15603402e-01,  1.31420001e-01,\n",
       "           -1.80000469e-01,  8.44073445e-02, -2.95532793e-02,\n",
       "           -2.08080947e-01,  9.78346914e-02]],\n",
       " \n",
       "         [[ 5.35043329e-02, -4.55412567e-02, -6.70069456e-03,\n",
       "            1.63291246e-02,  1.63815632e-01,  1.22444332e-02,\n",
       "            1.31992400e-01, -4.10570651e-02],\n",
       "          [ 1.72168612e-02, -1.36353612e-01,  1.40228644e-01,\n",
       "           -7.36374930e-02, -1.26193836e-01, -1.64975286e-01,\n",
       "            1.10456996e-01,  5.28431833e-02],\n",
       "          [-2.87591517e-02, -6.47321418e-02, -1.58121824e-01,\n",
       "            1.18133202e-01,  1.08110204e-01,  1.01093873e-01,\n",
       "            1.21580243e-01, -1.74608469e-01]]],\n",
       " \n",
       " \n",
       "        [[[-8.05653334e-02, -1.01050653e-01,  4.47354913e-02,\n",
       "            7.98255056e-02,  5.15595824e-02, -4.15845662e-02,\n",
       "            8.63754004e-02, -1.13214478e-01],\n",
       "          [-9.23403651e-02, -1.53125241e-01, -1.45219162e-01,\n",
       "            1.09800026e-01,  3.72182578e-02, -1.81132019e-01,\n",
       "           -1.74323797e-01, -4.21936810e-02],\n",
       "          [-1.56475455e-01,  5.63997254e-02,  1.80916771e-01,\n",
       "           -1.25513136e-01, -1.17847025e-01,  8.28834623e-02,\n",
       "            6.23443630e-03, -1.17106281e-01]],\n",
       " \n",
       "         [[-6.58287033e-02,  2.57587135e-02, -2.34884769e-02,\n",
       "           -7.51939416e-02, -1.63731724e-01,  3.62694710e-02,\n",
       "           -1.52214646e-01,  1.18659571e-01],\n",
       "          [-6.07924014e-02,  1.67567834e-01, -7.66196474e-02,\n",
       "            1.09516069e-01, -1.15080349e-01, -4.29658592e-02,\n",
       "           -1.57397687e-01,  2.57469714e-02],\n",
       "          [-1.26591206e-01,  1.77538127e-01,  1.45747408e-01,\n",
       "           -6.36392310e-02,  1.54361352e-01,  1.47459462e-01,\n",
       "           -1.83151677e-01,  4.81338799e-03]],\n",
       " \n",
       "         [[-1.54734075e-01, -1.73921436e-01, -5.88580370e-02,\n",
       "           -1.63900897e-01, -9.92855951e-02, -1.61357939e-01,\n",
       "            2.37002913e-02, -3.14481705e-02],\n",
       "          [ 1.44595787e-01,  1.21376261e-01, -1.19567581e-01,\n",
       "            1.58821538e-01, -1.82885289e-01, -1.82822347e-04,\n",
       "           -8.44416991e-02, -8.78080130e-02],\n",
       "          [-1.33191317e-01, -4.34535742e-03,  3.14934552e-03,\n",
       "           -8.46896544e-02,  8.16270858e-02, -4.14075106e-02,\n",
       "            2.64702626e-02, -1.53149709e-01]],\n",
       " \n",
       "         [[-1.40444040e-02,  6.09723181e-02,  5.31375110e-02,\n",
       "           -9.66669619e-03, -4.41865027e-02, -7.78475255e-02,\n",
       "            3.45723983e-03,  1.77778110e-01],\n",
       "          [-7.55748078e-02, -1.20452665e-01,  9.22677368e-02,\n",
       "            1.51449755e-01, -4.99403924e-02,  1.44038394e-01,\n",
       "           -2.03579605e-01,  1.24304399e-01],\n",
       "          [-3.00752819e-02, -6.76330328e-02, -1.82054877e-01,\n",
       "            1.17300138e-01,  3.57500315e-02, -1.12459078e-01,\n",
       "            1.48688674e-01, -1.64799288e-01]]]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(2, 2, 8, 16) dtype=float32, numpy=\n",
       " array([[[[ 5.05352020e-03, -2.82341242e-02, -4.57334518e-02,\n",
       "            2.46246159e-01,  9.43319798e-02, -7.70751834e-02,\n",
       "           -3.19665074e-02,  5.05304933e-02, -2.16878653e-02,\n",
       "            1.26348972e-01, -1.56001389e-01,  2.43784785e-02,\n",
       "            2.44576335e-02,  2.47635245e-02, -1.10052645e-01,\n",
       "           -1.95521772e-01],\n",
       "          [ 6.54556751e-02,  1.60078794e-01,  1.61315203e-02,\n",
       "            3.05495858e-02, -1.03032887e-01,  2.41208449e-01,\n",
       "           -1.95131898e-01,  2.04820007e-01,  1.19904719e-01,\n",
       "           -1.09251738e-01,  1.74002707e-01, -2.00994849e-01,\n",
       "            2.11147591e-01,  1.39710844e-01,  2.07300246e-01,\n",
       "            1.09595150e-01],\n",
       "          [-1.71915889e-01,  2.21413255e-01,  1.52281284e-01,\n",
       "            2.40632653e-01,  2.41668403e-01, -2.28683710e-01,\n",
       "           -3.69323492e-02,  3.05510759e-02, -5.82277775e-02,\n",
       "           -1.51659250e-01, -2.27850676e-03, -6.57517910e-02,\n",
       "           -1.38259530e-01,  7.26819634e-02, -1.43334508e-01,\n",
       "            1.53128266e-01],\n",
       "          [-2.23184705e-01, -7.63617754e-02,  1.92691684e-02,\n",
       "            1.36917651e-01,  1.91953778e-02,  2.83011794e-02,\n",
       "           -2.31930315e-01,  1.07665896e-01, -2.21687019e-01,\n",
       "           -1.95843697e-01,  7.89468288e-02, -1.31421864e-01,\n",
       "            1.49955869e-01,  4.68430519e-02, -1.18810952e-01,\n",
       "            7.51233697e-02],\n",
       "          [ 1.51286125e-02,  2.12906718e-01,  2.38206625e-01,\n",
       "           -5.55210114e-02, -4.52614427e-02,  1.20399594e-01,\n",
       "            6.68191910e-03, -1.22659743e-01, -2.02272117e-01,\n",
       "           -2.48100579e-01,  1.91702008e-01,  2.11593688e-01,\n",
       "            1.46039486e-01, -2.02293277e-01,  2.22432971e-01,\n",
       "            2.44883776e-01],\n",
       "          [ 2.15218663e-01, -2.46145308e-01,  9.35330987e-02,\n",
       "           -4.30258512e-02, -1.41821265e-01, -2.15009034e-01,\n",
       "           -1.96089387e-01, -3.79861593e-02,  1.17738247e-02,\n",
       "            2.92422771e-02, -1.42355204e-01,  5.50354719e-02,\n",
       "            1.88209713e-01,  5.14103770e-02, -6.48424029e-02,\n",
       "            1.00986719e-01],\n",
       "          [-9.93555784e-02, -6.47897124e-02, -2.43731916e-01,\n",
       "           -1.13449907e-02, -2.37156987e-01,  1.71300769e-02,\n",
       "            1.64078712e-01, -5.13586998e-02, -1.98060215e-01,\n",
       "           -2.41620898e-01,  9.74292755e-02, -2.23112226e-01,\n",
       "            1.51733324e-01,  9.07576680e-02, -2.19459176e-01,\n",
       "            1.62150204e-01],\n",
       "          [ 1.32115185e-01,  1.14163339e-01, -3.12335491e-02,\n",
       "           -7.48533010e-02, -6.83641434e-03,  1.27008080e-01,\n",
       "            5.14078736e-02, -4.90324497e-02,  9.04514194e-02,\n",
       "            1.77545667e-01, -1.68778419e-01,  3.05094719e-02,\n",
       "           -1.08114064e-01, -2.08503783e-01,  2.57843733e-02,\n",
       "            7.62059689e-02]],\n",
       " \n",
       "         [[-2.41044879e-01, -4.86564040e-02,  4.41948771e-02,\n",
       "            4.12873030e-02,  4.18158770e-02,  4.22586203e-02,\n",
       "           -8.54525566e-02,  1.46657348e-01, -1.17061317e-01,\n",
       "            2.36956179e-01,  4.48417068e-02,  1.12980485e-01,\n",
       "            1.48996413e-01, -2.07015395e-01, -9.20772552e-04,\n",
       "           -9.41104889e-02],\n",
       "          [ 1.87226921e-01,  1.40324950e-01,  1.78947046e-01,\n",
       "            1.75683424e-02,  9.55212116e-03,  1.03378475e-01,\n",
       "            1.00852251e-02,  2.35266566e-01, -1.95110679e-01,\n",
       "           -2.22851694e-01,  1.42531231e-01,  2.34175503e-01,\n",
       "           -1.84425890e-01, -1.02821231e-01,  8.74039531e-02,\n",
       "           -6.54193386e-02],\n",
       "          [ 7.40358829e-02,  6.08536005e-02,  5.22809029e-02,\n",
       "            1.56285107e-01, -5.49710989e-02, -2.06361890e-01,\n",
       "           -9.68451500e-02, -1.06478393e-01,  9.38093662e-02,\n",
       "            2.20124722e-02,  3.58008146e-02,  2.06709504e-02,\n",
       "           -2.02750146e-01,  8.84079933e-03,  1.16147101e-01,\n",
       "           -1.76018417e-01],\n",
       "          [-1.27151906e-01, -1.65687025e-01,  1.70618296e-03,\n",
       "            1.94360256e-01, -2.20826626e-01,  1.85610950e-01,\n",
       "           -1.82583153e-01, -2.65693665e-03,  8.94447565e-02,\n",
       "            8.95336270e-02, -4.66791987e-02, -2.13830709e-01,\n",
       "            1.43119097e-02, -2.48826921e-01,  2.26536155e-01,\n",
       "           -1.21492445e-01],\n",
       "          [ 1.23372793e-01,  1.30504906e-01, -2.03677297e-01,\n",
       "            2.23778665e-01,  3.55941057e-02,  8.78807306e-02,\n",
       "           -5.75137138e-02, -1.24817967e-01,  2.14111209e-02,\n",
       "           -2.13646710e-01,  2.02298880e-01, -2.45184720e-01,\n",
       "            4.73074913e-02, -6.62093759e-02, -1.53994858e-01,\n",
       "           -7.07846284e-02],\n",
       "          [ 1.23257816e-01, -1.70562029e-01,  4.34517860e-05,\n",
       "            2.19719410e-01,  1.85375571e-01,  6.23543262e-02,\n",
       "           -9.95135307e-02, -2.30823934e-01,  1.99583769e-02,\n",
       "            5.22490144e-02, -1.58288479e-01, -1.81270182e-01,\n",
       "            2.34488249e-02,  2.25232244e-02, -1.43206537e-01,\n",
       "           -1.36153340e-01],\n",
       "          [ 1.42705321e-01,  1.58986256e-01,  8.46373737e-02,\n",
       "           -1.60056144e-01, -1.04963779e-03, -5.40693998e-02,\n",
       "           -2.36076355e-01,  2.11422861e-01,  1.61902249e-01,\n",
       "           -4.89473343e-04, -8.86379406e-02, -1.78661808e-01,\n",
       "            2.39152908e-01, -9.26647186e-02,  1.83483556e-01,\n",
       "           -1.63479149e-01],\n",
       "          [ 1.54236495e-01, -1.55047059e-01, -2.59995461e-03,\n",
       "           -2.32581973e-01, -1.51406586e-01,  1.98506534e-01,\n",
       "           -2.44821310e-02, -9.57707167e-02,  1.28605604e-01,\n",
       "            7.43154883e-02, -1.50916278e-01, -2.37387896e-01,\n",
       "            1.50996864e-01,  6.41807914e-02, -1.32746696e-01,\n",
       "            1.32410645e-01]]],\n",
       " \n",
       " \n",
       "        [[[-1.87439442e-01, -2.39485025e-01,  2.42884755e-01,\n",
       "            1.52527571e-01,  1.85016274e-01,  1.76138639e-01,\n",
       "           -1.58321917e-01, -1.89227879e-01,  3.84318829e-03,\n",
       "            1.45022392e-01,  2.13860095e-01,  8.38621259e-02,\n",
       "           -1.70862913e-01, -6.86157942e-02, -7.11745024e-02,\n",
       "           -1.84372663e-01],\n",
       "          [-3.93628478e-02,  1.93043470e-01, -1.72675669e-01,\n",
       "            1.38191581e-02, -2.39810467e-01,  8.93502831e-02,\n",
       "            1.54982597e-01, -1.22145116e-01, -2.35746384e-01,\n",
       "            8.83739591e-02, -2.97194719e-02,  5.02557158e-02,\n",
       "           -1.62798882e-01, -7.28592873e-02,  4.61556353e-02,\n",
       "            8.24111700e-03],\n",
       "          [-1.84557855e-01, -5.73416948e-02, -2.30463266e-01,\n",
       "            4.03143764e-02,  2.16012418e-01, -2.47978568e-02,\n",
       "            2.46700883e-01, -7.15987682e-02, -9.60797071e-03,\n",
       "            9.96648073e-02, -7.25134015e-02, -1.71195030e-01,\n",
       "           -2.26120591e-01, -1.60835087e-01,  1.94969594e-01,\n",
       "            6.50064349e-02],\n",
       "          [ 2.07989216e-01, -1.79841042e-01, -1.03249550e-01,\n",
       "            1.87203467e-01,  1.27428770e-01, -1.07585728e-01,\n",
       "           -1.14607453e-01,  1.70578241e-01,  1.47204876e-01,\n",
       "           -2.08014846e-02,  2.34689713e-02,  2.61832476e-02,\n",
       "            1.63933218e-01, -8.22494030e-02,  2.00684071e-02,\n",
       "           -1.44543946e-01],\n",
       "          [-4.39238548e-03,  2.23175287e-01,  4.00583148e-02,\n",
       "           -1.64965570e-01,  1.82334185e-01,  1.46684587e-01,\n",
       "            1.41156256e-01, -1.62814856e-01, -2.01606989e-01,\n",
       "           -1.62871122e-01,  1.94141030e-01, -1.21149600e-01,\n",
       "            1.64196491e-01, -2.26042032e-01, -4.22583818e-02,\n",
       "            2.49638855e-01],\n",
       "          [-1.91519916e-01, -1.26612723e-01, -8.85379314e-03,\n",
       "            2.58072019e-02,  1.42814815e-01,  6.41459823e-02,\n",
       "            1.68236494e-01,  1.61178589e-01, -2.00762928e-01,\n",
       "            9.88225341e-02, -2.35099435e-01, -2.00223923e-01,\n",
       "            2.24004924e-01,  5.34591675e-02,  1.95738614e-01,\n",
       "           -2.22766578e-01],\n",
       "          [-2.08316863e-01, -1.58648372e-01,  2.07429379e-01,\n",
       "           -9.02663618e-02, -9.40968394e-02,  1.61407933e-01,\n",
       "            7.33608007e-03, -9.82683897e-03,  1.91362634e-01,\n",
       "           -3.61662507e-02, -3.15315686e-02, -1.64670125e-01,\n",
       "            1.58128127e-01, -8.44821930e-02,  1.53750524e-01,\n",
       "            1.60946682e-01],\n",
       "          [ 8.15699100e-02, -2.02509105e-01,  2.45927930e-01,\n",
       "           -1.14269495e-01,  1.16389036e-01,  1.76601946e-01,\n",
       "           -3.23727131e-02, -2.18175650e-01, -2.39458799e-01,\n",
       "            5.01464009e-02, -6.20825887e-02,  7.06199408e-02,\n",
       "           -4.38957214e-02,  1.62235200e-01,  3.78850102e-02,\n",
       "           -2.07246304e-01]],\n",
       " \n",
       "         [[-2.06474900e-01,  1.77957892e-01,  1.30720973e-01,\n",
       "            6.01405501e-02,  7.87937641e-03, -9.65774655e-02,\n",
       "           -2.23747313e-01,  2.49454618e-01,  8.74667168e-02,\n",
       "            1.07961297e-01,  5.01120687e-02,  1.39211833e-01,\n",
       "           -1.70528471e-01,  1.83217943e-01,  7.59577751e-03,\n",
       "            1.61166668e-01],\n",
       "          [-1.45085096e-01, -6.10842109e-02,  3.72818112e-02,\n",
       "           -1.10145926e-01,  1.52562261e-01, -4.86130118e-02,\n",
       "           -8.09656382e-02, -2.35419810e-01,  1.47971153e-01,\n",
       "            7.79727697e-02,  1.83308616e-01,  1.47012666e-01,\n",
       "           -1.79144084e-01, -1.22790575e-01, -1.47407770e-01,\n",
       "            1.37997761e-01],\n",
       "          [ 1.26223266e-01, -1.40649676e-01,  1.91504538e-01,\n",
       "            3.82817388e-02, -2.40605831e-01, -5.13162613e-02,\n",
       "            5.43687940e-02, -7.04206228e-02, -1.05452955e-01,\n",
       "           -1.19928658e-01, -2.28370309e-01,  2.10808516e-01,\n",
       "           -7.95696378e-02, -1.98505402e-01, -1.29750609e-01,\n",
       "            1.57378972e-01],\n",
       "          [ 1.18072033e-01,  7.96847343e-02, -2.43353426e-01,\n",
       "            1.70029998e-02,  3.47715020e-02, -6.52845502e-02,\n",
       "            1.12124443e-01, -1.60753727e-03,  2.43852139e-02,\n",
       "            1.53947651e-01,  1.14241064e-01,  2.01808572e-01,\n",
       "            2.16398299e-01,  4.91023660e-02,  8.76885653e-03,\n",
       "           -3.24679017e-02],\n",
       "          [-1.86848581e-01, -2.02338934e-01,  3.28962207e-02,\n",
       "            2.37900019e-01, -4.12617326e-02,  1.30795240e-01,\n",
       "           -2.38362372e-01,  4.15473580e-02,  1.96844935e-02,\n",
       "           -1.94219232e-01,  1.18811548e-01, -1.88700795e-01,\n",
       "            8.31390023e-02, -1.40160799e-01,  1.25277936e-01,\n",
       "           -1.48778021e-01],\n",
       "          [-7.20190406e-02, -1.06125176e-01, -1.85240686e-01,\n",
       "           -1.08102381e-01,  2.12299526e-01,  2.13495433e-01,\n",
       "           -8.26811790e-02,  4.69897389e-02, -1.77855968e-01,\n",
       "           -1.02210104e-01, -7.62937069e-02,  1.43164039e-01,\n",
       "           -2.41635919e-01,  5.13969660e-02,  1.81885898e-01,\n",
       "           -7.41513968e-02],\n",
       "          [ 2.46111646e-01, -2.07152367e-01, -6.85171485e-02,\n",
       "           -2.40706742e-01,  3.25697660e-02, -1.49049342e-01,\n",
       "            1.02628052e-01,  4.70194221e-02, -7.67720342e-02,\n",
       "            1.00304127e-01,  1.52968377e-01, -2.13966548e-01,\n",
       "            9.66972709e-02, -3.67438197e-02, -2.06869245e-01,\n",
       "            3.88133079e-02],\n",
       "          [ 1.77097201e-01,  1.96233511e-01,  1.51748538e-01,\n",
       "            1.69725418e-02,  2.11882412e-01, -1.16778612e-02,\n",
       "           -2.17716694e-01,  1.36195898e-01, -7.52553344e-02,\n",
       "            2.28503406e-01,  8.28370452e-02, -1.22151971e-01,\n",
       "           -2.64889002e-03, -2.22547770e-01,  1.81061506e-01,\n",
       "            8.08202624e-02]]]], dtype=float32)>]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(parameters.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': <tf.Variable 'Variable:0' shape=(4, 4, 3, 8) dtype=float32, numpy=\n",
       " array([[[[-1.23658627e-01,  1.75254926e-01,  4.83654141e-02,\n",
       "           -2.41704434e-02, -7.68316090e-02,  5.26223779e-02,\n",
       "            1.48695841e-01, -2.39661038e-02],\n",
       "          [ 5.91215342e-02,  6.57308251e-02,  5.04545271e-02,\n",
       "            4.22630161e-02,  1.43771455e-01,  4.71791476e-02,\n",
       "           -1.51913604e-02, -1.75044149e-01],\n",
       "          [-2.18333304e-02, -6.43371493e-02,  1.42622367e-01,\n",
       "            1.43017575e-01,  1.06085762e-01, -1.62646219e-01,\n",
       "           -1.85382515e-01, -7.07475618e-02]],\n",
       " \n",
       "         [[-9.18818861e-02,  1.77835554e-01, -1.05318427e-02,\n",
       "           -9.51305851e-02,  4.91149575e-02,  3.17690820e-02,\n",
       "            1.24407083e-01,  2.59022713e-02],\n",
       "          [-1.31662190e-03,  6.16760217e-02,  1.53001845e-02,\n",
       "           -2.10792124e-02, -7.66316205e-02,  8.63897949e-02,\n",
       "            1.27986372e-01,  6.22295886e-02],\n",
       "          [-1.04838662e-01, -8.87599438e-02, -3.42808217e-02,\n",
       "           -1.81069285e-01, -1.27109587e-02, -7.52631873e-02,\n",
       "            6.53646588e-02,  8.08598101e-03]],\n",
       " \n",
       "         [[ 5.06406426e-02,  5.06621860e-02,  2.61338651e-02,\n",
       "           -1.75214723e-01,  4.11807597e-02, -6.99200556e-02,\n",
       "           -3.11369896e-02, -5.00223041e-03],\n",
       "          [-1.63026512e-01,  1.73837915e-01, -6.05756864e-02,\n",
       "            1.54638544e-01,  5.53062409e-02,  1.58439979e-01,\n",
       "           -5.59359714e-02, -1.57574162e-01],\n",
       "          [ 9.77093428e-02,  3.45230773e-02, -1.24948621e-01,\n",
       "           -7.85065144e-02,  6.71739429e-02, -8.87942091e-02,\n",
       "           -1.77002370e-01, -1.59683824e-02]],\n",
       " \n",
       "         [[ 1.13024488e-01,  6.92025572e-02,  7.92694837e-02,\n",
       "            8.02269131e-02, -1.13886990e-01,  6.46219105e-02,\n",
       "            1.00560062e-01, -7.67127499e-02],\n",
       "          [-1.50510788e-01,  8.06650072e-02, -7.58177191e-02,\n",
       "            1.45543292e-01,  9.07062143e-02, -1.08764447e-01,\n",
       "            7.83076808e-02,  1.49426118e-01],\n",
       "          [-1.03878781e-01,  7.82113522e-02, -8.14339966e-02,\n",
       "           -1.50040329e-01, -1.36363417e-01, -1.25828415e-01,\n",
       "           -1.53908089e-01,  1.27230659e-01]]],\n",
       " \n",
       " \n",
       "        [[[ 2.08675563e-02,  3.83451395e-02, -1.23970121e-01,\n",
       "           -1.24406457e-01, -1.63279921e-02, -5.55873215e-02,\n",
       "            3.07854787e-02,  1.55403152e-01],\n",
       "          [-1.44970492e-01, -1.36569828e-01, -8.14419687e-02,\n",
       "           -1.60219252e-01, -1.39654562e-01, -1.55370682e-02,\n",
       "           -9.64163616e-02, -1.29094124e-02],\n",
       "          [ 5.65391779e-02,  8.46446231e-02, -1.76929384e-02,\n",
       "            1.46368250e-01, -6.27951771e-02, -1.03381343e-01,\n",
       "           -1.21049628e-01, -4.96765822e-02]],\n",
       " \n",
       "         [[-1.68520018e-01, -5.67592420e-02, -1.11121327e-01,\n",
       "           -6.08499795e-02, -8.15434754e-03, -8.69610459e-02,\n",
       "           -9.52252969e-02, -3.27294916e-02],\n",
       "          [ 1.38693020e-01, -6.26534224e-02,  9.41492170e-02,\n",
       "            1.23463616e-01, -7.31350482e-03, -6.83141276e-02,\n",
       "            6.41579926e-02, -1.25480384e-01],\n",
       "          [-1.49081796e-02,  1.09525330e-01, -9.34885591e-02,\n",
       "           -1.87750161e-02,  8.12485069e-02, -1.62501425e-01,\n",
       "            2.17728410e-03, -1.68449372e-01]],\n",
       " \n",
       "         [[ 1.31407365e-01, -1.47623643e-01, -6.52787909e-02,\n",
       "            1.23693898e-01,  7.86621422e-02, -6.04662076e-02,\n",
       "            9.34104621e-02, -1.71767980e-01],\n",
       "          [-1.65244907e-01,  8.08285847e-02, -1.43332779e-03,\n",
       "            1.71177790e-01,  9.04017240e-02,  1.02613792e-01,\n",
       "           -5.40596470e-02,  1.67233363e-01],\n",
       "          [ 1.59300312e-01,  1.36495054e-01, -1.42682537e-01,\n",
       "           -2.70159245e-02, -2.74126828e-02,  1.37590989e-01,\n",
       "            1.52320275e-02, -1.78916156e-02]],\n",
       " \n",
       "         [[-2.32855380e-02, -3.63177657e-02,  8.88221711e-02,\n",
       "            2.07915753e-02,  3.90654206e-02,  1.54213622e-01,\n",
       "            3.32528502e-02, -1.13749288e-01],\n",
       "          [-1.80674031e-01, -1.85423344e-02, -9.28968340e-02,\n",
       "            9.80359763e-02, -1.03885822e-01,  2.65919566e-02,\n",
       "            4.39704098e-02, -6.90252408e-02],\n",
       "          [ 8.24746937e-02, -6.83808327e-03, -1.73589915e-01,\n",
       "           -1.40066817e-01, -6.64445534e-02, -2.02597231e-02,\n",
       "           -1.70498222e-01, -4.57756221e-02]]],\n",
       " \n",
       " \n",
       "        [[[-9.91331041e-02,  1.87945306e-01, -2.22167969e-02,\n",
       "            2.96750516e-02, -1.46245956e-01, -4.53683734e-03,\n",
       "           -1.50366453e-02,  1.51778013e-02],\n",
       "          [ 1.72121540e-01,  8.58931690e-02, -6.01467863e-02,\n",
       "            3.78918648e-02, -2.01254040e-02, -8.71153846e-02,\n",
       "           -7.57838115e-02, -1.11003399e-01],\n",
       "          [ 1.89052820e-02, -1.45322114e-01, -9.56338793e-02,\n",
       "            8.86020511e-02, -5.25764674e-02,  7.05777258e-02,\n",
       "            2.42956262e-03, -8.48460197e-03]],\n",
       " \n",
       "         [[-1.76713154e-01,  2.89458744e-02, -2.87978500e-02,\n",
       "            1.75682887e-01,  5.23497164e-03, -1.01314075e-01,\n",
       "           -2.11479187e-01,  1.21660307e-01],\n",
       "          [ 1.76762506e-01,  1.97596356e-01, -1.73250198e-01,\n",
       "            1.06165186e-01, -8.60705450e-02, -3.37259918e-02,\n",
       "            1.34743348e-01,  3.95435393e-02],\n",
       "          [-1.20893419e-02,  9.50637534e-02, -7.15309158e-02,\n",
       "            1.68116435e-01,  6.07897192e-02, -3.16888243e-02,\n",
       "            2.53183991e-02, -1.38363242e-01]],\n",
       " \n",
       "         [[ 2.94848830e-02,  5.38828745e-02,  4.54851240e-02,\n",
       "           -1.14896215e-01,  1.41278759e-01, -1.77987739e-01,\n",
       "            6.72061648e-03,  1.39595568e-03],\n",
       "          [-1.40399754e-01,  1.25073493e-01, -1.38240471e-01,\n",
       "            1.75841376e-01, -4.05461043e-02,  4.57963347e-03,\n",
       "            3.94941792e-02, -1.27488226e-01],\n",
       "          [-7.50880241e-02,  1.15603402e-01,  1.31420001e-01,\n",
       "           -1.80000469e-01,  8.44073445e-02, -2.95532793e-02,\n",
       "           -2.08080947e-01,  9.78346914e-02]],\n",
       " \n",
       "         [[ 5.35043329e-02, -4.55412567e-02, -6.70069456e-03,\n",
       "            1.63291246e-02,  1.63815632e-01,  1.22444332e-02,\n",
       "            1.31992400e-01, -4.10570651e-02],\n",
       "          [ 1.72168612e-02, -1.36353612e-01,  1.40228644e-01,\n",
       "           -7.36374930e-02, -1.26193836e-01, -1.64975286e-01,\n",
       "            1.10456996e-01,  5.28431833e-02],\n",
       "          [-2.87591517e-02, -6.47321418e-02, -1.58121824e-01,\n",
       "            1.18133202e-01,  1.08110204e-01,  1.01093873e-01,\n",
       "            1.21580243e-01, -1.74608469e-01]]],\n",
       " \n",
       " \n",
       "        [[[-8.05653334e-02, -1.01050653e-01,  4.47354913e-02,\n",
       "            7.98255056e-02,  5.15595824e-02, -4.15845662e-02,\n",
       "            8.63754004e-02, -1.13214478e-01],\n",
       "          [-9.23403651e-02, -1.53125241e-01, -1.45219162e-01,\n",
       "            1.09800026e-01,  3.72182578e-02, -1.81132019e-01,\n",
       "           -1.74323797e-01, -4.21936810e-02],\n",
       "          [-1.56475455e-01,  5.63997254e-02,  1.80916771e-01,\n",
       "           -1.25513136e-01, -1.17847025e-01,  8.28834623e-02,\n",
       "            6.23443630e-03, -1.17106281e-01]],\n",
       " \n",
       "         [[-6.58287033e-02,  2.57587135e-02, -2.34884769e-02,\n",
       "           -7.51939416e-02, -1.63731724e-01,  3.62694710e-02,\n",
       "           -1.52214646e-01,  1.18659571e-01],\n",
       "          [-6.07924014e-02,  1.67567834e-01, -7.66196474e-02,\n",
       "            1.09516069e-01, -1.15080349e-01, -4.29658592e-02,\n",
       "           -1.57397687e-01,  2.57469714e-02],\n",
       "          [-1.26591206e-01,  1.77538127e-01,  1.45747408e-01,\n",
       "           -6.36392310e-02,  1.54361352e-01,  1.47459462e-01,\n",
       "           -1.83151677e-01,  4.81338799e-03]],\n",
       " \n",
       "         [[-1.54734075e-01, -1.73921436e-01, -5.88580370e-02,\n",
       "           -1.63900897e-01, -9.92855951e-02, -1.61357939e-01,\n",
       "            2.37002913e-02, -3.14481705e-02],\n",
       "          [ 1.44595787e-01,  1.21376261e-01, -1.19567581e-01,\n",
       "            1.58821538e-01, -1.82885289e-01, -1.82822347e-04,\n",
       "           -8.44416991e-02, -8.78080130e-02],\n",
       "          [-1.33191317e-01, -4.34535742e-03,  3.14934552e-03,\n",
       "           -8.46896544e-02,  8.16270858e-02, -4.14075106e-02,\n",
       "            2.64702626e-02, -1.53149709e-01]],\n",
       " \n",
       "         [[-1.40444040e-02,  6.09723181e-02,  5.31375110e-02,\n",
       "           -9.66669619e-03, -4.41865027e-02, -7.78475255e-02,\n",
       "            3.45723983e-03,  1.77778110e-01],\n",
       "          [-7.55748078e-02, -1.20452665e-01,  9.22677368e-02,\n",
       "            1.51449755e-01, -4.99403924e-02,  1.44038394e-01,\n",
       "           -2.03579605e-01,  1.24304399e-01],\n",
       "          [-3.00752819e-02, -6.76330328e-02, -1.82054877e-01,\n",
       "            1.17300138e-01,  3.57500315e-02, -1.12459078e-01,\n",
       "            1.48688674e-01, -1.64799288e-01]]]], dtype=float32)>,\n",
       " 'W2': <tf.Variable 'Variable:0' shape=(2, 2, 8, 16) dtype=float32, numpy=\n",
       " array([[[[ 5.05352020e-03, -2.82341242e-02, -4.57334518e-02,\n",
       "            2.46246159e-01,  9.43319798e-02, -7.70751834e-02,\n",
       "           -3.19665074e-02,  5.05304933e-02, -2.16878653e-02,\n",
       "            1.26348972e-01, -1.56001389e-01,  2.43784785e-02,\n",
       "            2.44576335e-02,  2.47635245e-02, -1.10052645e-01,\n",
       "           -1.95521772e-01],\n",
       "          [ 6.54556751e-02,  1.60078794e-01,  1.61315203e-02,\n",
       "            3.05495858e-02, -1.03032887e-01,  2.41208449e-01,\n",
       "           -1.95131898e-01,  2.04820007e-01,  1.19904719e-01,\n",
       "           -1.09251738e-01,  1.74002707e-01, -2.00994849e-01,\n",
       "            2.11147591e-01,  1.39710844e-01,  2.07300246e-01,\n",
       "            1.09595150e-01],\n",
       "          [-1.71915889e-01,  2.21413255e-01,  1.52281284e-01,\n",
       "            2.40632653e-01,  2.41668403e-01, -2.28683710e-01,\n",
       "           -3.69323492e-02,  3.05510759e-02, -5.82277775e-02,\n",
       "           -1.51659250e-01, -2.27850676e-03, -6.57517910e-02,\n",
       "           -1.38259530e-01,  7.26819634e-02, -1.43334508e-01,\n",
       "            1.53128266e-01],\n",
       "          [-2.23184705e-01, -7.63617754e-02,  1.92691684e-02,\n",
       "            1.36917651e-01,  1.91953778e-02,  2.83011794e-02,\n",
       "           -2.31930315e-01,  1.07665896e-01, -2.21687019e-01,\n",
       "           -1.95843697e-01,  7.89468288e-02, -1.31421864e-01,\n",
       "            1.49955869e-01,  4.68430519e-02, -1.18810952e-01,\n",
       "            7.51233697e-02],\n",
       "          [ 1.51286125e-02,  2.12906718e-01,  2.38206625e-01,\n",
       "           -5.55210114e-02, -4.52614427e-02,  1.20399594e-01,\n",
       "            6.68191910e-03, -1.22659743e-01, -2.02272117e-01,\n",
       "           -2.48100579e-01,  1.91702008e-01,  2.11593688e-01,\n",
       "            1.46039486e-01, -2.02293277e-01,  2.22432971e-01,\n",
       "            2.44883776e-01],\n",
       "          [ 2.15218663e-01, -2.46145308e-01,  9.35330987e-02,\n",
       "           -4.30258512e-02, -1.41821265e-01, -2.15009034e-01,\n",
       "           -1.96089387e-01, -3.79861593e-02,  1.17738247e-02,\n",
       "            2.92422771e-02, -1.42355204e-01,  5.50354719e-02,\n",
       "            1.88209713e-01,  5.14103770e-02, -6.48424029e-02,\n",
       "            1.00986719e-01],\n",
       "          [-9.93555784e-02, -6.47897124e-02, -2.43731916e-01,\n",
       "           -1.13449907e-02, -2.37156987e-01,  1.71300769e-02,\n",
       "            1.64078712e-01, -5.13586998e-02, -1.98060215e-01,\n",
       "           -2.41620898e-01,  9.74292755e-02, -2.23112226e-01,\n",
       "            1.51733324e-01,  9.07576680e-02, -2.19459176e-01,\n",
       "            1.62150204e-01],\n",
       "          [ 1.32115185e-01,  1.14163339e-01, -3.12335491e-02,\n",
       "           -7.48533010e-02, -6.83641434e-03,  1.27008080e-01,\n",
       "            5.14078736e-02, -4.90324497e-02,  9.04514194e-02,\n",
       "            1.77545667e-01, -1.68778419e-01,  3.05094719e-02,\n",
       "           -1.08114064e-01, -2.08503783e-01,  2.57843733e-02,\n",
       "            7.62059689e-02]],\n",
       " \n",
       "         [[-2.41044879e-01, -4.86564040e-02,  4.41948771e-02,\n",
       "            4.12873030e-02,  4.18158770e-02,  4.22586203e-02,\n",
       "           -8.54525566e-02,  1.46657348e-01, -1.17061317e-01,\n",
       "            2.36956179e-01,  4.48417068e-02,  1.12980485e-01,\n",
       "            1.48996413e-01, -2.07015395e-01, -9.20772552e-04,\n",
       "           -9.41104889e-02],\n",
       "          [ 1.87226921e-01,  1.40324950e-01,  1.78947046e-01,\n",
       "            1.75683424e-02,  9.55212116e-03,  1.03378475e-01,\n",
       "            1.00852251e-02,  2.35266566e-01, -1.95110679e-01,\n",
       "           -2.22851694e-01,  1.42531231e-01,  2.34175503e-01,\n",
       "           -1.84425890e-01, -1.02821231e-01,  8.74039531e-02,\n",
       "           -6.54193386e-02],\n",
       "          [ 7.40358829e-02,  6.08536005e-02,  5.22809029e-02,\n",
       "            1.56285107e-01, -5.49710989e-02, -2.06361890e-01,\n",
       "           -9.68451500e-02, -1.06478393e-01,  9.38093662e-02,\n",
       "            2.20124722e-02,  3.58008146e-02,  2.06709504e-02,\n",
       "           -2.02750146e-01,  8.84079933e-03,  1.16147101e-01,\n",
       "           -1.76018417e-01],\n",
       "          [-1.27151906e-01, -1.65687025e-01,  1.70618296e-03,\n",
       "            1.94360256e-01, -2.20826626e-01,  1.85610950e-01,\n",
       "           -1.82583153e-01, -2.65693665e-03,  8.94447565e-02,\n",
       "            8.95336270e-02, -4.66791987e-02, -2.13830709e-01,\n",
       "            1.43119097e-02, -2.48826921e-01,  2.26536155e-01,\n",
       "           -1.21492445e-01],\n",
       "          [ 1.23372793e-01,  1.30504906e-01, -2.03677297e-01,\n",
       "            2.23778665e-01,  3.55941057e-02,  8.78807306e-02,\n",
       "           -5.75137138e-02, -1.24817967e-01,  2.14111209e-02,\n",
       "           -2.13646710e-01,  2.02298880e-01, -2.45184720e-01,\n",
       "            4.73074913e-02, -6.62093759e-02, -1.53994858e-01,\n",
       "           -7.07846284e-02],\n",
       "          [ 1.23257816e-01, -1.70562029e-01,  4.34517860e-05,\n",
       "            2.19719410e-01,  1.85375571e-01,  6.23543262e-02,\n",
       "           -9.95135307e-02, -2.30823934e-01,  1.99583769e-02,\n",
       "            5.22490144e-02, -1.58288479e-01, -1.81270182e-01,\n",
       "            2.34488249e-02,  2.25232244e-02, -1.43206537e-01,\n",
       "           -1.36153340e-01],\n",
       "          [ 1.42705321e-01,  1.58986256e-01,  8.46373737e-02,\n",
       "           -1.60056144e-01, -1.04963779e-03, -5.40693998e-02,\n",
       "           -2.36076355e-01,  2.11422861e-01,  1.61902249e-01,\n",
       "           -4.89473343e-04, -8.86379406e-02, -1.78661808e-01,\n",
       "            2.39152908e-01, -9.26647186e-02,  1.83483556e-01,\n",
       "           -1.63479149e-01],\n",
       "          [ 1.54236495e-01, -1.55047059e-01, -2.59995461e-03,\n",
       "           -2.32581973e-01, -1.51406586e-01,  1.98506534e-01,\n",
       "           -2.44821310e-02, -9.57707167e-02,  1.28605604e-01,\n",
       "            7.43154883e-02, -1.50916278e-01, -2.37387896e-01,\n",
       "            1.50996864e-01,  6.41807914e-02, -1.32746696e-01,\n",
       "            1.32410645e-01]]],\n",
       " \n",
       " \n",
       "        [[[-1.87439442e-01, -2.39485025e-01,  2.42884755e-01,\n",
       "            1.52527571e-01,  1.85016274e-01,  1.76138639e-01,\n",
       "           -1.58321917e-01, -1.89227879e-01,  3.84318829e-03,\n",
       "            1.45022392e-01,  2.13860095e-01,  8.38621259e-02,\n",
       "           -1.70862913e-01, -6.86157942e-02, -7.11745024e-02,\n",
       "           -1.84372663e-01],\n",
       "          [-3.93628478e-02,  1.93043470e-01, -1.72675669e-01,\n",
       "            1.38191581e-02, -2.39810467e-01,  8.93502831e-02,\n",
       "            1.54982597e-01, -1.22145116e-01, -2.35746384e-01,\n",
       "            8.83739591e-02, -2.97194719e-02,  5.02557158e-02,\n",
       "           -1.62798882e-01, -7.28592873e-02,  4.61556353e-02,\n",
       "            8.24111700e-03],\n",
       "          [-1.84557855e-01, -5.73416948e-02, -2.30463266e-01,\n",
       "            4.03143764e-02,  2.16012418e-01, -2.47978568e-02,\n",
       "            2.46700883e-01, -7.15987682e-02, -9.60797071e-03,\n",
       "            9.96648073e-02, -7.25134015e-02, -1.71195030e-01,\n",
       "           -2.26120591e-01, -1.60835087e-01,  1.94969594e-01,\n",
       "            6.50064349e-02],\n",
       "          [ 2.07989216e-01, -1.79841042e-01, -1.03249550e-01,\n",
       "            1.87203467e-01,  1.27428770e-01, -1.07585728e-01,\n",
       "           -1.14607453e-01,  1.70578241e-01,  1.47204876e-01,\n",
       "           -2.08014846e-02,  2.34689713e-02,  2.61832476e-02,\n",
       "            1.63933218e-01, -8.22494030e-02,  2.00684071e-02,\n",
       "           -1.44543946e-01],\n",
       "          [-4.39238548e-03,  2.23175287e-01,  4.00583148e-02,\n",
       "           -1.64965570e-01,  1.82334185e-01,  1.46684587e-01,\n",
       "            1.41156256e-01, -1.62814856e-01, -2.01606989e-01,\n",
       "           -1.62871122e-01,  1.94141030e-01, -1.21149600e-01,\n",
       "            1.64196491e-01, -2.26042032e-01, -4.22583818e-02,\n",
       "            2.49638855e-01],\n",
       "          [-1.91519916e-01, -1.26612723e-01, -8.85379314e-03,\n",
       "            2.58072019e-02,  1.42814815e-01,  6.41459823e-02,\n",
       "            1.68236494e-01,  1.61178589e-01, -2.00762928e-01,\n",
       "            9.88225341e-02, -2.35099435e-01, -2.00223923e-01,\n",
       "            2.24004924e-01,  5.34591675e-02,  1.95738614e-01,\n",
       "           -2.22766578e-01],\n",
       "          [-2.08316863e-01, -1.58648372e-01,  2.07429379e-01,\n",
       "           -9.02663618e-02, -9.40968394e-02,  1.61407933e-01,\n",
       "            7.33608007e-03, -9.82683897e-03,  1.91362634e-01,\n",
       "           -3.61662507e-02, -3.15315686e-02, -1.64670125e-01,\n",
       "            1.58128127e-01, -8.44821930e-02,  1.53750524e-01,\n",
       "            1.60946682e-01],\n",
       "          [ 8.15699100e-02, -2.02509105e-01,  2.45927930e-01,\n",
       "           -1.14269495e-01,  1.16389036e-01,  1.76601946e-01,\n",
       "           -3.23727131e-02, -2.18175650e-01, -2.39458799e-01,\n",
       "            5.01464009e-02, -6.20825887e-02,  7.06199408e-02,\n",
       "           -4.38957214e-02,  1.62235200e-01,  3.78850102e-02,\n",
       "           -2.07246304e-01]],\n",
       " \n",
       "         [[-2.06474900e-01,  1.77957892e-01,  1.30720973e-01,\n",
       "            6.01405501e-02,  7.87937641e-03, -9.65774655e-02,\n",
       "           -2.23747313e-01,  2.49454618e-01,  8.74667168e-02,\n",
       "            1.07961297e-01,  5.01120687e-02,  1.39211833e-01,\n",
       "           -1.70528471e-01,  1.83217943e-01,  7.59577751e-03,\n",
       "            1.61166668e-01],\n",
       "          [-1.45085096e-01, -6.10842109e-02,  3.72818112e-02,\n",
       "           -1.10145926e-01,  1.52562261e-01, -4.86130118e-02,\n",
       "           -8.09656382e-02, -2.35419810e-01,  1.47971153e-01,\n",
       "            7.79727697e-02,  1.83308616e-01,  1.47012666e-01,\n",
       "           -1.79144084e-01, -1.22790575e-01, -1.47407770e-01,\n",
       "            1.37997761e-01],\n",
       "          [ 1.26223266e-01, -1.40649676e-01,  1.91504538e-01,\n",
       "            3.82817388e-02, -2.40605831e-01, -5.13162613e-02,\n",
       "            5.43687940e-02, -7.04206228e-02, -1.05452955e-01,\n",
       "           -1.19928658e-01, -2.28370309e-01,  2.10808516e-01,\n",
       "           -7.95696378e-02, -1.98505402e-01, -1.29750609e-01,\n",
       "            1.57378972e-01],\n",
       "          [ 1.18072033e-01,  7.96847343e-02, -2.43353426e-01,\n",
       "            1.70029998e-02,  3.47715020e-02, -6.52845502e-02,\n",
       "            1.12124443e-01, -1.60753727e-03,  2.43852139e-02,\n",
       "            1.53947651e-01,  1.14241064e-01,  2.01808572e-01,\n",
       "            2.16398299e-01,  4.91023660e-02,  8.76885653e-03,\n",
       "           -3.24679017e-02],\n",
       "          [-1.86848581e-01, -2.02338934e-01,  3.28962207e-02,\n",
       "            2.37900019e-01, -4.12617326e-02,  1.30795240e-01,\n",
       "           -2.38362372e-01,  4.15473580e-02,  1.96844935e-02,\n",
       "           -1.94219232e-01,  1.18811548e-01, -1.88700795e-01,\n",
       "            8.31390023e-02, -1.40160799e-01,  1.25277936e-01,\n",
       "           -1.48778021e-01],\n",
       "          [-7.20190406e-02, -1.06125176e-01, -1.85240686e-01,\n",
       "           -1.08102381e-01,  2.12299526e-01,  2.13495433e-01,\n",
       "           -8.26811790e-02,  4.69897389e-02, -1.77855968e-01,\n",
       "           -1.02210104e-01, -7.62937069e-02,  1.43164039e-01,\n",
       "           -2.41635919e-01,  5.13969660e-02,  1.81885898e-01,\n",
       "           -7.41513968e-02],\n",
       "          [ 2.46111646e-01, -2.07152367e-01, -6.85171485e-02,\n",
       "           -2.40706742e-01,  3.25697660e-02, -1.49049342e-01,\n",
       "            1.02628052e-01,  4.70194221e-02, -7.67720342e-02,\n",
       "            1.00304127e-01,  1.52968377e-01, -2.13966548e-01,\n",
       "            9.66972709e-02, -3.67438197e-02, -2.06869245e-01,\n",
       "            3.88133079e-02],\n",
       "          [ 1.77097201e-01,  1.96233511e-01,  1.51748538e-01,\n",
       "            1.69725418e-02,  2.11882412e-01, -1.16778612e-02,\n",
       "           -2.17716694e-01,  1.36195898e-01, -7.52553344e-02,\n",
       "            2.28503406e-01,  8.28370452e-02, -1.22151971e-01,\n",
       "           -2.64889002e-03, -2.22547770e-01,  1.81061506e-01,\n",
       "            8.08202624e-02]]]], dtype=float32)>}"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z3  = forward_propagation(X_train, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1080, 6), dtype=float32, numpy=\n",
       "array([[-0.45562488,  0.05303733,  0.30852288,  0.5754441 , -0.26206085,\n",
       "        -0.16456339],\n",
       "       [-0.5380889 ,  0.08783676,  0.3485534 ,  0.48973408, -0.30438644,\n",
       "        -0.25988322],\n",
       "       [-0.4501446 ,  0.08689437,  0.31781995,  0.49001268, -0.24152191,\n",
       "        -0.18733937],\n",
       "       ...,\n",
       "       [-0.48593748,  0.07073202,  0.35081795,  0.48986128, -0.27843747,\n",
       "        -0.28436267],\n",
       "       [-0.46101734,  0.07825373,  0.36105552,  0.5406629 , -0.25820228,\n",
       "        -0.2281376 ],\n",
       "       [-0.454681  ,  0.06969163,  0.3643374 ,  0.5109173 , -0.24611025,\n",
       "        -0.24798094]], dtype=float32)>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_propagation(X_train, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(4, 4, 3, 8), dtype=float32, numpy=\n",
       " array([[[[ 0.        , -0.02864277,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.07980798,  0.        ],\n",
       "          [ 0.        , -0.01322393,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.043991  ,  0.        ],\n",
       "          [ 0.        , -0.01256461,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.03227901,  0.        ]],\n",
       " \n",
       "         [[ 0.        , -0.02508529,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.12438801,  0.        ],\n",
       "          [ 0.        , -0.01345872,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.07662858,  0.        ],\n",
       "          [ 0.        , -0.01232117,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.05516328,  0.        ]],\n",
       " \n",
       "         [[ 0.        , -0.00134306,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.11157767,  0.        ],\n",
       "          [ 0.        , -0.00143899,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.06641377,  0.        ],\n",
       "          [ 0.        , -0.00201459,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.04551485,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.13449308,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.08927614,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.06497333,  0.        ]]],\n",
       " \n",
       " \n",
       "        [[[ 0.        , -0.03105791,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.06246018,  0.        ],\n",
       "          [ 0.        , -0.01142012,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.03121402,  0.        ],\n",
       "          [ 0.        , -0.0112272 ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.02325502,  0.        ]],\n",
       " \n",
       "         [[ 0.        , -0.0266108 ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.10001363,  0.        ],\n",
       "          [ 0.        , -0.01246458,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.056469  ,  0.        ],\n",
       "          [ 0.        , -0.01152322,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.03926875,  0.        ]],\n",
       " \n",
       "         [[ 0.        , -0.00153493,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.10052375,  0.        ],\n",
       "          [ 0.        , -0.00172679,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.05777949,  0.        ],\n",
       "          [ 0.        , -0.00220646,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.0395808 ,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.19606638,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.15886503,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.13593373,  0.        ]]],\n",
       " \n",
       " \n",
       "        [[[ 0.        , -0.02284765,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.06406727,  0.        ],\n",
       "          [ 0.        , -0.00978407,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.03176968,  0.        ],\n",
       "          [ 0.        , -0.00839259,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.02328249,  0.        ]],\n",
       " \n",
       "         [[ 0.        , -0.02354792,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.09734329,  0.        ],\n",
       "          [ 0.        , -0.01492053,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.05430049,  0.        ],\n",
       "          [ 0.        , -0.01336012,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.03719392,  0.        ]],\n",
       " \n",
       "         [[ 0.        , -0.00211052,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.13215573,  0.        ],\n",
       "          [ 0.        , -0.00182273,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.08938475,  0.        ],\n",
       "          [ 0.        , -0.00249426,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.06929779,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.32581875,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.30439615,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.28232187,  0.        ]]],\n",
       " \n",
       " \n",
       "        [[[ 0.        , -0.0155044 ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.07258345,  0.        ],\n",
       "          [ 0.        , -0.01079462,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.03668598,  0.        ],\n",
       "          [ 0.        , -0.00836917,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.02644381,  0.        ]],\n",
       " \n",
       "         [[ 0.        , -0.02793661,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.11433239,  0.        ],\n",
       "          [ 0.        , -0.02585065,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.06880569,  0.        ],\n",
       "          [ 0.        , -0.0240863 ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.04872452,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.24384113,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.21045917,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.18773842,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.4153228 ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.40590844,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.3858866 ,  0.        ]]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2, 8, 16), dtype=float32, numpy=\n",
       " array([[[[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.        ,  0.00037962,  0.        ,  0.        ,\n",
       "            0.        , -0.00191774,  0.        ,  0.00131334,\n",
       "           -0.00104634,  0.        , -0.00018664,  0.        ,\n",
       "            0.00065142,  0.00145899, -0.00040176, -0.00030462],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.00429026,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.01052853,  0.        ,\n",
       "            0.00761057,  0.        ,  0.        ,  0.02905827],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.00151703,  0.        ,  0.00035708,  0.00061584,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        , -0.0001673 , -0.00069393,\n",
       "            0.        ,  0.        ,  0.        , -0.00020969],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.        ,  0.01323425,  0.01000058, -0.00103463,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.00679496,  0.        ,  0.00494504,  0.0113996 ,\n",
       "            0.        ,  0.        ,  0.00134308,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ]]],\n",
       " \n",
       " \n",
       "        [[[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.        ,  0.00122963,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.00057074,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.00017376,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.00502982, -0.00454723,\n",
       "            0.        ,  0.02129395,  0.        ,  0.        ,\n",
       "            0.02102763,  0.        , -0.00084882,  0.00830069,\n",
       "            0.02086695,  0.        , -0.01962137,  0.01540783],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        , -0.00181219,  0.00094562,\n",
       "            0.        ,  0.        ,  0.        , -0.00209478],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [-0.0276225 ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        , -0.00880619,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.00067022],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ,  0.        ,  0.        ,  0.        ]]]],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 2, 8, 16])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['W2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1080, 6), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(4, 4, 3, 8) dtype=float32, numpy=\n",
       " array([[[[-1.23658627e-01,  1.28980517e-01,  2.99017727e-02,\n",
       "           -6.24340512e-02, -6.91982955e-02,  1.06515422e-01,\n",
       "            1.04723729e-01, -9.84828100e-02],\n",
       "          [ 5.91215342e-02,  1.51253743e-02,  3.71702872e-02,\n",
       "            2.81866197e-03,  1.56052932e-01,  1.00875892e-01,\n",
       "           -8.22528973e-02, -2.49311715e-01],\n",
       "          [-2.18333304e-02, -1.21484220e-01,  1.33416608e-01,\n",
       "            1.01510800e-01,  1.20735265e-01, -1.09636478e-01,\n",
       "           -2.63182074e-01, -1.46850213e-01]],\n",
       " \n",
       "         [[-9.18818861e-02,  1.15498811e-01, -2.75909007e-02,\n",
       "           -1.30599692e-01, -6.97350223e-03, -2.22572796e-02,\n",
       "            1.05418645e-01, -5.02428189e-02],\n",
       "          [-1.31662190e-03, -6.63878908e-03,  3.97404749e-03,\n",
       "           -5.65101951e-02, -1.32451385e-01,  3.23848948e-02,\n",
       "            9.22172815e-02, -1.40924500e-02],\n",
       "          [-1.04838662e-01, -1.64813533e-01, -4.10966612e-02,\n",
       "           -2.17375621e-01, -6.84325248e-02, -1.29180521e-01,\n",
       "            2.24229656e-02, -6.84419647e-02]],\n",
       " \n",
       "         [[ 5.06406426e-02, -9.01055336e-03, -6.21799789e-02,\n",
       "           -2.11179391e-01, -1.34015232e-02, -1.23953596e-01,\n",
       "           -4.81097810e-02, -5.00223041e-03],\n",
       "          [-1.63026512e-01,  1.06296182e-01, -1.38157219e-01,\n",
       "            1.19263411e-01,  8.86645284e-04,  1.04412392e-01,\n",
       "           -8.75667557e-02, -1.57574162e-01],\n",
       "          [ 9.77093428e-02, -4.15998250e-02, -1.98624134e-01,\n",
       "           -1.14562266e-01,  1.41283302e-02, -1.42811686e-01,\n",
       "           -2.15820521e-01, -1.59683824e-02]],\n",
       " \n",
       "         [[ 1.13024488e-01,  4.23113257e-02, -9.30145290e-03,\n",
       "            4.53229770e-02, -1.64168626e-01,  1.05820242e-02,\n",
       "            9.11269486e-02, -7.67127499e-02],\n",
       "          [-1.50510788e-01,  4.60816734e-02, -1.59122542e-01,\n",
       "            1.11735098e-01,  4.41439897e-02, -1.62802711e-01,\n",
       "            6.50643036e-02,  1.49426118e-01],\n",
       "          [-1.03878781e-01,  3.58053930e-02, -1.66824788e-01,\n",
       "           -1.84224889e-01, -1.79447055e-01, -1.79864228e-01,\n",
       "           -1.70077041e-01,  1.27230659e-01]]],\n",
       " \n",
       " \n",
       "        [[[ 2.08675563e-02, -7.98074063e-03, -1.44354567e-01,\n",
       "           -1.59040779e-01, -2.72561423e-03, -1.65705034e-03,\n",
       "           -1.01360073e-02,  7.89474472e-02],\n",
       "          [-1.44970492e-01, -1.86945632e-01, -9.69993994e-02,\n",
       "           -1.95153490e-01, -1.20106459e-01,  3.81046981e-02,\n",
       "           -1.59628674e-01, -8.65511298e-02],\n",
       "          [ 5.65391779e-02,  2.77491603e-02, -2.95180921e-02,\n",
       "            1.09920971e-01, -4.12268788e-02, -1.56548277e-01,\n",
       "           -1.94390044e-01, -1.25470564e-01]],\n",
       " \n",
       "         [[-1.68520018e-01, -1.18674792e-01, -1.29870042e-01,\n",
       "           -9.64802727e-02, -6.68123513e-02, -1.40991777e-01,\n",
       "           -1.13255821e-01, -1.05538130e-01],\n",
       "          [ 1.38693020e-01, -1.30619511e-01,  8.07897970e-02,\n",
       "            8.82740766e-02, -6.56005591e-02, -1.22337110e-01,\n",
       "            2.93263271e-02, -2.00522006e-01],\n",
       "          [-1.49081796e-02,  3.38704884e-02, -1.02715544e-01,\n",
       "           -5.47731780e-02,  2.29022298e-02, -2.16511711e-01,\n",
       "           -3.93157750e-02, -2.44761527e-01]],\n",
       " \n",
       "         [[ 1.31407365e-01, -2.06690058e-01, -1.52877107e-01,\n",
       "            8.68545696e-02,  2.36070491e-02, -1.14500612e-01,\n",
       "            7.32510015e-02, -1.71767980e-01],\n",
       "          [-1.65244907e-01,  1.38061214e-02, -8.12965930e-02,\n",
       "            1.35072902e-01,  3.58180217e-02,  4.85829897e-02,\n",
       "           -8.98215696e-02,  1.67233363e-01],\n",
       "          [ 1.59300312e-01,  6.06074035e-02, -2.20229641e-01,\n",
       "           -6.34776503e-02, -8.03759098e-02,  8.35657790e-02,\n",
       "           -2.77556553e-02, -1.78916156e-02]],\n",
       " \n",
       "         [[-2.32855380e-02, -6.24932013e-02, -4.98785404e-04,\n",
       "           -1.49061512e-02, -1.19249159e-02,  1.00174211e-01,\n",
       "            2.37810295e-02, -1.13749288e-01],\n",
       "          [-1.80674031e-01, -5.17337620e-02, -1.77360848e-01,\n",
       "            6.36159331e-02, -1.51270077e-01, -2.74461359e-02,\n",
       "            3.11598312e-02, -6.90252408e-02],\n",
       "          [ 8.24746937e-02, -4.84163091e-02, -2.57605970e-01,\n",
       "           -1.74708471e-01, -1.10451810e-01, -7.42957070e-02,\n",
       "           -1.86098486e-01, -4.57756221e-02]]],\n",
       " \n",
       " \n",
       "        [[[-9.91331041e-02,  1.40640438e-01, -3.40100005e-02,\n",
       "           -4.80938563e-03, -1.38528660e-01, -4.53683734e-03,\n",
       "           -4.96952161e-02, -6.16835803e-02],\n",
       "          [ 1.72121540e-01,  3.46214809e-02, -7.15371221e-02,\n",
       "            3.44858645e-03, -2.51443591e-03, -8.71153846e-02,\n",
       "           -1.31622940e-01, -1.85304016e-01],\n",
       "          [ 1.89052820e-02, -2.03014746e-01, -1.05509475e-01,\n",
       "            5.31393662e-02, -3.04396544e-02,  7.05777258e-02,\n",
       "           -6.20687418e-02, -8.44687819e-02]],\n",
       " \n",
       "         [[-1.76713154e-01, -3.44958678e-02, -3.52062881e-02,\n",
       "            1.39010787e-01, -6.68366998e-02, -1.01314075e-01,\n",
       "           -2.25807801e-01,  4.50490788e-02],\n",
       "          [ 1.76762506e-01,  1.27884790e-01, -1.77998707e-01,\n",
       "            6.98853657e-02, -1.58954248e-01, -3.37259918e-02,\n",
       "            1.04771167e-01, -3.25828269e-02],\n",
       "          [-1.20893419e-02,  1.74604766e-02, -7.46063963e-02,\n",
       "            1.31214589e-01, -1.21597713e-02, -3.16888243e-02,\n",
       "           -9.85492859e-03, -2.13340074e-01]],\n",
       " \n",
       "         [[ 2.94848830e-02, -4.58351523e-03, -1.13934791e-03,\n",
       "           -1.53286725e-01,  6.83618933e-02, -1.77987739e-01,\n",
       "           -1.48965381e-02,  1.39595568e-03],\n",
       "          [-1.40399754e-01,  5.90969771e-02, -1.80438429e-01,\n",
       "            1.37908086e-01, -1.14572830e-01,  4.57963347e-03,\n",
       "            3.48589034e-03, -1.27488226e-01],\n",
       "          [-7.50880241e-02,  4.06712741e-02,  9.06257331e-02,\n",
       "           -2.18357459e-01,  1.00517245e-02, -2.95532793e-02,\n",
       "           -2.50126868e-01,  9.78346914e-02]],\n",
       " \n",
       "         [[ 5.35043329e-02, -6.95877299e-02, -5.46578467e-02,\n",
       "           -1.96066126e-02,  9.93949398e-02,  1.22444332e-02,\n",
       "            1.20181724e-01, -4.10570651e-02],\n",
       "          [ 1.72168612e-02, -1.66300744e-01,  9.93789062e-02,\n",
       "           -1.08354345e-01, -1.91328689e-01, -1.64975286e-01,\n",
       "            9.64962095e-02,  5.28431833e-02],\n",
       "          [-2.87591517e-02, -1.03170671e-01, -1.93692669e-01,\n",
       "            8.31741020e-02,  4.30367105e-02,  1.01093873e-01,\n",
       "            1.05882399e-01, -1.74608469e-01]]],\n",
       " \n",
       " \n",
       "        [[[-8.05653334e-02, -1.35068655e-01,  3.43010314e-02,\n",
       "            4.17775400e-02,  2.43008789e-02, -4.15845662e-02,\n",
       "            1.02242164e-01, -1.13214478e-01],\n",
       "          [-9.23403651e-02, -1.92626685e-01, -1.56373963e-01,\n",
       "            7.03239813e-02,  3.48177645e-03, -1.81132019e-01,\n",
       "           -1.83299124e-01, -4.21936810e-02],\n",
       "          [-1.56475455e-01,  1.02179470e-02,  1.71242625e-01,\n",
       "           -1.66879132e-01, -1.49846673e-01,  8.28834623e-02,\n",
       "           -1.54209752e-02, -1.17106281e-01]],\n",
       " \n",
       "         [[-6.58287033e-02, -2.79580280e-02, -3.15057673e-02,\n",
       "           -1.15007706e-01, -2.18490183e-01,  3.62694710e-02,\n",
       "           -1.05907507e-01,  1.18659571e-01],\n",
       "          [-6.07924014e-02,  1.06250867e-01, -8.32486004e-02,\n",
       "            6.91497102e-02, -1.81458682e-01, -4.29658592e-02,\n",
       "           -1.36735305e-01,  2.57469714e-02],\n",
       "          [-1.26591206e-01,  1.08383946e-01,  1.40813246e-01,\n",
       "           -1.05416685e-01,  8.25438276e-02,  1.47459462e-01,\n",
       "           -1.74285218e-01,  4.81338799e-03]],\n",
       " \n",
       "         [[-1.54734075e-01, -1.99570328e-01, -9.93900225e-02,\n",
       "           -2.03327999e-01, -1.59504071e-01, -1.61357939e-01,\n",
       "            6.56098425e-02, -3.14481705e-02],\n",
       "          [ 1.44595787e-01,  8.87714997e-02, -1.56917751e-01,\n",
       "            1.19783409e-01, -2.57670015e-01, -1.82822347e-04,\n",
       "           -6.14327304e-02, -8.78080130e-02],\n",
       "          [-1.33191317e-01, -4.50580865e-02, -3.35766152e-02,\n",
       "           -1.24461323e-01, -1.11822272e-04, -4.14075106e-02,\n",
       "            3.87178957e-02, -1.53149709e-01]],\n",
       " \n",
       "         [[-1.40444040e-02,  4.57258746e-02,  1.21519854e-02,\n",
       "           -4.60249744e-02, -6.66115358e-02, -7.78475255e-02,\n",
       "            5.08075282e-02,  1.77778110e-01],\n",
       "          [-7.55748078e-02, -1.40644148e-01,  6.04331605e-02,\n",
       "            1.16531692e-01, -8.29507560e-02,  1.44038394e-01,\n",
       "           -1.63059041e-01,  1.24304399e-01],\n",
       "          [-3.00752819e-02, -9.51106250e-02, -2.05965176e-01,\n",
       "            8.21982697e-02, -1.54553109e-03, -1.12459078e-01,\n",
       "            1.83371052e-01, -1.64799288e-01]]]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(2, 2, 8, 16) dtype=float32, numpy=\n",
       " array([[[[ 5.05352020e-03, -2.82341242e-02, -4.57334518e-02,\n",
       "            2.46246159e-01,  9.43319798e-02, -7.70751834e-02,\n",
       "           -3.19665074e-02,  5.05304933e-02, -2.16878653e-02,\n",
       "            1.26348972e-01, -1.56001389e-01,  2.43784785e-02,\n",
       "            2.44576335e-02,  2.47635245e-02, -1.10052645e-01,\n",
       "           -1.95521772e-01],\n",
       "          [-9.36502125e-03,  7.86367580e-02, -4.56047691e-02,\n",
       "            7.24143162e-02, -1.55548215e-01,  7.29399025e-02,\n",
       "           -3.07665497e-01,  6.93779439e-02,  8.64977315e-02,\n",
       "           -2.06110492e-01,  2.26707205e-01, -1.21475726e-01,\n",
       "            2.62155861e-01,  1.29700482e-01,  1.29703954e-01,\n",
       "            2.91026592e-01],\n",
       "          [-1.24334417e-01,  1.04924716e-01,  2.05248848e-01,\n",
       "            2.18623340e-01,  2.41668403e-01, -2.45211869e-01,\n",
       "            4.55620252e-02,  5.63804470e-02, -9.30981785e-02,\n",
       "           -1.51659250e-01, -3.35535295e-02, -1.04187518e-01,\n",
       "           -2.08473638e-01,  1.29482090e-01, -1.64609253e-01,\n",
       "            1.81027547e-01],\n",
       "          [-2.71674007e-01, -9.86595303e-02, -5.19317728e-05,\n",
       "            8.81258473e-02, -2.57877074e-02,  3.16463895e-02,\n",
       "           -2.88201243e-01,  9.15934294e-02, -1.43156454e-01,\n",
       "           -2.49881640e-01,  1.91979304e-01, -5.50643429e-02,\n",
       "            5.63246012e-02,  1.10744551e-01, -1.71303436e-01,\n",
       "            2.60011405e-01],\n",
       "          [ 9.08294991e-02,  1.69828311e-01,  2.46928141e-01,\n",
       "            5.72113506e-02, -4.52614427e-02,  4.11985740e-02,\n",
       "            6.68191910e-03, -8.64419267e-02, -1.96961224e-01,\n",
       "           -2.48100579e-01,  2.14089617e-01,  3.77764583e-01,\n",
       "            1.16037391e-01, -1.83737695e-01,  1.41911536e-01,\n",
       "            2.55600125e-01],\n",
       "          [ 2.15218663e-01, -1.92177206e-01,  4.00661938e-02,\n",
       "            1.06383590e-02, -1.41821265e-01, -1.63924798e-01,\n",
       "           -1.96089387e-01,  1.21489987e-02,  1.17738247e-02,\n",
       "            2.92422771e-02, -1.42355204e-01,  5.50354719e-02,\n",
       "            1.88209713e-01,  1.05316170e-01, -1.18817024e-01,\n",
       "            1.00986719e-01],\n",
       "          [-2.80290306e-01, -2.53860235e-01, -3.40541035e-01,\n",
       "            2.46511489e-01, -2.72753328e-01, -1.78077340e-01,\n",
       "           -1.98060106e-02, -2.81049937e-01, -3.34229887e-01,\n",
       "           -2.95660615e-01,  2.41540045e-01, -3.74651179e-02,\n",
       "            2.47158363e-01, -2.77328622e-02, -4.03926432e-01,\n",
       "            3.62287939e-01],\n",
       "          [ 1.32115185e-01,  1.14163339e-01, -3.12335491e-02,\n",
       "           -7.48533010e-02, -6.83641434e-03,  1.27008080e-01,\n",
       "            5.14078736e-02, -4.90324497e-02,  9.04514194e-02,\n",
       "            1.77545667e-01, -1.68778419e-01,  3.05094719e-02,\n",
       "           -1.55188337e-01, -2.04688072e-01,  2.57843733e-02,\n",
       "            2.27717236e-02]],\n",
       " \n",
       "         [[-2.41044879e-01, -4.86564040e-02,  4.41948771e-02,\n",
       "            4.12873030e-02,  4.18158770e-02,  4.22586203e-02,\n",
       "           -8.54525566e-02,  1.46657348e-01, -1.17061317e-01,\n",
       "            2.36956179e-01,  4.48417068e-02,  1.12980485e-01,\n",
       "            1.48996413e-01, -2.07015395e-01, -9.20772552e-04,\n",
       "           -9.41104889e-02],\n",
       "          [ 1.43846661e-01, -7.13895354e-03,  1.44392893e-01,\n",
       "            9.47889611e-02, -1.23897828e-01, -1.34425674e-04,\n",
       "           -7.13479966e-02,  6.01517074e-02, -3.23984534e-01,\n",
       "           -2.95138627e-01,  1.97238386e-01,  2.87561685e-01,\n",
       "           -5.98950267e-01, -1.30889490e-01,  2.20115501e-02,\n",
       "            1.17365845e-01],\n",
       "          [ 4.17781575e-03, -9.78037901e-03,  1.34458214e-01,\n",
       "            8.60298127e-02, -5.49710989e-02, -2.07083181e-01,\n",
       "           -9.68451500e-02, -1.16805792e-01,  9.38093662e-02,\n",
       "            2.20124722e-02,  7.22965971e-02,  4.29921299e-02,\n",
       "           -2.68171489e-01,  7.94012547e-02,  9.20342132e-02,\n",
       "           -6.49456382e-02],\n",
       "          [-1.70065239e-01, -2.18103901e-01, -1.45844705e-02,\n",
       "            1.60557941e-01, -2.37140357e-01,  1.87717274e-01,\n",
       "           -2.64806300e-01, -1.93673186e-02,  9.09775570e-02,\n",
       "            3.54895964e-02,  6.63442165e-02, -1.33876398e-01,\n",
       "           -9.86794233e-02, -2.19523147e-01,  1.81162462e-01,\n",
       "            6.38395995e-02],\n",
       "          [-8.84983130e-03,  3.02381478e-02, -2.31450066e-01,\n",
       "            2.48844728e-01, -1.34009704e-01,  3.10538616e-02,\n",
       "           -5.75137138e-02, -1.87736049e-01, -3.20135280e-02,\n",
       "           -2.13646710e-01,  3.49730194e-01, -3.26303512e-01,\n",
       "           -7.16080097e-03, -9.44610592e-03, -2.14497045e-01,\n",
       "            1.01499021e-01],\n",
       "          [ 1.23257816e-01, -2.24489257e-01,  4.53018323e-02,\n",
       "            2.19719410e-01,  1.85375571e-01,  6.23543262e-02,\n",
       "           -9.95135307e-02, -1.79595470e-01,  1.99583769e-02,\n",
       "            5.22490144e-02, -1.58288479e-01, -1.81270182e-01,\n",
       "            2.34488249e-02,  7.47329816e-02, -1.09458081e-01,\n",
       "           -1.36153340e-01],\n",
       "          [-2.70911381e-02,  1.56482328e-02,  5.06629515e-03,\n",
       "            1.74102653e-02, -5.86772412e-02, -1.72580764e-01,\n",
       "           -3.19923282e-01, -2.63498239e-02,  1.57142192e-01,\n",
       "           -6.59642592e-02,  4.76192273e-02,  2.35500541e-02,\n",
       "           -2.46392891e-01, -1.17295407e-01,  6.47937581e-02,\n",
       "           -4.09924500e-02],\n",
       "          [ 1.54236495e-01, -1.55047059e-01, -2.59995461e-03,\n",
       "           -2.32581973e-01, -1.51406586e-01,  1.46073610e-01,\n",
       "           -2.44821310e-02, -9.57707167e-02,  1.28605604e-01,\n",
       "            7.43154883e-02, -1.50916278e-01, -2.37387896e-01,\n",
       "            1.50996864e-01,  6.41807914e-02, -1.32746696e-01,\n",
       "            1.32410645e-01]]],\n",
       " \n",
       " \n",
       "        [[[-1.87439442e-01, -2.39485025e-01,  2.42884755e-01,\n",
       "            1.52527571e-01,  1.85016274e-01,  1.76138639e-01,\n",
       "           -1.58321917e-01, -1.89227879e-01,  3.84318829e-03,\n",
       "            1.45022392e-01,  2.13860095e-01,  8.38621259e-02,\n",
       "           -1.70862913e-01, -6.86157942e-02, -7.11745024e-02,\n",
       "           -1.84372663e-01],\n",
       "          [-1.39815584e-01,  1.85514331e-01, -2.62319535e-01,\n",
       "           -1.12610541e-01, -2.93360323e-01, -3.98993082e-02,\n",
       "            7.10315034e-02, -1.52488112e-01, -3.81841719e-01,\n",
       "           -2.19491914e-01,  5.02102785e-02,  1.26290038e-01,\n",
       "           -4.44376394e-02, -1.00602925e-01,  3.06759290e-02,\n",
       "            2.16666445e-01],\n",
       "          [-1.84557855e-01, -1.07058898e-01, -2.30463266e-01,\n",
       "            1.74280126e-02,  2.16012418e-01,  2.37754714e-02,\n",
       "            3.05006295e-01, -1.19619071e-01, -9.60797071e-03,\n",
       "            9.96648073e-02, -1.12542987e-01, -1.53080255e-01,\n",
       "           -3.81848991e-01, -8.55079591e-02,  2.53109664e-01,\n",
       "            3.76105644e-02],\n",
       "          [ 1.20438188e-01, -1.75249204e-01, -1.61838070e-01,\n",
       "            1.43402010e-01,  1.02901869e-01, -7.48905167e-02,\n",
       "           -1.64890274e-01,  1.95859954e-01,  1.64057642e-01,\n",
       "           -7.48464465e-02,  1.36232734e-01,  9.85688865e-02,\n",
       "            1.97238903e-02,  7.57987797e-02, -1.25350179e-02,\n",
       "            3.83658707e-02],\n",
       "          [-5.87318577e-02,  2.23803729e-01,  8.93897861e-02,\n",
       "           -2.93531358e-01, -1.22685589e-01,  6.13987334e-02,\n",
       "            8.96624327e-02, -1.95846349e-01, -2.52672076e-01,\n",
       "           -2.16899753e-01,  2.23709792e-01, -1.01548105e-01,\n",
       "            1.37513608e-01, -2.72998363e-01, -1.22283429e-01,\n",
       "            2.67490029e-01],\n",
       "          [-1.37811646e-01, -1.26612723e-01, -8.85379314e-03,\n",
       "            2.58072019e-02,  1.42814815e-01,  1.01950169e-02,\n",
       "            1.68236494e-01,  1.61178589e-01, -2.50619918e-01,\n",
       "            9.88225341e-02, -2.88987070e-01, -2.00223923e-01,\n",
       "            1.70593783e-01,  5.34591675e-02,  1.95738614e-01,\n",
       "           -1.77571192e-01],\n",
       "          [-2.91083097e-01, -1.37073472e-01,  3.29870522e-01,\n",
       "           -2.35600352e-01, -1.38613239e-01,  2.51352564e-02,\n",
       "           -1.72821283e-01, -1.08209595e-01,  1.09333247e-01,\n",
       "           -9.13311988e-02,  6.67178109e-02,  5.94316842e-03,\n",
       "            3.86732876e-01, -1.34333730e-01,  3.66149321e-02,\n",
       "            3.31669122e-01],\n",
       "          [ 8.15699100e-02, -2.02509105e-01,  2.45927930e-01,\n",
       "           -1.14269495e-01,  1.16389036e-01,  1.76601946e-01,\n",
       "           -3.23727131e-02, -2.18175650e-01, -2.39458799e-01,\n",
       "            5.01464009e-02, -6.20825887e-02,  7.06199408e-02,\n",
       "           -4.38957214e-02,  1.62235200e-01,  3.78850102e-02,\n",
       "           -2.07246304e-01]],\n",
       " \n",
       "         [[-2.06474900e-01,  1.77957892e-01,  1.30720973e-01,\n",
       "            6.01405501e-02,  7.87937641e-03, -9.65774655e-02,\n",
       "           -2.23747313e-01,  2.49454618e-01,  8.74667168e-02,\n",
       "            1.07961297e-01,  5.01120687e-02,  1.39211833e-01,\n",
       "           -1.70528471e-01,  1.83217943e-01,  7.59577751e-03,\n",
       "            1.61166668e-01],\n",
       "          [-2.64883429e-01, -9.75348651e-02, -2.66314242e-02,\n",
       "           -2.14221954e-01, -7.25967064e-03, -1.13523871e-01,\n",
       "           -1.86582610e-01, -2.62741238e-01, -4.94517852e-03,\n",
       "           -2.32247964e-01,  2.34740332e-01,  2.52627969e-01,\n",
       "           -4.09240484e-01, -8.66077244e-02, -1.78291991e-01,\n",
       "            3.18161100e-01],\n",
       "          [ 1.26223266e-01, -2.12693438e-01,  1.91504538e-01,\n",
       "            6.48867153e-03, -2.40605831e-01,  1.72678772e-02,\n",
       "            5.43687940e-02, -1.30438015e-01, -1.05452955e-01,\n",
       "           -1.19928658e-01, -2.07328871e-01,  2.37892032e-01,\n",
       "           -7.95696378e-02, -1.98505402e-01, -1.80687919e-01,\n",
       "            2.75660813e-01],\n",
       "          [ 3.42836455e-02,  5.95946684e-02, -3.06958407e-01,\n",
       "           -2.24552471e-02,  4.51512774e-03, -3.62139270e-02,\n",
       "            2.74530891e-02,  2.32791808e-02,  3.05624064e-02,\n",
       "            9.99021679e-02,  2.28347719e-01,  2.79849172e-01,\n",
       "            1.61748871e-01,  1.37693241e-01, -1.21562555e-02,\n",
       "            1.51589096e-01],\n",
       "          [-2.44649351e-01, -2.15989456e-01, -3.45033407e-02,\n",
       "            2.13425636e-01, -2.00467318e-01,  1.86012983e-01,\n",
       "           -2.97431111e-01, -8.01651776e-02, -4.39755991e-02,\n",
       "           -2.48229265e-01,  1.66971862e-01, -1.35215402e-01,\n",
       "            2.71804612e-02, -1.77172348e-01,  4.86521758e-02,\n",
       "           -9.39043164e-02],\n",
       "          [-1.03190988e-01, -1.06125176e-01, -1.85240686e-01,\n",
       "           -1.08102381e-01,  2.12299526e-01,  1.59649462e-01,\n",
       "           -8.26811790e-02,  4.69897389e-02, -1.77855968e-01,\n",
       "           -1.02210104e-01, -3.03770304e-02,  1.43164039e-01,\n",
       "           -1.89415187e-01,  5.13969660e-02,  1.81885898e-01,\n",
       "           -2.03209221e-02],\n",
       "          [ 2.00016171e-01, -2.08720714e-01, -1.73292637e-01,\n",
       "           -3.09762418e-01, -3.88786420e-02, -2.52924591e-01,\n",
       "           -9.88234878e-02, -6.76421896e-02, -2.33143821e-01,\n",
       "           -8.06510746e-02,  2.91357011e-01, -6.55159950e-02,\n",
       "           -2.35292032e-01, -3.08387127e-04, -3.36518019e-01,\n",
       "            1.91818953e-01],\n",
       "          [ 1.77097201e-01,  1.96233511e-01,  1.51748538e-01,\n",
       "            1.69725418e-02,  2.11882412e-01,  3.55434082e-02,\n",
       "           -2.17716694e-01,  1.36195898e-01, -7.52553344e-02,\n",
       "            2.28503406e-01,  8.28370452e-02, -1.22151971e-01,\n",
       "           -2.64889002e-03, -2.22547770e-01,  1.81061506e-01,\n",
       "            8.08202624e-02]]]], dtype=float32)>]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(parameters.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 6), dtype=float32, numpy=\n",
       "array([[ 0.23551293, -0.02307653, -0.15652823,  1.9143677 , -0.63153493,\n",
       "        -2.7213254 ],\n",
       "       [-0.08194128, -0.04931334,  0.03292936,  1.6985412 , -0.2843245 ,\n",
       "        -2.7015092 ],\n",
       "       [-0.00704841, -0.22170335,  0.09864151,  1.9287674 , -0.3643383 ,\n",
       "        -2.923205  ],\n",
       "       [ 0.47276253, -0.08465371, -0.42349732,  1.8229952 , -0.66703653,\n",
       "        -2.6085067 ]], dtype=float32)>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**: although it may not match perfectly, your expected output should be close to ours and your cost value should decrease.\n",
    "\n",
    "<table> \n",
    "<tr>\n",
    "    <td> \n",
    "    **Cost after epoch 0 =**\n",
    "    </td>\n",
    "\n",
    "    <td> \n",
    "      1.917929\n",
    "    </td> \n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **Cost after epoch 5 =**\n",
    "    </td>\n",
    "\n",
    "    <td> \n",
    "      1.506757\n",
    "    </td> \n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **Train Accuracy   =**\n",
    "    </td>\n",
    "\n",
    "    <td> \n",
    "      0.940741\n",
    "    </td> \n",
    "</tr> \n",
    "\n",
    "<tr>\n",
    "    <td> \n",
    "    **Test Accuracy   =**\n",
    "    </td>\n",
    "\n",
    "    <td> \n",
    "      0.783333\n",
    "    </td> \n",
    "</tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have finished the assignment and built a model that recognizes SIGN language with almost 80% accuracy on the test set. If you wish, feel free to play around with this dataset further. You can actually improve its accuracy by spending more time tuning the hyperparameters, or using regularization (as this model clearly has a high variance). \n",
    "\n",
    "Once again, here's a thumbs up for your work! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[ 0.02967666,  0.01588823, -0.00883911, -0.05909321, -0.02914246, -0.00388799],\n",
    " [ 0.01311273,  0.01452304, -0.01275194, -0.03981852, -0.01704192,  0.00923324],\n",
    " [ 0.02188194, -0.00263658,-0.00437032, -0.03100904, -0.00459759, -0.00280901],\n",
    " [ 0.01739136,  0.00017546,  0.00341422, -0.02361486, -0.00927886,  0.01297478],\n",
    " [ 0.02191152,  0.00946656, -0.01157256, -0.04684199, -0.01396238,  0.00366075],\n",
    " [ 0.02457446,  0.00387859, -0.00148149, -0.02405689,  0.00068018, -0.00595315]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=int64, numpy=array([0, 1, 0, 0, 0, 0], dtype=int64)>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.argmax(a, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy.ndimage' has no attribute 'imread'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-1d83fd645d8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"images/thumbs_up.jpg\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmy_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmisc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'scipy.ndimage' has no attribute 'imread'"
     ]
    }
   ],
   "source": [
    "fname = \"images/thumbs_up.jpg\"\n",
    "image = np.array(ndimage.imread(fname, flatten=False))\n",
    "my_image = scipy.misc.imresize(image, size=(64,64))\n",
    "plt.imshow(my_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "bwbJV",
   "launcher_item_id": "0TkXB"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
