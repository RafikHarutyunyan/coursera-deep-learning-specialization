{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zVtw6n7bT110"
   },
   "source": [
    "# PyTorch\n",
    "\n",
    "- Initialize variables\n",
    "- Train algorithms \n",
    "- Implement a Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Exploring the Tensorflow Library\n",
    "\n",
    "To start, you will import the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rhZ0RUw8T111"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helping functions\n",
    "\n",
    "- load_dataset\n",
    "- random_mini_batches\n",
    "- convert_to_one_hot\n",
    "- predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_dataset = h5py.File('datasets/train_signs.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_signs.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "    \n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y\n",
    "\n",
    "\n",
    "def predict(X, parameters):\n",
    "    \n",
    "    W1 = torch.tensor(parameters[\"W1\"])\n",
    "    b1 = torch.tensor(parameters[\"b1\"])\n",
    "    W2 = torch.tensor(parameters[\"W2\"])\n",
    "    b2 = torch.tensor(parameters[\"b2\"])\n",
    "    W3 = torch.tensor(parameters[\"W3\"])\n",
    "    b3 = torch.tensor(parameters[\"b3\"])\n",
    "    \n",
    "    params = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3}\n",
    "    \n",
    "    z3 = forward_propagation_for_predict(X, params)\n",
    "    prediction = torch.argmax(z3)\n",
    "        \n",
    "    return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A1vVKBCQT114"
   },
   "source": [
    "Now that you have imported the library, we will walk you through its different applications. You will start with an example, where we compute for you the loss of one training example. \n",
    "$$loss = \\mathcal{L}(\\hat{y}, y) = (\\hat y^{(i)} - y^{(i)})^2 \\tag{1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JKAjoAbjT115"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = torch.tensor(36)            # Define y_hat constant. Set to 36.\n",
    "y = torch.tensor(39)                    # Define y. Set to 39\n",
    "\n",
    "loss = (y - y_hat) ** 2\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X15wlMDUT12D"
   },
   "source": [
    "### 1.1 - Linear function\n",
    "\n",
    "Lets start this programming exercise by computing the following equation: $Y = WX + b$, where $W$ and $X$ are random matrices and b is a random vector. \n",
    "\n",
    "**Exercise**: Compute $WX + b$ where $W, X$, and $b$ are drawn from a random normal distribution. W is of shape (4, 3), X is (3,1) and b is (4,1). As an example, here is how you would define a constant X that has shape (3,1):\n",
    "```python\n",
    "X = tf.constant(np.random.randn(3,1), name = \"X\")\n",
    "\n",
    "```\n",
    "You might find the following functions helpful: \n",
    "- tf.matmul(..., ...) to do a matrix multiplication\n",
    "- tf.add(..., ...) to do an addition\n",
    "- np.random.randn(...) to initialize randomly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ww5sBoFbT12D"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_function\n",
    "\n",
    "def linear_function():\n",
    "    \"\"\"\n",
    "    Implements a linear function: \n",
    "            Initializes X to be a random tensor of shape (3,1)\n",
    "            Initializes W to be a random tensor of shape (4,3)\n",
    "            Initializes b to be a random tensor of shape (4,1)\n",
    "    Returns: \n",
    "    result -- runs the session for Y = WX + b \n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    \"\"\"\n",
    "    Note, to ensure that the \"random\" numbers generated match the expected results,\n",
    "    please create the variables in the order given in the starting code below.\n",
    "    (Do not re-arrange the order).\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (4 lines of code)\n",
    "    X = torch.tensor(np.random.randn(3,1))\n",
    "    W = torch.tensor(np.random.randn(4,3))\n",
    "    b = torch.tensor(np.random.randn(4,1))\n",
    "    Y = torch.tensor(np.random.randn(4,1))\n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    # Create the session using tf.Session() and run it with sess.run(...) on the variable you want to calculate\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    result = torch.add(torch.matmul(W, X), b)\n",
    "\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3gOryVQT12G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = \n",
      "tensor([[-2.1566],\n",
      "        [ 2.9589],\n",
      "        [-1.0893],\n",
      "        [-0.8454]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print( \"result = \\n\" + str(linear_function()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R5netQ9IT12J"
   },
   "source": [
    "*** Expected Output ***: \n",
    "\n",
    "```\n",
    "result = \n",
    "[[-2.15657382]\n",
    " [ 2.95891446]\n",
    " [-1.08926781]\n",
    " [-0.84538042]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DUBum-E4T12K"
   },
   "source": [
    "### 1.2 - Computing the sigmoid \n",
    "Great! You just implemented a linear function. Tensorflow offers a variety of commonly used neural network functions like `tf.sigmoid` and `tf.softmax`. For this exercise lets compute the sigmoid function of an input. \n",
    "\n",
    "You will do this exercise using a placeholder variable `x`. When running the session, you should use the feed dictionary to pass in the input `z`. In this exercise, you will have to (i) create a placeholder `x`, (ii) define the operations needed to compute the sigmoid using `tf.sigmoid`, and then (iii) run the session. \n",
    "\n",
    "** Exercise **: Implement the sigmoid function below. You should use the following: \n",
    "\n",
    "- `tf.placeholder(tf.float32, name = \"...\")`\n",
    "- `tf.sigmoid(...)`\n",
    "- `sess.run(..., feed_dict = {x: z})`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "APv9bW9rT12K"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid of z\n",
    "    \n",
    "    Arguments:\n",
    "    z -- input value, scalar or vector\n",
    "    \n",
    "    Returns: \n",
    "    results -- the sigmoid of z\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    if type(z) == 'nd.array':\n",
    "        z = torch.from_numpy(z)\n",
    "        z = z.float()\n",
    "    else:\n",
    "        z = torch.tensor(z)\n",
    "        \n",
    "    result = torch.sigmoid(z)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLHdJxKVT12M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) = tensor(0.5000)\n",
      "sigmoid(12) = tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid(0) = \" + str(sigmoid(0)))\n",
    "print (\"sigmoid(12) = \" + str(sigmoid(12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4cl8Wgg9T12O"
   },
   "source": [
    "*** Expected Output ***: \n",
    "\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "**sigmoid(0)**\n",
    "</td>\n",
    "<td>\n",
    "0.5\n",
    "</td>\n",
    "</tr>\n",
    "<tr> \n",
    "<td>\n",
    "**sigmoid(12)**\n",
    "</td>\n",
    "<td>\n",
    "0.999994\n",
    "</td>\n",
    "</tr> \n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ytSt0fgTT12P"
   },
   "source": [
    "### 1.3 -  Computing the Cost\n",
    "\n",
    "You can also use a built-in function to compute the cost of your neural network. So instead of needing to write code to compute this as a function of $a^{[2](i)}$ and $y^{(i)}$ for i=1...m: \n",
    "$$ J = - \\frac{1}{m}  \\sum_{i = 1}^m  \\large ( \\small y^{(i)} \\log a^{ [2] (i)} + (1-y^{(i)})\\log (1-a^{ [2] (i)} )\\large )\\small\\tag{2}$$\n",
    "\n",
    "you can do it in one line of code in tensorflow!\n",
    "\n",
    "**Exercise**: Implement the cross entropy loss. The function you will use is: \n",
    "\n",
    "\n",
    "- `tf.nn.sigmoid_cross_entropy_with_logits(logits = ...,  labels = ...)`\n",
    "\n",
    "Your code should input `z`, compute the sigmoid (to get `a`) and then compute the cross entropy cost $J$. All this can be done using one call to `tf.nn.sigmoid_cross_entropy_with_logits`, which computes\n",
    "\n",
    "$$- \\frac{1}{m}  \\sum_{i = 1}^m  \\large ( \\small y^{(i)} \\log \\sigma(z^{[2](i)}) + (1-y^{(i)})\\log (1-\\sigma(z^{[2](i)})\\large )\\small\\tag{2}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oIRdDYOLT12P"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: cost\n",
    "\n",
    "def cost(logits, labels):\n",
    "    \"\"\"\n",
    "    Computes the cost using the sigmoid cross entropy\n",
    "    \n",
    "    Arguments:\n",
    "    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)\n",
    "    labels -- vector of labels y (1 or 0) \n",
    "    \n",
    "    Note: What we've been calling \"z\" and \"y\" in this class are respectively called \"logits\" and \"labels\" \n",
    "    in the TensorFlow documentation. So logits will feed into z, and labels into y. \n",
    "    \n",
    "    Returns:\n",
    "    cost -- runs the session of the cost (formula (2))\n",
    "    \"\"\"\n",
    "    \n",
    "    logits = torch.tensor(logits)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    cost = labels*-torch.log(torch.sigmoid(logits)) + (1-labels)*-torch.log(1-torch.sigmoid(logits))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nPB-lOYT12R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = tensor([0.7981, 0.9130, 0.4032, 0.3412], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "logits = np.array([0.2,0.4,0.7,0.9])\n",
    "\n",
    "cost = cost(logits, np.array([0,0,1,1]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X8sMySzyT12T"
   },
   "source": [
    "** Expected Output** : \n",
    "\n",
    "```\n",
    "cost = [ 0.79813886  0.91301525  0.40318605  0.34115386]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_sK1Rqm6T12U"
   },
   "source": [
    "### 1.4 - Using One Hot encodings\n",
    "\n",
    "Many times in deep learning you will have a y vector with numbers ranging from 0 to C-1, where C is the number of classes. If C is for example 4, then you might have the following y vector which you will need to convert as follows:\n",
    "\n",
    "\n",
    "<img src=\"images/onehot.png\" style=\"width:600px;height:150px;\">\n",
    "\n",
    "This is called a \"one hot\" encoding, because in the converted representation exactly one element of each column is \"hot\" (meaning set to 1). To do this conversion in numpy, you might have to write a few lines of code. In tensorflow, you can use one line of code: \n",
    "\n",
    "- tf.one_hot(labels, depth, axis) \n",
    "\n",
    "**Exercise:** Implement the function below to take one vector of labels and the total number of classes $C$, and return the one hot encoding. Use `tf.one_hot()` to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dlamXLu_T12U"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: one_hot_matrix\n",
    "\n",
    "def one_hot_matrix(labels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)\n",
    "    C = torch.tensor(C)\n",
    "    \n",
    "    # Use tf.one_hot, be careful with the axis (approx. 1 line)\n",
    "    one_hot = convert_to_one_hot(labels, C)\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Bi0je2yT12W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot = \n",
      "[[0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([1,2,3,0,2,1])\n",
    "one_hot = one_hot_matrix(labels, C = 4)\n",
    "print (\"one_hot = \\n\" + str(one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HlT0UczrT12Y"
   },
   "source": [
    "**Expected Output**: \n",
    "\n",
    "```\n",
    "one_hot = \n",
    "[[ 0.  0.  0.  1.  0.  0.]\n",
    " [ 1.  0.  0.  0.  0.  1.]\n",
    " [ 0.  1.  0.  0.  1.  0.]\n",
    " [ 0.  0.  1.  0.  0.  0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qsu1xyqFT12Z"
   },
   "source": [
    "### 1.5 - Initialize with zeros and ones\n",
    "\n",
    "Now you will learn how to initialize a vector of zeros and ones. The function you will be calling is `tf.ones()`. To initialize with zeros you could use tf.zeros() instead. These functions take in a shape and return an array of dimension shape full of zeros and ones respectively. \n",
    "\n",
    "**Exercise:** Implement the function below to take in a shape and to return an array (of the shape's dimension of ones). \n",
    "\n",
    " - tf.ones(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eOVWrcR2T12Z"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: ones\n",
    "\n",
    "def ones(shape):\n",
    "    \"\"\"\n",
    "    Creates an array of ones of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    shape -- shape of the array you want to create\n",
    "        \n",
    "    Returns: \n",
    "    ones -- array containing only ones\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Create \"ones\" tensor using tf.ones(...). (approx. 1 line)\n",
    "    ones = torch.ones(shape)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwHEVDv6T12b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones = tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print (\"ones = \" + str(ones([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hGgM2hSFT12g"
   },
   "source": [
    "**Expected Output:**\n",
    "\n",
    "<table> \n",
    "    <tr> \n",
    "        <td>\n",
    "            **ones**\n",
    "        </td>\n",
    "        <td>\n",
    "        [ 1.  1.  1.]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LW8S6sVzT12h"
   },
   "source": [
    "# 2 - Building your first neural network in tensorflow\n",
    "\n",
    "In this part of the assignment you will build a neural network using tensorflow. Remember that there are two parts to implement a tensorflow model:\n",
    "\n",
    "- Create the computation graph\n",
    "- Run the graph\n",
    "\n",
    "Let's delve into the problem you'd like to solve!\n",
    "\n",
    "### 2.0 - Problem statement: SIGNS Dataset\n",
    "\n",
    "One afternoon, with some friends we decided to teach our computers to decipher sign language. We spent a few hours taking pictures in front of a white wall and came up with the following dataset. It's now your job to build an algorithm that would facilitate communications from a speech-impaired person to someone who doesn't understand sign language.\n",
    "\n",
    "- **Training set**: 1080 pictures (64 by 64 pixels) of signs representing numbers from 0 to 5 (180 pictures per number).\n",
    "- **Test set**: 120 pictures (64 by 64 pixels) of signs representing numbers from 0 to 5 (20 pictures per number).\n",
    "\n",
    "Note that this is a subset of the SIGNS dataset. The complete dataset contains many more signs.\n",
    "\n",
    "Here are examples for each number, and how an explanation of how we represent the labels. These are the original pictures, before we lowered the image resolutoion to 64 by 64 pixels.\n",
    "<img src=\"images/hands.png\" style=\"width:800px;height:350px;\"><caption><center> <u><font color='purple'> **Figure 1**</u><font color='purple'>: SIGNS dataset <br> <font color='black'> </center>\n",
    "\n",
    "\n",
    "Run the following code to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wCgjv84yT12i"
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JYimgnMbT12k"
   },
   "source": [
    "Change the index below and run the cell to visualize some examples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wG0QwVtJT12k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 0\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2WP4-S2CT12m"
   },
   "source": [
    "As usual you flatten the image dataset, then normalize it by dividing by 255. On top of that, you will convert each label to a one-hot vector as shown in Figure 1. Run the cell below to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tn3gF5xLT12m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (12288, 1080)\n",
      "Y_train shape: (6, 1080)\n",
      "X_test shape: (12288, 120)\n",
      "Y_test shape: (6, 120)\n"
     ]
    }
   ],
   "source": [
    "# Flatten the training and test images\n",
    "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
    "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
    "# Normalize image vectors\n",
    "X_train = X_train_flatten/255.\n",
    "X_test = X_test_flatten/255.\n",
    "# Convert training and test labels to one hot matrices\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6)\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6)\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[1]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iN_KPZ0FT12o"
   },
   "source": [
    "**Note** that 12288 comes from $64 \\times 64 \\times 3$. Each image is square, 64 by 64 pixels, and 3 is for the RGB colors. Please make sure all these shapes make sense to you before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_GQMSJTtT12p"
   },
   "source": [
    "**Your goal** is to build an algorithm capable of recognizing a sign with high accuracy. To do so, you are going to build a tensorflow model that is almost the same as one you have previously built in numpy for cat recognition (but now using a softmax output). It is a great occasion to compare your numpy implementation to the tensorflow one. \n",
    "\n",
    "**The model** is *LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX*. The SIGMOID output layer has been converted to a SOFTMAX. A SOFTMAX layer generalizes SIGMOID to when there are more than two classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eyYz9y1XT12u"
   },
   "source": [
    "### 2.2 - Initializing the parameters\n",
    "\n",
    "Your second task is to initialize the parameters in tensorflow.\n",
    "\n",
    "**Exercise:** Implement the function below to initialize the parameters in tensorflow. You are going use Xavier Initialization for weights and Zero Initialization for biases. The shapes are given below. As an example, to help you, for W1 and b1 you could use: \n",
    "\n",
    "```python\n",
    "W1 = tf.get_variable(\"W1\", [25,12288], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n",
    "```\n",
    "Please use `seed = 1` to make sure your results match ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gPi-SeuWT12u"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [25, 12288]\n",
    "                        b1 : [25, 1]\n",
    "                        W2 : [12, 25]\n",
    "                        b2 : [12, 1]\n",
    "                        W3 : [6, 12]\n",
    "                        b3 : [6, 1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "\n",
    "        \n",
    "    ### START CODE HERE ### (approx. 6 lines of code)\n",
    "    torch.nn.init.xavier_uniform_(torch.ones(25,12288))\n",
    "    W1 = torch.nn.init.xavier_uniform_(torch.ones(25,12288))\n",
    "    b1 = torch.nn.init.xavier_uniform_(torch.ones(25,1))\n",
    "    W2 = torch.nn.init.xavier_uniform_(torch.ones(12,25))\n",
    "    b2 = torch.nn.init.xavier_uniform_(torch.ones(12,1))\n",
    "    W3 = torch.nn.init.xavier_uniform_(torch.ones(6,12))\n",
    "    b3 = torch.nn.init.xavier_uniform_(torch.ones(6,1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    W1 = W1.clone().detach().requires_grad_(True)\n",
    "    b1 = b1.clone().detach().requires_grad_(True)\n",
    "    W2 = W2.clone().detach().requires_grad_(True)\n",
    "    b2 = b2.clone().detach().requires_grad_(True)\n",
    "    W3 = W3.clone().detach().requires_grad_(True)\n",
    "    b3 = b3.clone().detach().requires_grad_(True)\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CcuKNYinT12x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = tensor([[ 0.0077,  0.0089,  0.0205,  ...,  0.0097, -0.0064,  0.0197],\n",
      "        [-0.0130,  0.0126, -0.0150,  ..., -0.0036,  0.0189,  0.0165],\n",
      "        [-0.0177, -0.0015,  0.0075,  ..., -0.0065, -0.0211, -0.0034],\n",
      "        ...,\n",
      "        [-0.0046,  0.0137,  0.0106,  ..., -0.0106, -0.0135,  0.0219],\n",
      "        [ 0.0195, -0.0101,  0.0066,  ..., -0.0112, -0.0116,  0.0101],\n",
      "        [ 0.0063, -0.0127, -0.0034,  ..., -0.0107,  0.0151,  0.0022]],\n",
      "       requires_grad=True)\n",
      "b1 = tensor([[-0.0436],\n",
      "        [-0.3277],\n",
      "        [-0.0512],\n",
      "        [-0.1573],\n",
      "        [ 0.3179],\n",
      "        [ 0.3237],\n",
      "        [-0.1571],\n",
      "        [ 0.0350],\n",
      "        [ 0.0449],\n",
      "        [-0.4632],\n",
      "        [-0.2352],\n",
      "        [-0.3526],\n",
      "        [ 0.4610],\n",
      "        [ 0.3014],\n",
      "        [ 0.4402],\n",
      "        [ 0.2857],\n",
      "        [ 0.0203],\n",
      "        [-0.0975],\n",
      "        [ 0.0913],\n",
      "        [-0.3378],\n",
      "        [-0.4601],\n",
      "        [ 0.3875],\n",
      "        [-0.3668],\n",
      "        [ 0.4721],\n",
      "        [ 0.2283]], requires_grad=True)\n",
      "W2 = tensor([[ 1.9156e-01,  1.5547e-01,  1.8643e-01, -1.3728e-01, -3.4408e-01,\n",
      "          3.5642e-02, -5.4882e-02,  2.3873e-01, -5.0277e-02, -2.8331e-01,\n",
      "          3.3167e-01,  2.5308e-01, -3.3363e-01, -9.2525e-02,  2.7305e-02,\n",
      "         -6.3211e-02, -2.9758e-01,  3.7450e-01, -2.8191e-01, -1.7534e-01,\n",
      "          3.1778e-01,  3.8467e-01, -3.0182e-01,  1.6422e-01, -1.4521e-02],\n",
      "        [-2.7034e-01,  1.1126e-02,  5.6726e-02,  3.4995e-01, -1.0439e-01,\n",
      "         -2.3876e-01,  1.8821e-01,  2.3986e-01,  2.6782e-02,  2.0960e-04,\n",
      "         -1.9761e-01, -3.8477e-01, -1.3049e-01, -2.0454e-01,  3.3379e-01,\n",
      "         -9.1474e-02,  2.2849e-01,  4.0205e-01,  2.0623e-01, -1.3080e-01,\n",
      "          1.9857e-01,  2.1746e-01,  3.3507e-01, -1.0683e-01, -1.0214e-02],\n",
      "        [ 9.8332e-02, -1.5557e-01, -1.2152e-01,  2.6608e-01,  3.7941e-01,\n",
      "         -5.0519e-02,  7.4059e-05, -1.2542e-01, -3.9554e-01, -1.9642e-01,\n",
      "         -5.6224e-02, -3.2785e-01, -3.9357e-01, -1.4353e-01,  3.2599e-01,\n",
      "         -3.4673e-01,  2.6489e-01, -2.7843e-01, -3.8221e-01,  2.8686e-01,\n",
      "          3.0729e-01, -1.1441e-01, -3.3626e-01, -2.8486e-01,  3.4634e-01],\n",
      "        [ 1.5131e-01,  3.2437e-04,  3.5863e-01,  3.6345e-01, -4.0179e-02,\n",
      "          3.3328e-01, -1.6514e-01,  7.7157e-02,  2.5439e-01,  3.6415e-01,\n",
      "          2.2748e-01, -2.3657e-01, -2.2907e-01,  3.7076e-02,  1.5770e-01,\n",
      "          3.8911e-01, -2.1714e-01, -2.2467e-02,  6.2193e-02,  2.3827e-01,\n",
      "         -9.4076e-02,  2.3307e-01,  7.4444e-02, -1.2132e-01, -3.7538e-01],\n",
      "        [ 1.5600e-01,  2.2120e-01, -2.1193e-01, -8.6497e-02,  5.7902e-02,\n",
      "         -2.0041e-01, -3.3697e-01,  9.5341e-02,  3.9088e-01,  3.0502e-01,\n",
      "         -2.2427e-01, -6.7284e-03,  1.1220e-01,  2.5913e-01,  3.4332e-01,\n",
      "         -3.7670e-01, -1.2538e-01,  1.7510e-01, -2.2704e-01, -3.4302e-02,\n",
      "          2.0576e-01, -3.0740e-01, -3.7267e-01,  3.7102e-03,  1.3901e-01],\n",
      "        [ 6.4136e-02,  2.5283e-01,  2.4783e-01,  2.9543e-01,  9.2163e-02,\n",
      "          6.8643e-02, -3.2311e-01, -3.3590e-01,  1.3681e-02, -1.5617e-02,\n",
      "          2.8815e-01,  3.7672e-01, -2.4495e-01,  4.3504e-02,  2.4853e-01,\n",
      "          2.9732e-01,  3.7058e-01,  8.1414e-02,  2.1673e-01, -2.3395e-01,\n",
      "          1.7757e-01, -6.5392e-02, -1.7584e-01,  3.1131e-01,  1.3738e-01],\n",
      "        [-1.0509e-01, -3.0926e-01,  3.1432e-01, -3.9081e-01,  1.7752e-01,\n",
      "          1.5175e-01,  3.1401e-01,  3.2372e-01,  2.9901e-01, -3.2118e-01,\n",
      "         -3.7480e-01, -3.7198e-01, -3.0657e-01, -3.7656e-01, -1.7662e-01,\n",
      "         -2.8064e-01, -1.7294e-01,  3.7261e-01, -3.6728e-01, -2.4912e-01,\n",
      "          1.6732e-02, -1.0780e-01, -1.1501e-01,  8.0470e-02, -3.4936e-01],\n",
      "        [-1.1004e-01, -3.8839e-01, -3.1607e-02, -6.8936e-02,  3.5126e-01,\n",
      "         -2.0561e-01, -2.7066e-01,  8.8627e-02, -1.8422e-01,  2.6023e-01,\n",
      "          1.7328e-01, -9.5825e-02,  2.8381e-01, -2.0250e-01,  1.5599e-01,\n",
      "         -3.0639e-01,  2.5849e-01, -1.9977e-01, -6.7103e-02,  2.6215e-01,\n",
      "         -3.5507e-01,  2.9264e-02,  3.1375e-01,  1.3717e-01, -5.3077e-02],\n",
      "        [ 1.6524e-01, -4.6226e-02, -2.5076e-01, -4.3849e-02,  6.3082e-02,\n",
      "         -3.6009e-01,  2.1895e-01,  3.4068e-01, -2.2138e-01,  3.9947e-01,\n",
      "          1.4473e-01, -8.8698e-02,  2.4408e-01, -1.2614e-01, -3.6575e-01,\n",
      "          3.0537e-01, -2.2708e-01,  7.4576e-02, -1.8196e-01,  3.8771e-01,\n",
      "         -2.2686e-01,  3.6404e-01,  2.5927e-02,  2.1456e-01,  3.3447e-01],\n",
      "        [ 2.1858e-01, -2.8077e-01, -1.4709e-01,  3.8137e-01, -4.0228e-01,\n",
      "         -1.6429e-01, -1.5845e-01, -4.0949e-02,  1.7432e-01, -3.4757e-01,\n",
      "          2.1382e-01, -3.3109e-01,  1.1681e-01,  1.9936e-02,  3.4507e-01,\n",
      "          2.5735e-01, -8.8509e-02, -4.7537e-02, -3.4396e-01, -3.0254e-01,\n",
      "          3.5745e-01, -1.4920e-01, -2.4459e-01, -2.1995e-01,  2.2206e-02],\n",
      "        [-1.6492e-02, -9.9721e-02, -1.7631e-02,  9.3708e-02,  3.5951e-01,\n",
      "          1.5582e-01,  2.4331e-02, -1.0022e-01, -3.0902e-01, -3.0457e-01,\n",
      "         -1.3475e-01, -3.3977e-01, -8.6270e-02,  2.3327e-01, -1.6992e-01,\n",
      "         -3.3282e-01,  2.9586e-01, -6.4354e-02, -2.8748e-01, -1.6313e-01,\n",
      "         -5.0387e-02,  2.9870e-01,  2.9629e-01, -1.4926e-01, -3.0391e-01],\n",
      "        [-5.8554e-02, -2.8566e-01,  2.6641e-01,  1.4835e-01,  2.9369e-01,\n",
      "          4.4751e-02,  1.1308e-01,  2.6254e-01,  3.9958e-01,  2.0594e-01,\n",
      "         -1.7044e-01, -6.5774e-02, -2.8662e-01, -9.1997e-02,  2.3693e-02,\n",
      "          9.5342e-03, -1.3586e-01, -3.2011e-02, -1.9928e-01,  1.6303e-02,\n",
      "          1.2178e-01,  1.3534e-01, -2.7074e-01,  3.1303e-01,  3.5631e-01]],\n",
      "       requires_grad=True)\n",
      "b2 = tensor([[ 0.1352],\n",
      "        [ 0.4452],\n",
      "        [-0.6047],\n",
      "        [-0.6694],\n",
      "        [ 0.2794],\n",
      "        [ 0.4542],\n",
      "        [-0.2815],\n",
      "        [ 0.3066],\n",
      "        [-0.0307],\n",
      "        [ 0.1106],\n",
      "        [ 0.1131],\n",
      "        [ 0.2145]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters()\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kzAVM5y8T12z"
   },
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table> \n",
    "    <tr> \n",
    "        <td>\n",
    "            **W1**\n",
    "        </td>\n",
    "        <td>\n",
    "         < tf.Variable 'W1:0' shape=(25, 12288) dtype=float32_ref >\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr> \n",
    "        <td>\n",
    "            **b1**\n",
    "        </td>\n",
    "        <td>\n",
    "        < tf.Variable 'b1:0' shape=(25, 1) dtype=float32_ref >\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr> \n",
    "        <td>\n",
    "            **W2**\n",
    "        </td>\n",
    "        <td>\n",
    "        < tf.Variable 'W2:0' shape=(12, 25) dtype=float32_ref >\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr> \n",
    "        <td>\n",
    "            **b2**\n",
    "        </td>\n",
    "        <td>\n",
    "        < tf.Variable 'b2:0' shape=(12, 1) dtype=float32_ref >\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IR5UvbGxT12z"
   },
   "source": [
    "As expected, the parameters haven't been evaluated yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cnuAGFn2T120"
   },
   "source": [
    "### 2.3 - Forward propagation in tensorflow \n",
    "\n",
    "You will now implement the forward propagation module in tensorflow. The function will take in a dictionary of parameters and it will complete the forward pass. The functions you will be using are: \n",
    "\n",
    "- `tf.add(...,...)` to do an addition\n",
    "- `tf.matmul(...,...)` to do a matrix multiplication\n",
    "- `tf.nn.relu(...)` to apply the ReLU activation\n",
    "\n",
    "**Question:** Implement the forward pass of the neural network. We commented for you the numpy equivalents so that you can compare the tensorflow implementation to numpy. It is important to note that the forward propagation stops at `z3`. The reason is that in tensorflow the last linear layer output is given as input to the function computing the loss. Therefore, you don't need `a3`!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nC7CYNk0T120"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    X = torch.tensor(X).float()\n",
    "        \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "                                                           # Numpy Equivalents:\n",
    "    Z1 = torch.add(torch.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n",
    "    activation = torch.nn.ReLU()\n",
    "    A1 = activation(Z1)                                    # A1 = relu(Z1)\n",
    "    Z2 = torch.add(torch.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = activation(Z2)                                    # A2 = relu(Z2)\n",
    "    Z3 = torch.add(torch.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hioQQqyxT122",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z3 = tensor([[-0.7308, -0.5509, -0.6863,  ..., -0.8974, -0.7495, -0.8575],\n",
      "        [ 0.6114,  0.2149,  0.3349,  ...,  0.4078,  0.2126,  0.4788],\n",
      "        [-1.2890, -1.5103, -1.4751,  ..., -1.5806, -1.6854, -1.4728],\n",
      "        [-1.1336, -1.3901, -1.3646,  ..., -1.4058, -1.5938, -1.3775],\n",
      "        [-1.4380, -1.5664, -1.6194,  ..., -1.7317, -1.7472, -1.6496],\n",
      "        [-0.0250, -0.1104, -0.0832,  ..., -0.1239, -0.2125, -0.1786]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters = initialize_parameters()\n",
    "Z3 = forward_propagation(X_train, parameters)\n",
    "print(\"Z3 = \" + str(Z3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRrS7RzpT124"
   },
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table> \n",
    "    <tr> \n",
    "        <td>\n",
    "            **Z3**\n",
    "        </td>\n",
    "        <td>\n",
    "        Tensor(\"Add_2:0\", shape=(6, ?), dtype=float32)\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FDjgAHp6T125"
   },
   "source": [
    "You may have noticed that the forward propagation doesn't output any cache. You will understand why below, when we get to brackpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RXqHnAEnT125"
   },
   "source": [
    "### 2.4 Compute cost\n",
    "\n",
    "As seen before, it is very easy to compute the cost using:\n",
    "```python\n",
    "tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = ..., labels = ...))\n",
    "```\n",
    "**Question**: Implement the cost function below. \n",
    "- It is important to know that the \"`logits`\" and \"`labels`\" inputs of `tf.nn.softmax_cross_entropy_with_logits` are expected to be of shape (number of examples, num_classes). We have thus transposed Z3 and Y for you.\n",
    "- Besides, `tf.reduce_mean` basically does the summation over the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1_bzQXSJT125"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost \n",
    "\n",
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "\n",
    "    Y = torch.tensor(Y)\n",
    "    \n",
    "    cost = torch.mean(Y*-torch.log(torch.sigmoid(Z3)) + (1-Y)*-torch.log(1-torch.sigmoid(Z3)))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HahBCJVT127"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = tensor(0.6800, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters()\n",
    "Z3 = forward_propagation(X_train, parameters)\n",
    "cost = compute_cost(Z3, Y_train)\n",
    "print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GT7MzPxET12-"
   },
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table> \n",
    "    <tr> \n",
    "        <td>\n",
    "            **cost**\n",
    "        </td>\n",
    "        <td>\n",
    "        Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True, weight_decay = 1e-5):\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    X_train = torch.tensor(X_train).float()\n",
    "    Y_train = torch.tensor(Y_train)\n",
    "    X_test = torch.tensor(X_test).float()\n",
    "    Y_test = torch.tensor(Y_test)\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters()\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = torch.optim.Adam([W1,b1,W2,b2,W3,b3], lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                X = minibatch_X\n",
    "                Y = minibatch_Y\n",
    "                \n",
    "                Z3 = forward_propagation(X, parameters)\n",
    "                loss_value = compute_cost(Z3, Y)\n",
    "                minibatch_cost = loss_value\n",
    "                optimizer.zero_grad()\n",
    "                loss_value.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "                print(Z3)\n",
    "                print(parameters['W1'])\n",
    "                print(parameters['W2'])\n",
    "                print(parameters['W3'])\n",
    "                \n",
    "                Z3_train = forward_propagation(X_train, parameters)\n",
    "                correct_prediction = torch.tensor((torch.argmax(Z3_train, dim = 0) == torch.argmax(Y_train, dim = 0)), dtype=torch.float)\n",
    "                accuracy = torch.mean(correct_prediction)\n",
    "                print(\"Train: \", accuracy)\n",
    "                \n",
    "                Z3_test = forward_propagation(X_test, parameters)\n",
    "                correct_prediction = torch.tensor((torch.argmax(Z3_test, dim = 0) == torch.argmax(Y_test, dim = 0)), dtype=torch.float)\n",
    "                accuracy = torch.mean(correct_prediction)\n",
    "                print(\"Test: \", accuracy)\n",
    "                \n",
    "                \n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "\n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per fives)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    # lets save the parameters in a variable\n",
    "    print (\"Parameters have been trained!\")\n",
    "\n",
    "    Z3_train = forward_propagation(X_train, parameters)\n",
    "    correct_prediction = torch.tensor((torch.argmax(Z3_train, dim = 0) == torch.argmax(Y_train, dim = 0)), dtype=torch.float)\n",
    "    accuracy = torch.mean(correct_prediction)\n",
    "    print(\"Test: \", accuracy)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Z3_test = forward_propagation(X_test, parameters)\n",
    "    correct_prediction = torch.tensor((torch.argmax(Z3_test, dim = 0) == torch.argmax(Y_test, dim = 0)), dtype=torch.float)\n",
    "    accuracy = torch.mean(correct_prediction)\n",
    "    print(\"Test: \", accuracy)\n",
    "\n",
    "#     print (\"Train Accuracy:\", accuracy.eval({Z3, Y_train}))\n",
    "#     print (\"Test Accuracy:\", accuracy.eval({X_test, Y_test}))\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.740225\n",
      "tensor([[-0.2464, -0.3457, -0.3652, -0.1569, -0.2780, -0.3452, -0.2071, -0.3145,\n",
      "         -0.2218, -0.3099, -0.2646, -0.2767, -0.2419, -0.2735, -0.2981, -0.2927,\n",
      "         -0.3101, -0.2439, -0.2432, -0.2747, -0.3382, -0.2221, -0.3054, -0.3262],\n",
      "        [-0.5031, -0.5516, -0.6410, -0.8036, -0.4927, -0.6535, -0.4910, -0.6880,\n",
      "         -0.5512, -0.5531, -0.4390, -0.5179, -0.6598, -0.4508, -0.4265, -0.7148,\n",
      "         -0.5218, -0.5693, -0.4282, -0.5710, -0.4415, -0.5460, -0.5155, -0.5445],\n",
      "        [-0.8157, -0.8228, -0.8024, -0.7430, -0.8015, -0.8083, -0.8035, -0.7960,\n",
      "         -0.8073, -0.8326, -0.7696, -0.7983, -0.7790, -0.7943, -0.7838, -0.7753,\n",
      "         -0.8265, -0.7843, -0.7712, -0.7547, -0.7999, -0.8255, -0.8136, -0.8177],\n",
      "        [ 0.2859,  0.2837,  0.2330,  0.1102,  0.2974,  0.2043,  0.3100,  0.1845,\n",
      "          0.2633,  0.2462,  0.4681,  0.3308,  0.1964,  0.4058,  0.4297,  0.1886,\n",
      "          0.2630,  0.2971,  0.4678,  0.3169,  0.4163,  0.1916,  0.2748,  0.2927],\n",
      "        [ 1.0342,  1.1140,  1.1795,  1.1607,  1.0353,  1.1953,  0.9824,  1.1950,\n",
      "          1.0454,  1.1255,  0.8670,  1.0145,  1.1252,  0.9350,  0.9202,  1.1759,\n",
      "          1.1012,  1.0273,  0.8518,  1.0176,  0.9671,  1.1075,  1.0815,  1.0907],\n",
      "        [-0.9916, -0.9634, -0.9372, -0.9580, -0.9557, -0.9612, -0.9822, -0.9591,\n",
      "         -0.9934, -1.0018, -0.8684, -0.9416, -0.9592, -0.9182, -0.8875, -0.9349,\n",
      "         -0.9889, -0.9412, -0.8794, -0.8803, -0.8993, -1.0399, -0.9687, -0.9610]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[ 1.3790e-02, -5.0572e-05,  1.4431e-02,  ..., -1.3265e-02,\n",
      "          9.9577e-03,  6.0849e-03],\n",
      "        [ 2.0131e-02,  1.1349e-02, -1.8978e-02,  ..., -1.9028e-02,\n",
      "         -1.1780e-02,  1.0867e-02],\n",
      "        [ 2.0829e-02, -3.0751e-03,  2.2303e-02,  ...,  4.1166e-03,\n",
      "          2.1214e-02, -2.0556e-02],\n",
      "        ...,\n",
      "        [-1.2651e-02,  1.7005e-03, -1.3956e-02,  ...,  1.3824e-02,\n",
      "         -8.0341e-03, -1.9046e-02],\n",
      "        [ 1.3624e-02, -1.0836e-02,  1.5147e-03,  ...,  1.7099e-02,\n",
      "         -1.3393e-02, -8.8856e-03],\n",
      "        [ 2.9241e-03, -5.7105e-03,  2.3320e-03,  ..., -3.5249e-03,\n",
      "         -2.1989e-02,  6.0053e-03]], requires_grad=True)\n",
      "tensor([[ 0.1621, -0.1274,  0.0516,  0.3836,  0.0496, -0.2597, -0.2245,  0.2739,\n",
      "          0.0549,  0.1483,  0.1743,  0.1153, -0.3424, -0.1491, -0.2742,  0.3350,\n",
      "         -0.1414, -0.3754, -0.3860, -0.2411,  0.0438, -0.0126, -0.0764, -0.2935,\n",
      "          0.1821],\n",
      "        [ 0.3823, -0.1347,  0.3545, -0.0392,  0.1207,  0.2081, -0.1908,  0.1670,\n",
      "         -0.3071, -0.1853, -0.0112,  0.2021, -0.0319,  0.4019,  0.0905, -0.3322,\n",
      "          0.0301,  0.3563,  0.1837, -0.1130, -0.1410, -0.2954,  0.3326, -0.1107,\n",
      "          0.2079],\n",
      "        [-0.2593, -0.2526,  0.1613, -0.1245, -0.2651, -0.0153,  0.2383, -0.2228,\n",
      "         -0.1480,  0.1096, -0.2601,  0.3062, -0.2656,  0.3587,  0.2493,  0.0280,\n",
      "         -0.0415,  0.2226,  0.3899,  0.3148, -0.3502, -0.2269,  0.2673,  0.2274,\n",
      "          0.2217],\n",
      "        [ 0.0921, -0.2224, -0.3032, -0.1125, -0.1107,  0.3884,  0.1148,  0.0872,\n",
      "          0.1360,  0.0751, -0.2399, -0.3451,  0.3946,  0.0535, -0.1053, -0.0542,\n",
      "          0.0447, -0.0103,  0.1693,  0.2810, -0.1437,  0.1394,  0.3024, -0.1509,\n",
      "         -0.1043],\n",
      "        [-0.0321, -0.3359,  0.2364,  0.1174, -0.2323, -0.2917, -0.1278, -0.0212,\n",
      "         -0.0671,  0.3116,  0.0922,  0.3186,  0.0548, -0.3264, -0.0927, -0.0537,\n",
      "          0.1163,  0.3788,  0.0748,  0.0863, -0.1253,  0.1628, -0.1145, -0.1411,\n",
      "         -0.2552],\n",
      "        [-0.0858, -0.3077, -0.3878,  0.1135,  0.3315,  0.3882,  0.0274, -0.2502,\n",
      "         -0.3344, -0.0838,  0.2909, -0.2000,  0.2265, -0.0941, -0.0156, -0.1328,\n",
      "          0.3108,  0.1171, -0.1187, -0.2532,  0.2501,  0.0986, -0.4013, -0.2580,\n",
      "          0.1415],\n",
      "        [ 0.3110, -0.2129,  0.2282, -0.0750, -0.3489, -0.3731, -0.2471,  0.0636,\n",
      "          0.2195,  0.3928,  0.0314, -0.2825, -0.0160, -0.2647, -0.3252, -0.2833,\n",
      "         -0.3943, -0.2342,  0.3085,  0.3366, -0.2585,  0.1280, -0.0421,  0.2510,\n",
      "          0.2645],\n",
      "        [ 0.0323, -0.1467,  0.2699,  0.2565, -0.2965,  0.0040,  0.0096,  0.0029,\n",
      "          0.0948,  0.2289, -0.2317, -0.2427,  0.1533, -0.0472, -0.3816,  0.2216,\n",
      "         -0.0625, -0.2517,  0.3429, -0.3199,  0.0973, -0.1903,  0.1224,  0.1334,\n",
      "          0.0347],\n",
      "        [ 0.3540, -0.0536,  0.1078, -0.3416, -0.3513,  0.2425, -0.2243, -0.3567,\n",
      "          0.3153,  0.2815,  0.3017,  0.2101, -0.0962, -0.2280,  0.0018, -0.3525,\n",
      "          0.1399, -0.0713, -0.3144, -0.3724,  0.0294, -0.3891, -0.1375,  0.2793,\n",
      "          0.2870],\n",
      "        [-0.1186,  0.1357, -0.1868, -0.1586,  0.3892, -0.1880, -0.3892, -0.2514,\n",
      "         -0.0817, -0.2286, -0.2292,  0.2176, -0.3173,  0.2386, -0.0601,  0.2762,\n",
      "          0.3435,  0.1827, -0.1445, -0.1707,  0.2124, -0.1238, -0.3816,  0.1467,\n",
      "          0.2376],\n",
      "        [-0.1895, -0.3941,  0.2875,  0.3433,  0.3159,  0.1249, -0.3203, -0.1552,\n",
      "          0.0428, -0.1791,  0.3222, -0.1334, -0.2935,  0.2555,  0.3157, -0.2607,\n",
      "         -0.0577, -0.1499,  0.0146,  0.3282, -0.0475,  0.1639,  0.0109,  0.0165,\n",
      "          0.2774],\n",
      "        [ 0.1174, -0.2222, -0.3261,  0.3195,  0.0689, -0.3231,  0.0107, -0.1473,\n",
      "         -0.0654, -0.2004, -0.1031, -0.1717, -0.0515,  0.1620, -0.0972,  0.3605,\n",
      "         -0.3057, -0.0411, -0.2967,  0.0977, -0.3935,  0.0498, -0.1318,  0.2296,\n",
      "         -0.0602]], requires_grad=True)\n",
      "tensor([[ 0.3303,  0.5337,  0.2091,  0.1434, -0.5736,  0.2480, -0.3590,  0.0871,\n",
      "          0.2433, -0.2346,  0.2556,  0.0673],\n",
      "        [-0.0767,  0.0486, -0.4705, -0.1199,  0.0460,  0.2837,  0.5260, -0.0897,\n",
      "         -0.1281,  0.4123,  0.5337, -0.3537],\n",
      "        [ 0.1983,  0.3928,  0.1939, -0.5224,  0.0758,  0.4793, -0.1013, -0.1911,\n",
      "         -0.4127,  0.0906, -0.1821, -0.1434],\n",
      "        [ 0.3180,  0.4358, -0.1223,  0.4288,  0.4454,  0.0215, -0.3392, -0.4236,\n",
      "          0.2343,  0.5113, -0.3709,  0.5143],\n",
      "        [-0.5201, -0.3115,  0.0007,  0.0150, -0.0340, -0.0662,  0.4426,  0.3846,\n",
      "          0.5677, -0.2241, -0.0541,  0.4263],\n",
      "        [ 0.2560, -0.4926,  0.1863,  0.2133,  0.4513, -0.4434,  0.4529, -0.4290,\n",
      "          0.1715, -0.4998,  0.0325, -0.1078]], requires_grad=True)\n",
      "Train:  tensor(0.1667)\n",
      "Test:  tensor(0.1667)\n",
      "Cost after epoch 100: 0.261409\n",
      "tensor([[-4.3127,  1.8403, -1.7566, -5.8664, -4.1359, -4.2708, -2.0259, -1.8075,\n",
      "         -3.6591, -3.5399, -6.0370, -3.1331, -4.8759, -1.6776,  2.1694, -2.8439,\n",
      "         -3.0661, -1.8576, -2.6468, -4.6736, -2.5017, -3.8125, -2.5836, -5.9225],\n",
      "        [-4.0435, -6.8749,  2.1068,  1.8119,  3.0333,  2.4023, -3.4385, -4.3107,\n",
      "         -4.1104, -1.7632, -4.9750,  1.8935,  0.4057, -4.0754, -7.9347, -4.1436,\n",
      "         -4.1220, -1.9545,  1.4112,  1.6801, -5.2600, -4.3653, -5.1410,  2.1794],\n",
      "        [-1.3958, -1.7683, -2.2412, -2.1245, -2.3352, -2.3511, -1.5499, -1.6340,\n",
      "         -1.6275, -1.6870, -1.7925, -2.5857, -2.2656, -1.4820, -1.7375, -1.4931,\n",
      "         -1.5706, -1.3194, -1.7276, -2.4234, -1.4871, -1.3630, -1.4001, -2.3239],\n",
      "        [-3.2748, -4.5833, -4.2673, -3.4206, -3.2856, -5.0342, -0.1830, -3.0635,\n",
      "         -1.6319, -0.0776, -2.4854, -3.7022, -3.5955, -0.5756, -3.0363, -0.5301,\n",
      "         -1.1271, -4.0295, -4.4840, -2.5383,  1.0463,  0.5234, -2.3353, -3.4498],\n",
      "        [-0.8113, -2.0378, -4.1170, -2.1255, -3.6411, -2.4248, -3.3716, -2.0356,\n",
      "         -1.6486, -3.1973,  1.5443, -2.5277, -1.4584, -3.3521, -2.4799, -2.4085,\n",
      "         -2.5267, -3.4179, -3.9697, -2.5851, -3.2727, -3.0120, -1.4514, -1.9660],\n",
      "        [-0.5542, -5.0100, -3.1585, -0.8535, -2.2560, -1.8164, -2.9547, -2.6937,\n",
      "         -2.0131, -2.2991, -0.9237, -3.4471, -1.8691, -2.9573, -5.6735, -2.3452,\n",
      "         -2.3717, -1.2566, -1.3374, -2.5577, -3.1847, -1.8817, -1.9088, -1.2944]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[ 7.8693e-03, -7.1128e-04,  8.3023e-03,  ..., -8.4849e-03,\n",
      "          4.5454e-03,  1.5688e-03],\n",
      "        [ 2.6476e-02,  1.9086e-02, -6.8598e-03,  ...,  3.6118e-02,\n",
      "          4.0258e-02,  5.3277e-02],\n",
      "        [ 2.0390e-02, -4.3388e-03,  1.6572e-02,  ..., -3.0641e-02,\n",
      "         -4.2482e-03, -3.3754e-02],\n",
      "        ...,\n",
      "        [-1.2012e-02,  1.5773e-03, -1.3183e-02,  ...,  1.2926e-02,\n",
      "         -7.5295e-03, -1.7757e-02],\n",
      "        [ 5.2567e-39, -8.8348e-38,  3.7979e-38,  ...,  2.6489e-38,\n",
      "          4.4578e-38,  9.2796e-38],\n",
      "        [ 2.8296e-03, -5.5746e-03,  2.2476e-03,  ..., -3.4247e-03,\n",
      "         -2.1205e-02,  5.7580e-03]], requires_grad=True)\n",
      "tensor([[ 1.5229e-01, -1.5304e-01,  5.0599e-02,  3.8824e-01,  3.9922e-02,\n",
      "         -2.8545e-02, -2.2183e-01,  2.8643e-01,  2.5066e-02,  1.4432e-01,\n",
      "          1.6519e-01,  6.8418e-02, -3.3214e-01, -1.1301e-01, -1.7325e-01,\n",
      "          3.4510e-01, -1.5251e-01, -3.5754e-01, -3.9964e-01, -1.9819e-01,\n",
      "          1.0880e-20, -5.6848e-02, -7.4266e-02, -4.6984e-02,  1.4623e-01],\n",
      "        [ 1.0919e-01, -1.3482e-01,  3.5217e-01, -4.0882e-02,  1.1708e-01,\n",
      "          8.9778e-03, -1.9081e-01,  1.6070e-01, -3.0463e-01, -1.8544e-01,\n",
      "         -1.1104e-02,  1.9203e-01, -3.1723e-02,  3.8322e-01,  3.2831e-07,\n",
      "         -3.3139e-01,  2.9005e-02,  3.5388e-01,  1.8025e-01, -1.0443e-01,\n",
      "         -3.5210e-04, -2.8209e-01,  3.2829e-01, -1.4009e-05,  2.0119e-01],\n",
      "        [-2.4169e-01, -3.1358e-01,  1.6437e-01, -1.2509e-01, -1.4393e-01,\n",
      "          1.2141e-37,  2.7269e-01, -2.5639e-01, -1.9468e-01,  1.0695e-01,\n",
      "         -2.3615e-01,  1.9390e-01, -2.5663e-01,  2.3445e-01,  1.6240e-01,\n",
      "          4.0291e-02, -6.3184e-02,  2.0535e-01,  3.9722e-01,  2.7919e-01,\n",
      "         -8.4924e-02, -3.2073e-01,  2.5631e-01,  1.4926e-02,  1.4308e-01],\n",
      "        [ 4.7862e-07, -1.9909e-01, -3.0060e-01, -1.8191e-05, -1.0215e-01,\n",
      "          1.1399e-01,  1.1376e-01,  8.6075e-02,  1.3507e-01,  7.3806e-02,\n",
      "         -2.3635e-01, -3.1266e-01,  3.9180e-01,  4.8598e-02, -6.1145e-06,\n",
      "         -5.3951e-02,  4.4288e-02, -1.0203e-02,  1.6571e-01,  2.7466e-01,\n",
      "         -4.3167e-04,  3.0886e-04,  2.9954e-01, -7.1407e-04, -9.6963e-02],\n",
      "        [-2.8323e-02, -3.4690e-01,  2.3800e-01,  1.1237e-01, -1.8586e-01,\n",
      "         -4.5873e-02, -1.3077e-01,  6.2649e-03, -1.1714e-01,  3.3857e-01,\n",
      "          8.8266e-02,  2.2536e-01,  5.3867e-02, -2.4541e-01, -2.3663e-02,\n",
      "         -5.7437e-02,  1.3881e-01,  3.6483e-01,  7.3835e-02,  8.1814e-02,\n",
      "         -8.4533e-05,  2.6314e-01, -1.1227e-01, -3.5395e-04, -2.0746e-01],\n",
      "        [-9.7260e-08, -5.5773e-02, -2.8068e-01,  2.0596e-05,  3.1424e-01,\n",
      "          1.1382e-01,  2.3920e-02, -2.2334e-01, -3.2864e-01, -7.3827e-02,\n",
      "          2.8682e-01, -1.8199e-01,  2.2315e-01, -8.5675e-02,  2.7270e-38,\n",
      "         -1.3020e-01,  3.0796e-01,  1.1561e-01, -6.3681e-02, -1.4078e-01,\n",
      "          2.4030e-02,  1.8808e-06, -3.9094e-01, -2.7718e-02,  1.3301e-01],\n",
      "        [ 5.7852e-02, -2.3529e-01,  2.2819e-01, -8.2805e-02, -8.6929e-02,\n",
      "         -1.0208e-01, -2.5111e-01,  4.3711e-02,  2.1365e-01,  3.9135e-01,\n",
      "          2.8400e-02, -4.0561e-02, -1.5702e-02, -3.0998e-02, -2.2994e-01,\n",
      "         -2.9130e-01, -3.9888e-01, -2.1909e-01,  3.1241e-01,  3.1700e-01,\n",
      "         -2.7969e-02,  1.3214e-01, -4.1184e-02,  2.4433e-02,  3.0915e-02],\n",
      "        [ 2.9389e-02, -1.0905e-01,  2.7864e-01,  2.6703e-01, -2.7334e-01,\n",
      "          2.1215e-39,  1.7886e-02, -8.3159e-03,  1.5427e-01,  2.2897e-01,\n",
      "         -2.2718e-01, -2.0723e-01,  1.5142e-01, -4.1026e-02, -2.7851e-01,\n",
      "          2.2719e-01, -5.4363e-02, -2.4734e-01,  3.6318e-01, -3.0569e-01,\n",
      "          1.4564e-06, -1.9863e-01,  1.2018e-01,  1.8510e-04,  3.1963e-02],\n",
      "        [ 8.7751e-02, -5.3322e-02,  1.0729e-01, -3.2129e-01, -3.3123e-01,\n",
      "          2.0726e-02, -2.2309e-01, -3.4528e-01,  3.1204e-01,  2.7839e-01,\n",
      "          2.9792e-01,  1.9576e-01, -9.4942e-02, -2.1273e-01,  4.5519e-38,\n",
      "         -3.5076e-01,  1.3889e-01, -7.0494e-02, -2.8954e-01, -3.0262e-01,\n",
      "         -7.2796e-38, -1.1453e-01, -1.3455e-01,  3.8785e-02,  2.7202e-01],\n",
      "        [-3.9850e-05,  2.2709e-04, -1.4527e-01, -1.1489e-03,  3.6925e-01,\n",
      "         -4.5850e-03, -3.4328e-01, -2.2154e-01, -8.0367e-02, -2.0165e-01,\n",
      "         -2.2618e-01,  2.0124e-01, -3.1273e-01,  2.1992e-01, -1.1177e-12,\n",
      "          2.7111e-01,  3.4054e-01,  1.8053e-01, -8.1719e-02, -9.7091e-02,\n",
      "          1.0164e-02, -7.2058e-05, -3.7237e-01,  5.3574e-04,  2.2393e-01],\n",
      "        [-1.5608e-01, -4.0847e-01,  2.8713e-01,  3.4657e-01,  6.1036e-02,\n",
      "          8.1201e-05, -3.4589e-01, -1.6624e-01,  8.5542e-02, -1.9089e-01,\n",
      "          6.5297e-02, -1.8607e-04, -4.6948e-02,  2.6519e-02,  2.2170e-01,\n",
      "         -2.6038e-01, -6.2732e-02, -6.6903e-04,  6.6464e-03,  6.9326e-02,\n",
      "         -4.1972e-18,  1.5043e-01,  5.6128e-38,  1.0445e-37,  3.7702e-02],\n",
      "        [ 3.4442e-05, -2.0434e-01, -3.2187e-01,  6.3486e-02,  1.9545e-10,\n",
      "         -6.5894e-02,  1.0507e-02, -1.4333e-01, -6.4479e-02, -1.9202e-01,\n",
      "         -3.4967e-02, -2.2957e-03, -4.9758e-02,  1.3915e-03, -1.4157e-06,\n",
      "          3.5692e-01, -2.7670e-01, -3.6718e-11, -2.6619e-01,  7.3688e-02,\n",
      "         -1.1800e-01,  7.9358e-17, -1.2775e-01,  1.5717e-02, -1.1786e-12]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 0.3156,  0.5231,  0.2542,  0.1426, -0.6254,  0.2455, -0.5055,  0.0672,\n",
      "          0.2396, -0.2289,  0.2077,  0.0573],\n",
      "        [-0.0748,  0.0473, -0.6116, -0.1189,  0.0478,  0.2799,  0.5600, -0.0918,\n",
      "         -0.1257,  0.3994,  0.5865, -0.2125],\n",
      "        [ 0.1795,  0.3720,  0.1732, -0.5029,  0.0719,  0.4673, -0.0226, -0.2003,\n",
      "         -0.3989,  0.0862, -0.0516, -0.0957],\n",
      "        [ 0.3104,  0.4297, -0.2243,  0.4268,  0.4337,  0.0213, -0.4454, -0.4365,\n",
      "          0.2318,  0.5026, -0.4746,  0.3917],\n",
      "        [-0.5696, -0.3020,  0.0146,  0.0149, -0.0749, -0.0650,  0.4081,  0.3502,\n",
      "          0.5534, -0.2150, -0.1336,  0.3438],\n",
      "        [ 0.2096, -0.3926,  0.2921,  0.2099,  0.4066, -0.3953,  0.4556, -0.4731,\n",
      "          0.1428, -0.4102, -0.0529, -0.0431]], requires_grad=True)\n",
      "Train:  tensor(0.6898)\n",
      "Test:  tensor(0.6667)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 200: 0.195275\n",
      "tensor([[ -1.3540,  -5.6665,  -3.6981,  -6.1180,  -2.2907,   4.2881,   1.2318,\n",
      "          -4.6047,  -3.7188,   3.5407,  -5.2808,   3.4937,  -4.7814,  -1.7665,\n",
      "          -5.7976,  -4.5681,  -2.7637,  -2.2776,  -3.2883,  -4.1678,  -5.8264,\n",
      "           3.7780,  -3.3243,  -5.4380],\n",
      "        [ -8.7860,   3.4243,  -5.6854,  -4.9065,  -4.1135, -12.1612,  -5.3594,\n",
      "           3.3184,  -6.8276, -12.6105,  -4.1785,  -7.5491,  -3.6065,  -5.7353,\n",
      "          -4.0335,  -6.3531,  -5.7583,  -4.5967,  -3.9556,  -1.6844,  -2.4542,\n",
      "         -12.5610,  -7.7125,   2.3884],\n",
      "        [ -1.4158,  -1.3178,  -1.8360,  -1.6042,  -0.7061,  -2.2109,  -1.2414,\n",
      "          -1.5093,  -1.1655,  -2.2297,  -2.1445,  -0.9193,  -1.9764,  -2.0980,\n",
      "          -2.1942,  -1.1808,  -2.1330,  -1.5244,  -1.5497,  -1.1786,  -1.7597,\n",
      "          -2.2315,  -1.6015,  -1.3543],\n",
      "        [ -3.6050,  -6.2628,  -0.8760,  -0.1204,  -5.5047,  -4.1036,  -6.3172,\n",
      "          -7.0067,  -3.1040,  -4.5367,   2.1123,  -8.5975,  -2.0107,  -4.7923,\n",
      "          -6.7881,  -3.5315,   2.6366,  -4.4989,  -3.2283,  -6.3004,   0.4067,\n",
      "          -3.8145,  -5.0942,  -7.2496],\n",
      "        [ -2.7038,  -4.3338,  -3.2701,  -2.2124,  -4.4752,  -1.9191,  -4.5102,\n",
      "          -4.0893,  -2.6587,  -1.2866,  -3.0496,  -4.7125,  -2.5402,  -2.3027,\n",
      "           0.3993,  -1.9638,  -3.1048,  -3.1322,  -3.6456,  -3.6860,  -3.8550,\n",
      "          -2.0962,  -1.3461,  -3.5511],\n",
      "        [ -2.0744,  -2.5610,  -3.0349,  -0.5947,  -0.8351,  -6.4623,  -4.3323,\n",
      "          -3.6912,  -0.1815,  -5.9868,  -3.2254,  -4.7592,  -3.0585,  -4.3544,\n",
      "          -1.6820,  -0.3968,  -4.2757,  -2.7660,  -2.9140,  -1.6388,  -2.2906,\n",
      "          -6.3515,  -1.2924,  -2.2416]], grad_fn=<AddBackward0>)\n",
      "tensor([[ 3.7402e-03, -3.2816e-04,  3.6658e-03,  ..., -3.4112e-03,\n",
      "          1.7369e-03,  5.7115e-04],\n",
      "        [ 3.1887e-02,  2.5784e-02,  4.0971e-03,  ...,  6.6921e-02,\n",
      "          6.8261e-02,  7.2756e-02],\n",
      "        [ 1.8306e-02, -7.0368e-03,  9.7253e-03,  ..., -5.1875e-02,\n",
      "         -1.7707e-02, -3.6158e-02],\n",
      "        ...,\n",
      "        [-8.6281e-03,  1.1189e-03, -9.1931e-03,  ...,  8.7109e-03,\n",
      "         -4.9515e-03, -1.1475e-02],\n",
      "        [ 5.2567e-39, -8.8348e-38,  3.7979e-38,  ...,  2.6489e-38,\n",
      "          4.4578e-38,  9.2796e-38],\n",
      "        [ 2.4097e-03, -4.7190e-03,  1.8864e-03,  ..., -2.7819e-03,\n",
      "         -1.6913e-02,  4.5433e-03]], requires_grad=True)\n",
      "tensor([[ 1.3192e-01, -1.8472e-01,  4.8809e-02,  3.9403e-01,  1.0789e-02,\n",
      "         -8.1030e-07, -2.2482e-01,  3.2700e-01, -3.0285e-02,  1.4261e-01,\n",
      "          1.2000e-01,  3.5327e-03, -2.7702e-01, -2.4055e-02, -9.1211e-02,\n",
      "          3.5201e-01, -1.6156e-01, -2.7034e-01, -4.2150e-01, -7.1553e-02,\n",
      "         -3.3590e-38, -7.4455e-02, -6.2112e-02, -1.7055e-05,  4.3937e-02],\n",
      "        [ 1.6551e-03, -1.2755e-01,  3.4577e-01, -4.0051e-02,  9.7519e-02,\n",
      "          3.9867e-10, -1.8381e-01,  1.3141e-01, -2.8863e-01, -1.7665e-01,\n",
      "         -1.0606e-02,  1.4154e-01, -3.0325e-02,  2.9213e-01,  1.0049e-37,\n",
      "         -3.2053e-01,  2.8376e-02,  3.3975e-01,  1.7020e-01, -6.4994e-02,\n",
      "         -1.1957e-20, -2.3822e-01,  3.0334e-01,  1.0428e-34,  1.6504e-01],\n",
      "        [-1.8860e-01, -3.7169e-01,  1.6805e-01, -1.2445e-01, -1.0429e-02,\n",
      "          1.2141e-37,  2.8727e-01, -3.0191e-01, -2.4886e-01,  1.0959e-01,\n",
      "         -1.3749e-01,  3.0263e-02, -2.0922e-01,  4.8600e-02,  1.3733e-01,\n",
      "          4.6275e-02, -8.3980e-02,  1.2847e-01,  3.9927e-01,  1.4871e-01,\n",
      "         -4.6841e-04, -3.6721e-01,  1.9987e-01,  1.2231e-08,  1.7265e-02],\n",
      "        [ 1.0426e-37, -1.0636e-01, -2.8503e-01, -3.8494e-34, -6.2830e-02,\n",
      "          2.0323e-03,  1.0791e-01,  7.9723e-02,  1.2998e-01,  6.6854e-02,\n",
      "         -2.1590e-01, -1.8520e-01,  3.7541e-01,  2.7284e-02, -7.7361e-38,\n",
      "         -5.2174e-02,  4.2072e-02, -9.3775e-03,  1.4582e-01,  2.3975e-01,\n",
      "         -6.3288e-20,  4.0410e-21,  2.8310e-01, -3.5227e-18, -6.2300e-02],\n",
      "        [-2.4416e-02, -3.6801e-01,  2.3718e-01,  1.0583e-01, -5.8707e-02,\n",
      "         -1.4798e-05, -1.3696e-01,  7.2160e-02, -1.8212e-01,  3.5546e-01,\n",
      "          6.7870e-02,  5.3071e-02,  4.8481e-02, -7.1900e-02, -9.2731e-03,\n",
      "         -5.9709e-02,  1.5625e-01,  2.9280e-01,  6.3427e-02,  5.9406e-02,\n",
      "         -4.4331e-26,  3.3112e-01, -9.9611e-02, -1.2483e-20, -7.2520e-02],\n",
      "        [-7.3235e-38, -4.6370e-05, -8.3399e-02,  6.1244e-33,  2.3103e-01,\n",
      "          2.0180e-03,  1.0503e-02, -1.1896e-01, -2.9555e-01, -3.4386e-02,\n",
      "          2.6318e-01, -1.0522e-01,  2.0389e-01, -4.8715e-02,  2.7270e-38,\n",
      "         -1.1539e-01,  2.9162e-01,  1.0725e-01, -1.8942e-03, -1.0502e-02,\n",
      "          2.7199e-07,  1.1356e-37, -3.3473e-01, -6.7336e-07,  9.1813e-02],\n",
      "        [ 5.7190e-05, -2.5832e-01,  2.3265e-01, -8.6392e-02, -5.3084e-04,\n",
      "         -1.1927e-03, -2.5558e-01,  1.5738e-02,  2.2079e-01,  3.9697e-01,\n",
      "          1.5458e-02, -7.0820e-06, -1.3902e-02, -1.3580e-06, -1.7582e-01,\n",
      "         -2.9610e-01, -4.0852e-01, -1.4824e-01,  3.2419e-01,  2.2547e-01,\n",
      "         -7.1285e-07,  1.3863e-01, -3.6049e-02,  3.0243e-07,  1.3355e-06],\n",
      "        [ 2.5865e-02, -6.8165e-02,  2.8101e-01,  2.6725e-01, -1.7326e-01,\n",
      "          2.1215e-39,  2.6375e-02, -6.1034e-02,  2.2379e-01,  2.2431e-01,\n",
      "         -2.0163e-01, -8.9070e-02,  1.4045e-01, -1.7544e-02, -2.1385e-01,\n",
      "          2.2531e-01, -4.9767e-02, -2.2271e-01,  3.8463e-01, -2.3432e-01,\n",
      "          1.1822e-37, -2.1078e-01,  1.0718e-01,  5.2285e-23,  1.9271e-02],\n",
      "        [ 5.5532e-04, -5.0531e-02,  1.0533e-01, -2.2649e-01, -2.3709e-01,\n",
      "          1.0506e-07, -2.1503e-01, -2.8392e-01,  2.9381e-01,  2.6102e-01,\n",
      "          2.7607e-01,  1.2916e-01, -8.7599e-02, -1.4188e-01,  4.5519e-38,\n",
      "         -3.3910e-01,  1.3342e-01, -6.5642e-02, -1.8302e-01, -1.2245e-01,\n",
      "         -7.2796e-38, -2.0777e-03, -1.1809e-01,  5.3993e-06,  1.9899e-01],\n",
      "        [-2.3828e-29, -5.4848e-03, -1.2678e-01, -6.0807e-03,  2.7417e-01,\n",
      "         -3.6997e-12, -2.8518e-01, -1.6280e-01, -7.6386e-02, -1.8634e-01,\n",
      "         -2.0865e-01,  1.2776e-01, -2.8639e-01,  1.3761e-01, -1.6537e-38,\n",
      "          2.5232e-01,  3.2313e-01,  1.6780e-01, -5.7911e-02, -5.1247e-03,\n",
      "          9.2863e-10, -5.7656e-03, -3.2184e-01,  3.6096e-19,  1.5826e-01],\n",
      "        [-6.5382e-02, -4.3225e-01,  2.9470e-01,  3.5893e-01,  7.7532e-05,\n",
      "          3.0311e-26, -3.7937e-01, -2.1502e-01,  1.4401e-01, -2.0454e-01,\n",
      "          1.1321e-04, -5.4719e-23, -1.6976e-05,  5.0914e-07,  1.9547e-01,\n",
      "         -2.6748e-01, -7.1942e-02, -2.1094e-18,  5.7193e-03,  1.5771e-04,\n",
      "          6.7890e-38,  1.5667e-01,  5.6128e-38,  1.0445e-37,  4.5444e-06],\n",
      "        [ 4.7968e-30, -1.2560e-01, -2.9726e-01,  9.6750e-05,  4.2927e-38,\n",
      "         -1.1910e-04,  9.6236e-03, -1.2142e-01, -5.9096e-02, -1.4859e-01,\n",
      "         -6.6881e-05, -2.5067e-14, -4.0059e-02,  6.0061e-16, -1.1515e-37,\n",
      "          3.3639e-01, -1.6081e-01, -3.6105e-38, -1.4808e-01,  1.4107e-02,\n",
      "         -2.3908e-03, -1.3660e-37, -1.0548e-01,  1.7210e-08, -1.3188e-38]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 3.2821e-01,  4.6527e-01,  2.8230e-01,  1.3825e-01, -6.4549e-01,\n",
      "          2.3129e-01, -6.1233e-01,  7.4418e-02,  2.1859e-01, -1.9732e-01,\n",
      "          2.1874e-01,  2.1671e-02],\n",
      "        [-7.6747e-02,  4.0280e-02, -7.0297e-01, -1.1276e-01,  3.9741e-02,\n",
      "          2.5791e-01,  5.7539e-01, -9.5460e-02, -1.1207e-01,  3.3121e-01,\n",
      "          6.3436e-01, -3.3787e-02],\n",
      "        [ 1.7124e-01,  2.7433e-01,  1.5206e-01, -4.0524e-01,  7.0074e-02,\n",
      "          4.0229e-01, -2.9444e-02, -1.9923e-01, -3.2674e-01,  6.3926e-02,\n",
      "          8.8697e-02, -1.0255e-02],\n",
      "        [ 3.2743e-01,  3.9525e-01, -3.2673e-01,  4.1470e-01,  4.4917e-01,\n",
      "          2.0421e-02, -5.5702e-01, -4.2603e-01,  2.1749e-01,  4.5369e-01,\n",
      "         -6.1825e-01,  1.6202e-01],\n",
      "        [-5.8209e-01, -2.4962e-01,  1.8559e-02,  1.4520e-02, -7.8171e-02,\n",
      "         -5.8143e-02,  4.1924e-01,  3.5573e-01,  4.7754e-01, -1.6778e-01,\n",
      "         -2.1056e-01,  1.4516e-01],\n",
      "        [ 2.1196e-01, -1.7415e-01,  3.5243e-01,  1.9061e-01,  4.0805e-01,\n",
      "         -2.2896e-01,  5.1113e-01, -4.8085e-01,  5.0943e-02, -1.9568e-01,\n",
      "         -1.1562e-01, -2.2410e-04]], requires_grad=True)\n",
      "Train:  tensor(0.8509)\n",
      "Test:  tensor(0.8250)\n",
      "Cost after epoch 300: 0.150247\n",
      "tensor([[ -6.6377,  -3.8231,  -3.3830,  -4.8174,  -2.9085,  -5.4778,  -8.2954,\n",
      "          -5.3566,  -4.8395,  -7.9847,  -5.8990,  -5.3508,  -5.7108,  -5.9507,\n",
      "          -7.3563,  -5.3208,  -4.3100,  -5.8039,  -1.6074,  -8.4673,  -1.6534,\n",
      "          -6.2904,  -4.6473,  -2.2402],\n",
      "        [  1.8657,  -4.1065,  -1.1992,  -5.9333,  -5.4935,  -6.9798,  -5.4727,\n",
      "          -4.3563,  -7.4441,   3.0316,  -4.3446,   1.2960,   2.8310,   1.4922,\n",
      "          -8.0490,  -1.9293,  -8.1054,  -7.1651,  -4.3507,  -1.5280,  -6.3057,\n",
      "          -3.0584,  -2.7261,  -7.7644],\n",
      "        [ -1.0483,  -1.5741,  -1.0335,  -0.5577,  -2.6432,  -2.2466,  -2.5376,\n",
      "          -2.9040,  -1.1893,  -2.1882,  -1.2813,  -2.0179,  -1.6795,  -1.5309,\n",
      "          -3.6299,  -0.6155,  -2.8654,  -2.6769,   0.2439,  -2.3384,  -3.0145,\n",
      "          -1.0150,  -0.4364,  -1.2464],\n",
      "        [ -9.4127,   1.9759,  -6.1198, -12.8439,   1.4483,   2.0223,   6.9702,\n",
      "          -8.9931,  -3.6687,  -7.0480,  -6.1587,  -9.6642,  -8.2970,  -7.2517,\n",
      "          -3.3349,  -7.6868, -10.8671,   4.0340,  -4.0084,  -4.9814,   3.6606,\n",
      "          -4.1295, -11.2716,  -3.6417],\n",
      "        [ -3.9242,  -6.3408,  -4.9783,  -2.3271,  -4.1193,  -3.3478,  -3.5466,\n",
      "           0.9232,  -2.7511,  -2.9576,  -3.5403,  -2.5115,  -4.2195,  -4.0236,\n",
      "           1.4097,  -5.0193,   2.0237,  -3.2695,  -7.3505,  -1.0344,  -3.9886,\n",
      "          -4.3026,  -3.6792,  -4.1339],\n",
      "        [ -2.5101,  -2.8153,  -4.6447,   0.6959,  -6.3835,  -2.0920,  -2.7631,\n",
      "          -2.9371,  -1.4410,  -3.2917,  -0.6648,  -4.8732,  -4.4253,  -3.8761,\n",
      "          -1.9570,  -1.5106,  -2.7475,  -3.3432,  -3.4577,  -1.1958,  -6.5414,\n",
      "          -2.7710,  -1.1579,  -4.3293]], grad_fn=<AddBackward0>)\n",
      "tensor([[-1.1535e-03, -1.2145e-03, -1.1718e-03,  ..., -1.3835e-03,\n",
      "         -1.3227e-03, -1.2880e-03],\n",
      "        [ 3.6243e-02,  3.1264e-02,  1.3364e-02,  ...,  7.9802e-02,\n",
      "          7.8421e-02,  7.5851e-02],\n",
      "        [ 1.5962e-02, -9.8137e-03,  3.0432e-03,  ..., -6.3983e-02,\n",
      "         -2.3288e-02, -3.2064e-02],\n",
      "        ...,\n",
      "        [-1.3920e-03,  1.6826e-04, -1.2594e-03,  ...,  9.8730e-04,\n",
      "         -4.8948e-04, -1.0328e-03],\n",
      "        [ 5.2567e-39, -8.8348e-38,  3.7979e-38,  ...,  2.6489e-38,\n",
      "          4.4578e-38,  9.2796e-38],\n",
      "        [ 9.9530e-04, -1.8857e-03,  7.1912e-04,  ..., -8.8523e-04,\n",
      "         -4.8734e-03,  1.2316e-03]], requires_grad=True)\n",
      "tensor([[ 6.1893e-02, -2.1979e-01,  4.5565e-02,  3.9740e-01,  7.6608e-06,\n",
      "         -6.0806e-03, -2.2653e-01,  3.8275e-01, -8.3397e-02,  1.4236e-01,\n",
      "          2.4542e-02,  2.0050e-10, -1.2441e-01, -6.8421e-06, -4.3066e-03,\n",
      "          3.5771e-01, -1.7227e-01, -9.1674e-02, -4.4154e-01, -6.6862e-04,\n",
      "         -5.6944e-05, -9.0917e-02, -2.3532e-02, -1.7771e-27,  1.0574e-04],\n",
      "        [ 8.9157e-14, -9.4320e-02,  3.1290e-01, -3.5774e-02,  3.6857e-02,\n",
      "         -1.7518e-38, -1.4994e-01,  4.6696e-02, -2.1706e-01, -1.3575e-01,\n",
      "         -8.2449e-03,  3.2509e-02, -2.3675e-02,  1.0547e-01,  1.0049e-37,\n",
      "         -2.6793e-01,  2.5158e-02,  2.7360e-01,  1.2477e-01, -5.4234e-03,\n",
      "         -1.5206e-38, -1.0763e-01,  2.0323e-01, -5.9290e-38,  6.1653e-02],\n",
      "        [-5.9690e-02, -4.3992e-01,  1.6945e-01, -1.2716e-01, -9.8090e-09,\n",
      "          2.6160e-03,  2.9852e-01, -3.3988e-01, -2.9926e-01,  1.1208e-01,\n",
      "         -1.3501e-02,  3.8125e-06, -8.0340e-02,  4.3468e-05,  5.7481e-02,\n",
      "          4.8403e-02, -1.0178e-01,  1.5060e-02,  4.0109e-01,  1.2685e-02,\n",
      "          1.0386e-03, -4.0366e-01,  6.4400e-02, -1.6406e-38,  2.8593e-07],\n",
      "        [ 1.0426e-37, -6.1058e-03, -2.1512e-01, -4.1160e-38, -4.9064e-03,\n",
      "          3.1779e-13,  8.0880e-02,  5.2427e-02,  1.0537e-01,  3.8932e-02,\n",
      "         -1.3471e-01, -2.6525e-02,  2.9985e-01,  1.1640e-03, -7.7361e-38,\n",
      "         -4.3408e-02,  3.1738e-02, -5.8946e-03,  7.4374e-02,  1.2340e-01,\n",
      "         -8.0655e-39,  1.1711e-38,  2.1023e-01, -1.1874e-39, -6.0295e-03],\n",
      "        [-1.0797e-02, -4.0527e-01,  2.3864e-01,  1.0210e-01, -2.5701e-04,\n",
      "          1.6337e-04, -1.4549e-01,  1.1735e-01, -2.3624e-01,  3.6642e-01,\n",
      "          1.6589e-02,  8.0641e-05,  2.7215e-02, -3.8512e-04, -5.1567e-05,\n",
      "         -6.3191e-02,  1.7226e-01,  1.1980e-01,  5.6822e-02,  1.0665e-02,\n",
      "         -1.3952e-03,  3.8991e-01, -5.2225e-02, -1.1211e-38, -6.2786e-04],\n",
      "        [-7.3235e-38, -1.6620e-03, -4.2498e-02, -2.7727e-03,  6.6587e-02,\n",
      "          3.0420e-13,  2.1420e-03, -3.2226e-02, -1.7911e-01, -2.1588e-02,\n",
      "          1.6948e-01, -7.9626e-03,  1.2707e-01, -2.4121e-03,  2.7270e-38,\n",
      "         -9.2402e-02,  2.3409e-01,  7.1279e-02, -2.0452e-03, -1.1186e-08,\n",
      "         -2.0641e-38, -1.9047e-03, -1.7161e-01,  1.2838e-38,  1.3803e-02],\n",
      "        [ 1.7100e-23, -2.8559e-01,  2.3654e-01, -9.0345e-02, -6.6612e-17,\n",
      "          6.0849e-03, -2.6047e-01,  4.1662e-03,  2.2975e-01,  4.0408e-01,\n",
      "          5.4101e-04, -9.9251e-31, -7.1153e-03,  9.2696e-38, -5.0356e-02,\n",
      "         -3.0014e-01, -4.2292e-01, -2.5185e-02,  3.3443e-01,  5.8763e-02,\n",
      "          2.5563e-03,  1.4546e-01, -1.7362e-02, -1.7952e-38, -7.3923e-38],\n",
      "        [ 1.2822e-02, -1.5960e-02,  2.8289e-01,  2.6754e-01, -2.7111e-02,\n",
      "         -1.0613e-05,  3.5239e-02, -1.0691e-01,  2.8197e-01,  2.2199e-01,\n",
      "         -1.0950e-01, -2.0166e-03,  9.3560e-02, -1.6389e-04, -6.7227e-02,\n",
      "          2.2540e-01, -4.3543e-02, -1.3019e-01,  4.0171e-01, -7.6404e-02,\n",
      "         -2.9257e-04, -2.1976e-01,  5.7838e-02,  1.6781e-38,  1.1924e-03],\n",
      "        [ 8.8803e-17, -4.0364e-02,  9.2405e-02, -6.4014e-02, -6.4999e-02,\n",
      "         -1.8173e-38, -1.7952e-01, -1.2717e-01,  2.1331e-01,  1.8517e-01,\n",
      "          1.8645e-01,  1.8460e-02, -5.6458e-02, -2.2310e-02,  4.5519e-38,\n",
      "         -2.8770e-01,  1.0479e-01, -4.4411e-02, -3.5166e-02, -4.2765e-03,\n",
      "         -7.2796e-38, -3.1652e-03, -5.8834e-02,  7.3654e-32,  5.1868e-02],\n",
      "        [-3.3345e-38, -1.0020e-02, -1.2785e-01, -1.2667e-02,  8.9974e-02,\n",
      "          1.1020e-38, -2.4905e-01, -1.1413e-01, -6.6875e-02, -1.8448e-01,\n",
      "         -1.3638e-01,  1.5725e-02, -1.8376e-01,  1.6994e-02, -1.6537e-38,\n",
      "          2.3307e-01,  2.9656e-01,  1.1333e-01, -5.6920e-02, -4.3398e-10,\n",
      "         -1.1658e-38, -1.1573e-02, -1.6835e-01,  5.6711e-39,  3.2532e-02],\n",
      "        [-9.2552e-04, -4.6131e-01,  2.9992e-01,  3.6985e-01,  1.5115e-22,\n",
      "          5.1913e-04, -4.0233e-01, -2.7067e-01,  1.8156e-01, -2.1801e-01,\n",
      "          2.1530e-21, -2.0669e-38, -1.7117e-27, -1.7525e-38,  1.0273e-01,\n",
      "         -2.7180e-01, -7.3864e-02, -1.6174e-39,  2.1309e-03,  2.1072e-20,\n",
      "          6.8561e-04,  1.5784e-01,  5.6128e-38,  1.0445e-37,  1.2174e-32],\n",
      "        [ 3.8070e-38, -3.2810e-02, -2.1844e-01, -1.1949e-02,  4.2927e-38,\n",
      "         -3.0592e-21, -1.5575e-04, -5.5916e-02, -3.8307e-02, -8.2443e-02,\n",
      "         -1.8586e-21, -1.2700e-39, -1.2244e-02, -1.2885e-39, -1.1515e-37,\n",
      "          2.5390e-01, -4.1109e-02, -3.6105e-38, -4.1430e-02,  1.6626e-06,\n",
      "         -8.6334e-13, -1.3660e-37, -3.8378e-02, -1.3214e-38, -1.3188e-38]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 3.3959e-01,  2.8650e-01,  2.8600e-01,  1.1657e-01, -6.6793e-01,\n",
      "          1.6828e-01, -6.7115e-01,  7.9883e-02,  1.3555e-01, -1.0055e-01,\n",
      "          2.2314e-01,  6.1220e-04],\n",
      "        [-7.7421e-02,  1.6771e-02, -7.8526e-01, -8.4398e-02,  3.0222e-02,\n",
      "          1.6916e-01,  5.8948e-01, -9.8059e-02, -6.0367e-02,  1.6241e-01,\n",
      "          6.6784e-01, -6.0298e-06],\n",
      "        [ 1.5657e-01,  8.8739e-02,  1.7753e-01, -1.9298e-01,  7.3694e-02,\n",
      "          2.1980e-01, -1.1289e-01, -1.9726e-01, -1.4721e-01,  2.1612e-02,\n",
      "          1.8112e-01, -5.9028e-03],\n",
      "        [ 3.4211e-01,  2.6509e-01, -4.2836e-01,  3.5570e-01,  4.6497e-01,\n",
      "          1.6116e-02, -6.4862e-01, -4.1603e-01,  1.5470e-01,  2.9112e-01,\n",
      "         -7.2315e-01,  1.0238e-02],\n",
      "        [-5.9788e-01, -1.0569e-01,  1.7044e-02,  1.2478e-02, -8.0997e-02,\n",
      "         -3.1518e-02,  4.2931e-01,  3.6257e-01,  2.7521e-01, -5.1278e-02,\n",
      "         -2.4523e-01,  5.2671e-03],\n",
      "        [ 2.1835e-01, -1.4747e-02,  3.8420e-01,  1.1502e-01,  4.0919e-01,\n",
      "         -4.1611e-02,  5.9213e-01, -4.8852e-01,  2.7112e-04, -2.4087e-02,\n",
      "         -1.6974e-01, -1.3756e-04]], requires_grad=True)\n",
      "Train:  tensor(0.9185)\n",
      "Test:  tensor(0.8750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 400: 0.101976\n",
      "tensor([[  0.1759,  -7.6098,  -4.4339,  -4.8755,  -7.9327,  -2.6429,   5.1380,\n",
      "          -3.2139,  -7.1502,  -2.8582,   5.9095,  -4.6090,  -7.1798,   4.9109,\n",
      "          -9.0021,   4.8131,  -3.5840,   5.5143,  -8.9965,  -9.6652,  -7.8780,\n",
      "          -1.0451,   1.8271,   4.9244],\n",
      "        [ -2.4187,  -4.6960,  -3.8345,  -2.2747,   4.7766,  -4.9516, -18.0904,\n",
      "         -15.6053,  -9.4611, -15.1411, -18.6121,  -6.7948,   4.1394, -19.1636,\n",
      "           3.0880, -10.1092,  -8.8552, -16.9754,   2.6188,   4.0562,  -9.3991,\n",
      "         -10.6206, -19.8965, -10.7852],\n",
      "        [ -2.6364,  -1.5297,  -1.5391,  -0.4982,  -0.0308,  -0.3447,  -4.6724,\n",
      "          -3.1359,  -3.6944,  -3.4767,  -4.1230,  -4.4807,  -2.7022,  -3.4728,\n",
      "          -2.1391,  -6.2381,  -5.2581,  -3.2828,  -1.1884,  -3.2849,  -3.2483,\n",
      "           1.2896,  -3.0514,  -3.4903],\n",
      "        [ -3.8322,  -1.1505,  -9.9609, -11.8912,  -8.4520, -14.5813,  -7.1477,\n",
      "          -5.2102,  -3.9289,  -4.0115,  -5.3602,   4.0753,  -7.9957,  -5.0504,\n",
      "         -10.1631,  -2.3979,  -7.8826,  -2.9315,  -9.1138,  -9.7636,  -4.1722,\n",
      "         -11.2801,  -5.7495,  -8.5167],\n",
      "        [ -7.2575,  -9.2560,  -5.1982,  -4.0228,  -7.4898,  -2.4666,  -4.0430,\n",
      "           0.9051,  -1.9930,   0.4901,  -3.3820,  -2.0549,  -3.3627,  -3.2339,\n",
      "          -3.3250,  -4.4054,   2.3940,  -5.8001,  -4.5594,  -1.8905,   1.1820,\n",
      "          -4.7340,  -2.3005,  -6.2724],\n",
      "        [ -7.6433,   3.5070,  -1.0073,  -3.0307,  -3.0700,  -5.3844,  -3.8842,\n",
      "          -3.6424,   1.6571,  -4.5720,  -6.7237,  -7.9784,  -7.5032,  -6.1959,\n",
      "          -3.2597,  -9.9398,  -5.2353,  -6.0476,  -3.9960,  -4.6939,  -3.2506,\n",
      "          -4.1518,  -3.6155,  -5.7292]], grad_fn=<AddBackward0>)\n",
      "tensor([[-1.3143e-05, -1.0679e-05, -7.7346e-06,  ..., -5.6883e-06,\n",
      "         -5.8715e-06, -5.9151e-06],\n",
      "        [ 4.0108e-02,  3.6068e-02,  2.1280e-02,  ...,  8.1460e-02,\n",
      "          7.5838e-02,  6.6321e-02],\n",
      "        [ 1.2934e-02, -1.3211e-02, -4.6048e-03,  ..., -6.9082e-02,\n",
      "         -2.0807e-02, -1.9565e-02],\n",
      "        ...,\n",
      "        [-5.0052e-08,  4.0208e-09, -1.7637e-08,  ...,  4.6013e-09,\n",
      "         -1.0247e-09, -1.2561e-09],\n",
      "        [-1.9629e-06, -1.6404e-06, -1.9860e-06,  ..., -5.8757e-06,\n",
      "         -4.9359e-06, -5.8891e-06],\n",
      "        [ 2.9931e-06, -1.5836e-05, -4.0545e-07,  ..., -5.0032e-06,\n",
      "         -8.3129e-06, -2.5653e-06]], requires_grad=True)\n",
      "tensor([[ 8.6841e-03, -2.5973e-01,  4.1885e-02,  4.0090e-01,  1.5007e-25,\n",
      "         -3.5830e-03, -2.3221e-01,  4.4798e-01, -1.2446e-01,  1.4157e-01,\n",
      "          6.0734e-06, -2.9237e-38, -5.4978e-03, -8.6115e-29, -1.8569e-10,\n",
      "          3.6206e-01, -1.8552e-01, -1.2047e-03, -4.5924e-01, -1.1538e-15,\n",
      "         -5.0644e-05, -1.0431e-01, -1.2085e-04, -3.9694e-04,  1.5104e-09],\n",
      "        [-2.2534e-38, -7.5200e-03,  2.3336e-01, -1.3012e-02,  2.1825e-04,\n",
      "         -1.7518e-38, -1.0732e-01,  1.0168e-01,  2.6702e-03, -5.5932e-02,\n",
      "         -2.0667e-03,  1.8371e-05, -6.0989e-03,  2.1697e-03,  1.0049e-37,\n",
      "         -1.5730e-01,  9.4314e-03,  1.1018e-01,  4.7747e-02, -5.2796e-09,\n",
      "         -1.5206e-38,  3.6769e-02,  4.2315e-02, -5.9290e-38,  5.3036e-04],\n",
      "        [-7.1045e-03, -5.1162e-01,  1.7049e-01, -1.2972e-01,  2.2489e-38,\n",
      "          2.8411e-03,  3.1457e-01, -3.7945e-01, -3.5014e-01,  1.1327e-01,\n",
      "         -6.7999e-08,  2.0337e-32, -1.1070e-03,  6.3587e-24,  7.2833e-04,\n",
      "          5.1232e-02, -1.1459e-01,  1.8437e-07,  4.0428e-01,  3.3082e-08,\n",
      "          8.9609e-04, -4.3989e-01,  3.5626e-04, -3.0512e-03,  2.3466e-17],\n",
      "        [ 1.0426e-37, -9.7659e-10, -6.4404e-02, -4.1160e-38, -3.1950e-09,\n",
      "         -2.3491e-38,  1.7728e-02,  5.5831e-03,  3.5104e-02,  2.1134e-03,\n",
      "         -1.6284e-02, -1.9290e-06,  1.2257e-01,  1.7471e-11, -7.7361e-38,\n",
      "         -1.5921e-02,  6.8082e-03, -4.5697e-04,  2.6586e-03,  7.5222e-03,\n",
      "         -8.0655e-39,  1.1711e-38,  5.9239e-02, -1.1874e-39, -1.3657e-08],\n",
      "        [-4.3380e-03, -4.4778e-01,  2.4068e-01,  9.8765e-02, -2.8208e-18,\n",
      "         -3.7025e-03, -1.4890e-01,  1.4868e-01, -2.9964e-01,  3.7597e-01,\n",
      "          7.6502e-06,  5.8607e-22,  1.1705e-03, -1.8484e-17, -2.0500e-18,\n",
      "         -6.5492e-02,  1.9271e-01,  4.0915e-03,  5.3029e-02,  8.2538e-07,\n",
      "         -1.2521e-03,  4.3852e-01, -1.7733e-03, -5.9442e-04, -9.4374e-09],\n",
      "        [-7.3235e-38, -3.0100e-03, -2.5833e-02, -6.4934e-03,  2.8931e-04,\n",
      "         -2.4287e-38, -3.1613e-03, -3.5774e-03, -3.7587e-02, -1.6522e-02,\n",
      "          2.7246e-02, -6.2576e-09,  1.4643e-02, -9.7554e-11,  2.7270e-38,\n",
      "         -5.4414e-02,  1.2104e-01,  8.4413e-03, -4.0914e-03,  2.7817e-38,\n",
      "         -2.0641e-38, -3.9104e-03, -1.6995e-02,  1.2838e-38,  4.8386e-07],\n",
      "        [ 2.9676e-03, -3.2421e-01,  2.3977e-01, -9.4643e-02,  2.3437e-38,\n",
      "         -7.2952e-04, -2.6871e-01,  1.7297e-02,  2.4719e-01,  4.1346e-01,\n",
      "         -3.0421e-10,  1.5539e-38, -1.7635e-04,  1.5023e-38, -1.2448e-04,\n",
      "         -3.0384e-01, -4.4621e-01, -2.9863e-06,  3.4267e-01,  1.4871e-04,\n",
      "          2.2742e-03,  1.5449e-01, -3.1458e-04,  1.9364e-04, -3.5591e-17],\n",
      "        [ 5.4598e-03,  4.5331e-02,  2.8466e-01,  2.6734e-01, -2.7390e-06,\n",
      "          1.9176e-04,  4.2278e-02, -1.5268e-01,  3.3723e-01,  2.2048e-01,\n",
      "         -6.8848e-03, -1.0373e-12,  1.2077e-02, -1.9339e-16, -3.7497e-04,\n",
      "          2.2492e-01, -3.7119e-02, -1.2329e-02,  4.1532e-01, -6.1068e-04,\n",
      "         -2.6481e-04, -2.2438e-01,  2.3437e-03,  1.8176e-03, -9.1969e-08],\n",
      "        [-1.8077e-38, -1.5648e-02,  8.0023e-02, -5.7840e-02, -2.3657e-04,\n",
      "         -1.8173e-38, -1.0653e-01, -1.7997e-02,  1.3160e-01,  1.5793e-01,\n",
      "          3.6996e-02,  6.8951e-07, -5.4829e-03, -1.6176e-06,  4.5519e-38,\n",
      "         -2.4242e-01,  8.0329e-02, -5.3910e-03, -2.4461e-02, -4.5125e-11,\n",
      "         -7.2796e-38, -1.9203e-03, -1.6472e-03, -2.2962e-38,  9.9894e-05],\n",
      "        [-3.3345e-38, -6.9045e-03, -1.2381e-01, -1.5188e-02,  1.0610e-03,\n",
      "          1.1020e-38, -2.1212e-01, -6.3437e-02, -4.4303e-02, -1.7646e-01,\n",
      "         -1.9543e-02,  2.5112e-07, -3.1443e-02,  3.0861e-07, -1.6537e-38,\n",
      "          2.1755e-01,  2.7118e-01,  1.6719e-02, -5.1832e-02,  2.4808e-38,\n",
      "         -1.1658e-38, -1.2168e-02, -1.6717e-02,  5.6711e-39,  1.1985e-05],\n",
      "        [-2.9091e-03, -5.0707e-01,  3.0421e-01,  3.7943e-01, -2.3020e-38,\n",
      "          7.4146e-03, -4.1487e-01, -3.2615e-01,  2.0014e-01, -2.3010e-01,\n",
      "         -9.5893e-14, -2.0669e-38,  2.3474e-38, -1.7525e-38,  5.4147e-03,\n",
      "         -2.7273e-01, -7.1398e-02, -1.6174e-39, -2.3103e-03, -2.0332e-38,\n",
      "          5.5838e-04,  1.5218e-01,  5.6128e-38,  2.2721e-03,  1.5564e-17],\n",
      "        [ 1.6983e-03,  5.1461e-02, -2.1650e-01, -8.3631e-03,  4.2927e-38,\n",
      "         -3.5278e-03,  6.6786e-02,  2.5205e-01, -1.1122e-01, -2.7096e-02,\n",
      "          3.1925e-38, -1.2700e-39, -1.7716e-05, -1.2885e-39, -1.1515e-37,\n",
      "          2.9539e-01, -1.1334e-01, -3.6105e-38, -3.4589e-02,  3.5230e-33,\n",
      "          1.8342e-38, -1.0749e-01, -1.9355e-04, -1.2739e-04,  1.7609e-17]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 3.5015e-01,  2.1831e-02,  2.6833e-01,  4.7546e-02, -6.9439e-01,\n",
      "          3.9652e-02, -6.8759e-01,  8.3658e-02,  1.6124e-02, -7.2974e-03,\n",
      "          2.3516e-01,  1.4941e-01],\n",
      "        [-7.5252e-02,  1.3615e-02, -8.5603e-01, -1.8480e-02,  2.4093e-02,\n",
      "          2.8493e-02,  6.1000e-01, -9.8153e-02, -2.4663e-03,  2.2054e-02,\n",
      "          7.0129e-01, -4.7619e-02],\n",
      "        [ 1.4678e-01, -1.0374e-01,  2.5384e-01, -2.1515e-02,  8.2039e-02,\n",
      "          3.5231e-02, -2.5090e-01, -1.8914e-01, -9.0049e-03,  2.9179e-03,\n",
      "          2.1413e-01, -4.0142e-01],\n",
      "        [ 3.5387e-01,  2.6932e-02, -5.1585e-01,  1.8643e-01,  4.7916e-01,\n",
      "          4.4709e-03, -7.4187e-01, -4.0637e-01,  3.2274e-02,  8.1148e-02,\n",
      "         -7.9619e-01,  1.4974e-03],\n",
      "        [-6.1612e-01, -6.4947e-02,  3.3641e-02,  5.4327e-03, -8.5367e-02,\n",
      "         -1.2619e-03,  4.2187e-01,  3.7000e-01,  6.4011e-02, -1.6297e-04,\n",
      "         -2.7916e-01, -1.6917e-01],\n",
      "        [ 2.2388e-01, -5.7110e-02,  3.9254e-01,  1.0973e-02,  4.0625e-01,\n",
      "         -3.1666e-05,  6.8161e-01, -4.9898e-01,  1.6207e-17, -1.3047e-06,\n",
      "         -2.0550e-01, -3.7768e-02]], requires_grad=True)\n",
      "Train:  tensor(0.9630)\n",
      "Test:  tensor(0.8917)\n",
      "Cost after epoch 500: 0.063526\n",
      "tensor([[-4.5741e+00, -3.4850e+00, -6.3325e+00, -7.2377e+00, -3.3455e+00,\n",
      "         -1.1209e+01, -6.5686e+00, -9.8567e+00, -8.0652e+00, -6.9604e+00,\n",
      "          2.2497e+00, -4.7007e+00, -9.7592e+00, -1.6777e+00, -9.2720e+00,\n",
      "         -1.2346e+01, -6.4935e+00, -5.1902e+00, -3.6675e+00, -1.0157e+01,\n",
      "         -1.0748e+01, -9.7370e+00, -7.0021e+00, -1.3707e+00],\n",
      "        [-4.5648e+00, -2.7670e+00, -1.1434e+01, -1.0716e+01,  2.5142e+00,\n",
      "         -6.9931e+00,  2.6937e+00, -9.6745e+00,  4.6863e+00,  3.1612e+00,\n",
      "         -9.0277e+00, -7.0244e+00, -6.5553e+00, -7.9616e+00,  8.0346e+00,\n",
      "         -1.7034e+01,  3.3010e+00, -9.2486e+00, -3.9163e+00, -9.5906e+00,\n",
      "         -9.1754e+00, -7.2950e+00, -7.9442e+00, -2.9951e+00],\n",
      "        [ 6.6059e-01,  1.2589e+00, -4.0729e+00,  1.8572e+00, -5.5154e-03,\n",
      "         -5.6155e-01, -1.2802e+00, -2.1557e+00, -2.3262e+00, -2.5173e+00,\n",
      "         -1.4629e+00, -6.8623e+00, -4.2908e+00,  2.1507e-03, -3.8923e+00,\n",
      "         -2.1617e+00, -2.1883e+00, -3.1431e+00,  5.3330e-01, -3.5386e+00,\n",
      "         -2.8163e+00, -1.3831e+00, -3.1672e+00,  6.8313e-01],\n",
      "        [-9.8446e+00, -1.4771e+01, -1.3093e+01, -6.2247e+00, -1.0502e+01,\n",
      "         -1.9267e+01, -1.7484e+01, -4.6557e+00, -9.3812e+00, -1.3025e+01,\n",
      "         -1.4855e+01,  8.1570e+00, -6.4103e+00, -1.6353e+01, -1.2346e+01,\n",
      "         -2.8492e+00, -4.0618e+00, -1.4577e+01, -2.2822e+01, -1.1730e+01,\n",
      "         -5.9095e+00, -9.8325e+00, -8.5842e+00, -1.2158e+01],\n",
      "        [-4.3227e+00, -5.5204e+00,  2.8731e+00, -2.5055e+00, -7.7965e+00,\n",
      "         -2.8758e-01, -5.7733e+00,  2.3174e+00, -4.9146e+00, -4.0855e+00,\n",
      "         -6.7512e+00, -8.2521e+00, -2.2822e+00, -6.0355e+00, -5.1510e+00,\n",
      "         -1.6527e+00, -8.1137e+00,  1.6824e+00, -3.7492e+00, -7.8748e-01,\n",
      "          3.0022e+00,  1.9500e+00, -3.3111e+00, -5.2842e+00],\n",
      "        [-6.1272e+00, -7.0747e+00, -2.8463e+00, -3.4131e+00, -1.1080e+01,\n",
      "          3.9807e+00, -4.8178e+00, -5.6579e+00, -9.3923e+00, -8.8125e+00,\n",
      "         -5.0647e+00, -5.1780e+00,  1.9847e+00, -1.7074e+00, -7.8637e+00,\n",
      "          4.9751e+00, -6.5051e+00, -3.5591e+00, -3.4391e+00,  4.5030e+00,\n",
      "         -4.6426e+00, -3.1857e+00, -9.2027e-01, -9.7909e+00]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[-9.8542e-31, -1.6950e-32,  5.0560e-35,  ...,  1.3059e-37,\n",
      "          2.6985e-37,  2.3621e-37],\n",
      "        [ 4.3108e-02,  3.9941e-02,  2.7531e-02,  ...,  8.2273e-02,\n",
      "          7.3111e-02,  5.8025e-02],\n",
      "        [ 1.0370e-02, -1.6432e-02, -1.1876e-02,  ..., -7.5073e-02,\n",
      "         -2.0277e-02, -1.0604e-02],\n",
      "        ...,\n",
      "        [ 5.5405e-38, -1.1044e-37,  5.4162e-38,  ..., -5.5526e-38,\n",
      "          5.3157e-38,  4.8550e-38],\n",
      "        [-7.9975e-16, -2.7577e-16, -9.6110e-17,  ..., -5.9529e-18,\n",
      "         -3.8272e-19, -6.2741e-20],\n",
      "        [-8.9647e-05, -7.3700e-05, -5.8256e-05,  ..., -9.3405e-05,\n",
      "         -8.9121e-05, -8.3349e-05]], requires_grad=True)\n",
      "tensor([[ 6.1621e-03, -3.0168e-01,  3.9359e-02,  4.0606e-01, -4.4927e-38,\n",
      "         -3.5224e-03, -2.4161e-01,  5.0757e-01, -1.5971e-01,  1.3862e-01,\n",
      "          2.2873e-29, -2.9237e-38, -2.3804e-10,  5.0440e-06,  2.5071e-03,\n",
      "          3.6518e-01, -1.9591e-01, -2.2592e-14, -4.7432e-01,  2.6777e-38,\n",
      "          2.3535e-03, -1.1217e-01, -2.9703e-18, -3.9462e-04, -5.4066e-03],\n",
      "        [-2.2534e-38, -2.8523e-02,  2.4179e-01,  9.6670e-04,  1.4683e-17,\n",
      "         -1.7518e-38, -2.0061e-01,  2.6245e-01,  1.7463e-01, -5.7069e-02,\n",
      "         -9.2334e-07,  6.9395e-26, -3.2441e-06, -2.0163e-08, -3.5761e-04,\n",
      "         -1.7119e-01, -1.2768e-02,  3.0692e-03,  3.5101e-02,  2.7284e-38,\n",
      "         -1.5960e-03,  1.0221e-01,  2.7325e-05, -5.9290e-38,  2.1987e-03],\n",
      "        [-5.7638e-04, -5.8416e-01,  1.7262e-01, -1.3027e-01,  2.2489e-38,\n",
      "          2.7081e-03,  3.2869e-01, -4.1457e-01, -3.9812e-01,  1.1314e-01,\n",
      "          2.1792e-38, -1.9913e-38, -2.4332e-14,  4.5554e-07, -2.3520e-03,\n",
      "          5.3452e-02, -1.2038e-01, -2.5677e-38,  4.0657e-01, -2.8459e-38,\n",
      "          3.1667e-03, -4.6549e-01,  1.9448e-17, -3.0244e-03,  3.1708e-03],\n",
      "        [ 1.0426e-37,  2.9837e-38, -2.8767e-04, -4.1160e-38,  3.2684e-38,\n",
      "         -2.3491e-38,  4.7890e-06,  2.0804e-08,  1.1033e-04,  1.4066e-10,\n",
      "         -2.5410e-07,  1.9481e-35,  4.3885e-03, -4.2019e-38, -7.7361e-38,\n",
      "         -6.4536e-05,  1.3038e-06, -2.3012e-10,  1.8591e-11,  2.0886e-09,\n",
      "         -8.0655e-39,  1.1711e-38,  1.8797e-04, -1.1874e-39,  3.2551e-38],\n",
      "        [-1.6314e-03, -4.8556e-01,  2.4326e-01,  9.5678e-02,  2.1626e-38,\n",
      "         -3.6306e-03, -1.4880e-01,  1.7798e-01, -3.7602e-01,  3.8688e-01,\n",
      "          5.2021e-27, -2.1361e-38,  1.8462e-11, -3.3343e-18,  4.2957e-03,\n",
      "         -6.6715e-02,  2.1927e-01,  3.6552e-11,  5.1506e-02, -8.8753e-36,\n",
      "         -3.4946e-04,  4.8318e-01, -7.6379e-12, -5.8486e-04, -7.9825e-04],\n",
      "        [-7.3235e-38, -8.0944e-05, -1.6544e-02, -4.8943e-03,  3.3019e-18,\n",
      "         -2.4287e-38, -8.6842e-04, -9.5685e-05, -7.7199e-04, -1.0518e-02,\n",
      "          3.0460e-06,  2.5398e-38,  1.6052e-07,  3.1446e-38,  2.7270e-38,\n",
      "         -3.5121e-02,  6.9931e-02,  6.5189e-08, -1.5364e-03,  2.7817e-38,\n",
      "         -2.0641e-38, -1.4384e-03, -1.3839e-07,  1.2838e-38, -2.9374e-38],\n",
      "        [ 1.5470e-03, -3.6044e-01,  2.4231e-01, -9.9127e-02,  2.3437e-38,\n",
      "         -7.1730e-04, -2.7726e-01,  3.3793e-02,  2.6881e-01,  4.2286e-01,\n",
      "         -1.2616e-37,  1.5539e-38, -9.3997e-14, -5.7006e-04, -1.7850e-03,\n",
      "         -3.0740e-01, -4.6996e-01, -7.5531e-33,  3.5073e-01,  4.0741e-20,\n",
      "         -7.9184e-04,  1.6482e-01, -2.4439e-14,  1.9261e-04,  2.8699e-03],\n",
      "        [ 3.6023e-03,  1.1038e-01,  2.8541e-01,  2.6569e-01, -8.9806e-34,\n",
      "          1.8827e-04,  5.0699e-02, -1.9923e-01,  3.9401e-01,  2.1964e-01,\n",
      "         -1.9402e-09,  2.4782e-38,  1.8129e-07, -8.6880e-18, -4.1717e-03,\n",
      "          2.2459e-01, -3.3275e-02, -4.6261e-08,  4.2661e-01, -4.1241e-16,\n",
      "         -2.1483e-03, -2.3293e-01,  3.0403e-11,  1.7999e-03,  2.8525e-03],\n",
      "        [-1.8077e-38, -8.5715e-05,  3.6808e-02, -3.3247e-02, -8.1541e-19,\n",
      "         -1.8173e-38, -9.0396e-03, -6.2379e-07,  1.5034e-02,  6.9932e-02,\n",
      "          1.5383e-05, -2.5473e-38, -1.2238e-08,  1.5962e-35,  4.5519e-38,\n",
      "         -1.0923e-01,  1.9698e-02, -4.1224e-08, -3.3538e-03,  2.2774e-38,\n",
      "         -7.2796e-38, -1.2173e-04, -2.3117e-12, -2.2962e-38,  3.8323e-21],\n",
      "        [-3.3345e-38,  5.4035e-02, -1.0082e-01,  2.1650e-03,  9.9074e-15,\n",
      "          1.1020e-38, -1.2951e-01, -2.7655e-02, -9.1785e-02, -1.5006e-01,\n",
      "         -8.0061e-07, -2.9043e-38, -5.8098e-06, -2.5186e-38, -1.6537e-38,\n",
      "          2.1765e-01,  3.1152e-01,  6.3698e-07, -3.2528e-02,  2.4808e-38,\n",
      "         -1.1658e-38,  2.3020e-02, -1.3228e-07,  5.6711e-39, -3.6194e-03],\n",
      "        [-1.2524e-03, -5.7273e-01,  3.0736e-01,  3.8865e-01, -2.3020e-38,\n",
      "          7.2795e-03, -4.2848e-01, -3.6005e-01,  2.1004e-01, -2.4037e-01,\n",
      "         -1.1752e-37, -2.0669e-38,  2.1389e-18, -7.1220e-05,  2.8019e-03,\n",
      "         -2.7218e-01, -7.2143e-02, -1.6174e-39, -1.0261e-02, -2.0332e-38,\n",
      "          2.7952e-04,  1.4905e-01,  5.6128e-38,  2.2468e-03, -4.7736e-03],\n",
      "        [ 3.9282e-06,  5.2263e-02, -2.2560e-01, -1.4031e-02,  4.2927e-38,\n",
      "         -3.2896e-03,  8.5903e-02,  4.0042e-01, -1.6792e-01, -7.3715e-03,\n",
      "          3.1925e-38, -1.2700e-39,  5.9367e-18, -2.0057e-38, -6.3731e-04,\n",
      "          3.0990e-01, -1.6012e-01, -3.6105e-38, -3.2967e-02, -2.8932e-38,\n",
      "          1.4740e-03, -1.6082e-01, -4.5738e-18, -1.2454e-04, -3.1141e-03]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 3.5985e-01,  6.1820e-02,  2.4215e-01,  4.6772e-04, -7.2047e-01,\n",
      "          3.3316e-05, -6.8918e-01,  8.7924e-02,  2.3217e-07, -5.8066e-02,\n",
      "          2.6317e-01,  2.0237e-01],\n",
      "        [-7.4674e-02,  9.3809e-02, -9.0546e-01, -5.0763e-06,  1.8685e-02,\n",
      "          5.3305e-06,  6.2267e-01, -9.9339e-02, -3.4223e-11, -2.6211e-02,\n",
      "          7.3000e-01, -1.1524e-01],\n",
      "        [ 1.4602e-01, -4.0638e-01,  3.1787e-01, -4.4848e-07,  9.2920e-02,\n",
      "          9.7115e-06, -3.6810e-01, -1.8017e-01, -3.4850e-09,  7.8797e-02,\n",
      "          2.2770e-01, -6.3917e-01],\n",
      "        [ 3.6369e-01, -1.0873e-01, -5.9176e-01,  2.1862e-02,  4.9249e-01,\n",
      "          3.2580e-06, -8.2478e-01, -3.9624e-01,  1.2410e-05,  1.0262e-01,\n",
      "         -8.7067e-01,  1.1186e-02],\n",
      "        [-6.3322e-01, -2.2411e-01,  5.2951e-02,  5.4642e-05, -8.9183e-02,\n",
      "         -9.7372e-12,  4.1374e-01,  3.7778e-01,  1.6084e-04, -7.0555e-02,\n",
      "         -3.3754e-01, -2.5889e-01],\n",
      "        [ 2.2799e-01, -6.6853e-02,  3.9444e-01,  3.6211e-08,  4.0562e-01,\n",
      "         -1.8068e-26,  7.5790e-01, -5.0820e-01, -2.6439e-38, -1.6145e-01,\n",
      "         -2.2175e-01, -2.3036e-02]], requires_grad=True)\n",
      "Train:  tensor(0.9852)\n",
      "Test:  tensor(0.9167)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 600: 0.040633\n",
      "tensor([[ -5.1782,   6.5222,  -5.8754,  -4.0203,   6.7683,  -6.7911,  -2.2074,\n",
      "          -7.5999, -10.0137, -12.0534,  -9.8857,  -6.7061, -12.1795, -11.8561,\n",
      "           5.3928, -13.6094,  -5.7105,  -6.9109, -12.9201,   7.5254,  -6.9869,\n",
      "          -5.4909,  -4.5111,  -3.6520],\n",
      "        [  4.9644, -24.1221, -12.5297,  -9.6450, -19.7332, -12.0261, -10.9047,\n",
      "         -13.9413, -11.4418,  -9.5699,  -9.4683, -15.5092, -10.7453,   5.5904,\n",
      "         -23.3390, -19.7253, -12.5932, -15.5396,  -8.2785, -15.4879, -12.8386,\n",
      "          -7.0788, -21.4942,  -7.6033],\n",
      "        [ -5.9288,  -6.8802,  -5.8168,   1.0903,  -8.0131,  -3.8180,  -8.9292,\n",
      "          -5.4044,   2.7215,  -3.0578,  -5.1188,  -2.3364,  -4.7077,  -7.6455,\n",
      "          -5.9680,  -4.1005,   4.6318,  -9.3479, -11.3609,  -8.9964,  -9.7051,\n",
      "           1.5033,  -3.0371,   0.7412],\n",
      "        [-13.6167,  -8.3091, -14.9901, -16.7621,  -6.5016, -14.0470,   2.2905,\n",
      "          -4.1053,  -8.2286, -10.8729,  -5.2990, -13.6521,  -9.3304, -15.9932,\n",
      "          -4.1864,  -4.1514,  -8.6698,  -9.6017,  -2.8429, -11.3306,  -7.8246,\n",
      "         -15.2352,  -8.7175, -18.8218],\n",
      "        [ -3.3122,  -7.2327,   0.9985,  -2.1149,  -2.1043,   0.9739,  -5.4108,\n",
      "           2.0997,  -2.4693,  -4.9436,  -5.9651,   2.3054,   6.9262,  -2.0987,\n",
      "          -5.2054,  -2.5476,  -4.5794,  -1.3803,  -2.0755,  -6.8277,   1.1180,\n",
      "          -4.3856,   2.9313,  -5.1521],\n",
      "        [-10.8281,  -5.4096,  -1.6654,  -9.2116, -14.1811,  -2.1035,  -7.3146,\n",
      "          -7.1359,  -2.6452,   3.7298,   1.1151,  -5.3575,  -5.6098,  -6.4264,\n",
      "         -10.2983,   5.8684,  -9.6357,   2.0165,   2.5785,  -8.0249,  -1.9807,\n",
      "          -8.0944,  -5.9546,  -5.6328]], grad_fn=<AddBackward0>)\n",
      "tensor([[ 3.2760e-39, -8.0536e-39, -1.3736e-38,  ..., -3.4298e-38,\n",
      "         -3.3268e-38, -2.3765e-38],\n",
      "        [ 4.5221e-02,  4.2967e-02,  3.2482e-02,  ...,  8.4710e-02,\n",
      "          7.3218e-02,  5.4020e-02],\n",
      "        [ 8.9859e-03, -1.8600e-02, -1.7621e-02,  ..., -8.2299e-02,\n",
      "         -2.2915e-02, -6.9396e-03],\n",
      "        ...,\n",
      "        [-1.0945e-12, -4.5774e-13, -1.2352e-13,  ..., -5.9681e-16,\n",
      "         -6.3169e-17, -1.2718e-17],\n",
      "        [-4.9931e-05, -4.1570e-05, -3.3966e-05,  ..., -2.8004e-05,\n",
      "         -2.3232e-05, -2.0649e-05],\n",
      "        [-1.6887e-27, -3.2595e-29, -9.3474e-32,  ..., -3.4645e-29,\n",
      "         -5.5142e-30, -6.2623e-31]], requires_grad=True)\n",
      "tensor([[ 9.3389e-04, -3.4461e-01,  3.6865e-02,  4.1118e-01, -1.9278e-03,\n",
      "         -1.4280e-03, -2.5024e-01,  5.6315e-01, -2.0059e-01,  1.3423e-01,\n",
      "          1.2011e-03, -8.2364e-38, -2.3177e-03, -1.7939e-03,  3.0822e-03,\n",
      "          3.6792e-01, -2.0312e-01,  8.1862e-04, -4.8914e-01, -1.1569e-04,\n",
      "          5.9473e-04, -1.2106e-01,  1.1771e-04, -1.9915e-03, -4.5133e-03],\n",
      "        [-2.2534e-38, -4.0880e-02,  2.4905e-01,  7.4924e-03,  3.1407e-03,\n",
      "         -3.1469e-03, -2.4808e-01,  3.7107e-01,  2.6321e-01, -5.6601e-02,\n",
      "         -1.0468e-03, -2.4282e-38, -1.3331e-03,  2.2209e-03,  2.5288e-03,\n",
      "         -1.7765e-01, -2.8428e-02,  2.4735e-03,  2.9608e-02,  3.3119e-03,\n",
      "         -1.4212e-05,  1.2257e-01,  4.0214e-03, -2.2693e-03,  1.4154e-03],\n",
      "        [-3.7972e-10, -6.4917e-01,  1.7425e-01, -1.3059e-01, -4.3614e-03,\n",
      "          2.7064e-03,  3.4136e-01, -4.3496e-01, -4.3531e-01,  1.1328e-01,\n",
      "          1.2763e-03,  1.0588e-37,  1.2678e-03,  1.2380e-03, -8.3172e-04,\n",
      "          5.5591e-02, -1.2666e-01, -2.9684e-03,  4.0794e-01, -5.5148e-04,\n",
      "          8.9405e-04, -4.8734e-01,  1.6063e-03, -3.7630e-03,  2.4807e-03],\n",
      "        [ 1.0426e-37, -6.9285e-38, -3.2076e-14,  7.5954e-40,  3.2684e-38,\n",
      "         -2.3491e-38,  2.7096e-21,  1.0266e-33, -5.3024e-16,  5.1243e-38,\n",
      "          2.0674e-38,  2.6616e-38,  5.2818e-11, -4.2019e-38, -7.7361e-38,\n",
      "         -1.0123e-14,  2.6932e-22,  5.5392e-38,  5.2213e-38, -2.0998e-38,\n",
      "         -8.0655e-39,  9.5081e-38,  2.4136e-19, -1.1874e-39,  3.2551e-38],\n",
      "        [-7.1945e-06, -5.2093e-01,  2.4619e-01,  9.3843e-02,  3.4728e-03,\n",
      "         -1.7431e-03, -1.5122e-01,  2.1466e-01, -4.4082e-01,  3.9898e-01,\n",
      "         -3.2387e-03,  1.3373e-38, -1.0948e-03,  1.9346e-03,  1.5246e-03,\n",
      "         -6.6921e-02,  2.4409e-01, -2.1910e-03,  5.0721e-02,  2.7705e-03,\n",
      "         -1.3907e-05,  5.2929e-01, -8.3511e-04,  4.2225e-03, -6.7584e-04],\n",
      "        [-7.3235e-38, -7.0518e-14, -1.4324e-03, -1.0346e-03, -2.7398e-38,\n",
      "         -2.4287e-38, -6.5094e-07, -8.0679e-14, -1.4125e-13, -8.7681e-04,\n",
      "          3.8720e-33,  2.5398e-38, -2.8368e-38,  3.1446e-38,  2.7270e-38,\n",
      "         -3.2604e-03,  4.2163e-03, -3.5437e-38, -6.7219e-06,  2.7817e-38,\n",
      "         -2.0641e-38, -5.6020e-06,  2.1645e-38,  1.2838e-38, -2.9374e-38],\n",
      "        [ 4.2372e-05, -3.8045e-01,  2.4429e-01, -1.0456e-01,  1.7051e-03,\n",
      "          5.6432e-04, -2.8202e-01,  3.6817e-02,  2.8342e-01,  4.3136e-01,\n",
      "         -4.1562e-03,  1.4899e-38,  1.7343e-03,  3.8246e-03, -3.4906e-03,\n",
      "         -3.1150e-01, -4.8675e-01,  1.9678e-04,  3.5950e-01,  1.9571e-03,\n",
      "         -2.2392e-04,  1.7418e-01,  2.7186e-03, -1.4953e-04,  2.5784e-03],\n",
      "        [ 3.6496e-04,  1.7623e-01,  2.8638e-01,  2.6369e-01,  2.1736e-03,\n",
      "         -1.9665e-03,  5.9939e-02, -2.4760e-01,  4.4885e-01,  2.1935e-01,\n",
      "          8.5797e-04, -6.7443e-38,  2.0628e-03, -2.4604e-04, -2.6012e-03,\n",
      "          2.2431e-01, -2.9682e-02,  1.6720e-03,  4.3772e-01, -2.2486e-03,\n",
      "         -3.5993e-04, -2.4255e-01, -7.4713e-04,  8.5009e-04,  2.4341e-03],\n",
      "        [-1.8077e-38, -3.1382e-18,  5.8114e-04, -1.6537e-03,  1.9517e-38,\n",
      "         -1.8173e-38, -1.3943e-08,  2.6422e-38,  1.6511e-07,  1.3313e-03,\n",
      "          5.2380e-27, -2.5473e-38,  3.1866e-38,  2.0279e-38,  4.5519e-38,\n",
      "         -3.8951e-03,  9.9154e-06,  3.9145e-38, -4.8451e-08,  2.2774e-38,\n",
      "         -7.2796e-38, -1.8621e-11,  2.9329e-38, -2.2962e-38, -2.7259e-38],\n",
      "        [-3.3345e-38,  9.3145e-02, -1.0216e-01,  6.2307e-03,  2.2573e-03,\n",
      "         -1.6463e-03, -1.1420e-01, -6.1508e-02, -1.6115e-01, -1.5622e-01,\n",
      "          9.5436e-04, -2.9043e-38, -5.3196e-04, -2.9414e-03,  2.6048e-03,\n",
      "          2.2097e-01,  3.7416e-01, -1.1700e-03, -3.9828e-02, -2.6718e-03,\n",
      "         -1.2465e-37,  4.3986e-02, -4.3531e-03,  4.1454e-03, -1.7139e-04],\n",
      "        [-1.1785e-05, -6.4689e-01,  3.0874e-01,  3.9764e-01, -9.1293e-04,\n",
      "          8.2606e-03, -4.4448e-01, -3.7702e-01,  2.1687e-01, -2.4966e-01,\n",
      "          1.8571e-03, -2.0240e-38, -2.6045e-03, -2.7988e-03,  1.5300e-03,\n",
      "         -2.7072e-01, -7.7201e-02, -9.8250e-04, -1.9843e-02,  1.2296e-03,\n",
      "          5.4119e-07,  1.4785e-01,  4.4601e-03,  4.0942e-03, -3.4915e-03],\n",
      "        [ 4.1247e-22,  8.8897e-02, -2.3251e-01, -1.7173e-02, -9.7115e-04,\n",
      "         -1.8543e-03,  9.8004e-02,  5.1326e-01, -2.0076e-01,  3.4506e-03,\n",
      "         -3.2169e-03, -4.6932e-40, -1.9008e-03,  2.7503e-05,  2.1696e-03,\n",
      "          3.1893e-01, -1.8551e-01,  2.6656e-03, -3.1205e-02,  4.6110e-04,\n",
      "          7.4485e-06, -2.0894e-01,  2.5309e-04, -3.2906e-03, -2.1655e-03]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 3.6617e-01,  1.7732e-01,  2.2410e-01,  3.0683e-15, -7.4801e-01,\n",
      "          2.8009e-24, -7.0323e-01,  9.1372e-02, -2.2619e-38, -1.1555e-01,\n",
      "          2.8606e-01,  2.3396e-01],\n",
      "        [-7.5974e-02,  1.2443e-01, -9.4677e-01, -1.8138e-23,  1.2188e-02,\n",
      "          1.5944e-30,  6.2612e-01, -1.0210e-01,  3.1890e-38, -4.5299e-02,\n",
      "          7.5515e-01, -1.7756e-01],\n",
      "        [ 1.5045e-01, -6.1318e-01,  3.5523e-01,  5.0546e-32,  1.0112e-01,\n",
      "          1.0350e-28, -4.4001e-01, -1.7532e-01,  2.5826e-38,  1.5437e-01,\n",
      "          2.4404e-01, -8.1183e-01],\n",
      "        [ 3.7474e-01, -1.7174e-01, -6.6662e-01,  5.4877e-07,  5.0669e-01,\n",
      "          7.7328e-26, -8.8360e-01, -3.8675e-01,  2.1331e-27,  1.7156e-01,\n",
      "         -9.4058e-01,  2.8204e-02],\n",
      "        [-6.4984e-01, -3.2364e-01,  6.5485e-02,  3.7016e-14, -9.3081e-02,\n",
      "          3.9226e-38,  4.1910e-01,  3.8544e-01,  4.4610e-20, -1.2339e-01,\n",
      "         -4.0626e-01, -3.2311e-01],\n",
      "        [ 2.3158e-01, -2.3691e-02,  4.0140e-01,  1.0995e-36,  4.0872e-01,\n",
      "          9.2844e-39,  8.1254e-01, -5.1486e-01, -2.6439e-38, -3.0989e-01,\n",
      "         -2.3821e-01, -1.4028e-02]], requires_grad=True)\n",
      "Train:  tensor(0.9954)\n",
      "Test:  tensor(0.8833)\n",
      "Cost after epoch 700: 0.023739\n",
      "tensor([[-6.1697e+00,  4.4500e+00, -8.8674e+00, -1.4321e+01, -7.0695e+00,\n",
      "         -5.4485e+00, -1.0026e+01, -2.7274e+00, -3.1622e+00, -6.0935e+00,\n",
      "         -7.5808e+00, -5.7291e+00, -9.2260e+00,  5.0293e+00, -6.3027e+00,\n",
      "         -1.6122e+01, -5.3683e+00, -1.3621e+01, -7.6784e+00, -1.6261e+01,\n",
      "         -7.4581e+00, -1.1534e+01,  9.3418e+00, -9.1429e+00],\n",
      "        [-9.2174e+00, -4.2301e+00, -9.5939e+00, -4.1813e+00, -1.8211e+01,\n",
      "         -6.9819e+00, -1.6352e+01, -1.3903e+01, -1.1089e+01, -3.1952e+00,\n",
      "          5.6651e+00, -1.4364e+01, -1.1572e+01, -1.2667e+01,  7.3460e+00,\n",
      "         -3.4267e+00, -2.0258e+01, -6.8117e+00, -1.3921e+01, -8.6968e+00,\n",
      "         -4.9992e+00, -9.2088e+00, -2.8653e+01, -3.0680e+00],\n",
      "        [ 2.4752e+00, -1.9071e+00, -2.6632e+00, -1.0597e+01, -4.4357e+00,\n",
      "         -7.4637e-01, -9.9011e+00,  1.8206e+00, -1.1196e+01,  4.4051e+00,\n",
      "         -3.8211e+00,  4.3914e+00,  4.4308e+00, -6.1984e+00, -3.9545e+00,\n",
      "         -6.0307e+00, -2.4663e-02, -4.1330e+00, -6.1330e+00, -6.5065e+00,\n",
      "          4.5912e+00, -4.1572e+00, -8.9005e+00,  4.4710e+00],\n",
      "        [-7.9089e+00, -1.0603e+01, -1.6349e+01, -8.5473e+00,  2.0527e+00,\n",
      "         -1.0572e+01, -3.2940e+00, -1.7359e+01,  1.1358e+01, -1.3239e+01,\n",
      "         -1.1871e+01, -1.5355e+01, -1.3080e+01, -1.4024e+01, -1.5498e+01,\n",
      "         -7.4684e+00,  2.2328e+00, -1.3669e+01, -1.7751e+01, -7.5341e+00,\n",
      "         -1.6433e+01, -8.1628e+00, -1.0619e+01, -1.5286e+01],\n",
      "        [-3.2468e+00, -1.0385e+01,  2.2472e+00, -6.6986e+00, -2.9351e+00,\n",
      "          1.5009e+00, -4.7244e+00, -5.7109e+00, -1.0529e+01, -9.6232e+00,\n",
      "         -6.9850e+00, -5.0542e+00, -3.1640e+00, -1.0783e+01, -8.9257e+00,\n",
      "          2.4722e+00, -6.8900e+00,  2.1950e+00,  4.7240e+00, -1.0102e+00,\n",
      "         -8.2607e+00, -7.2015e+00, -3.2222e+00, -8.7200e+00],\n",
      "        [-1.2195e+01, -1.7952e+01, -4.5624e+00,  5.3657e+00, -1.0294e+01,\n",
      "         -1.5194e+01,  2.9219e+00, -8.0399e+00, -1.0915e+01, -1.1687e+01,\n",
      "         -1.5326e+01, -8.2421e+00, -8.3999e+00, -6.5707e+00, -1.3488e+01,\n",
      "         -4.1571e+00, -7.9391e+00, -2.8495e+00, -4.5794e+00,  1.3345e+00,\n",
      "         -6.0096e+00,  8.5402e-01, -1.3495e+01, -8.2393e+00]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[ 3.2760e-39, -8.0536e-39, -1.3736e-38,  ..., -3.4298e-38,\n",
      "         -3.3268e-38, -2.3765e-38],\n",
      "        [ 4.6429e-02,  4.5056e-02,  3.6084e-02,  ...,  8.6929e-02,\n",
      "          7.3612e-02,  5.1237e-02],\n",
      "        [ 8.2605e-03, -1.9945e-02, -2.1940e-02,  ..., -8.8021e-02,\n",
      "         -2.5483e-02, -4.9041e-03],\n",
      "        ...,\n",
      "        [ 2.5974e-39, -9.5866e-39, -1.6834e-38,  ..., -1.2619e-37,\n",
      "         -8.7567e-38, -6.2748e-38],\n",
      "        [-4.1708e-23, -3.9822e-24, -2.0101e-25,  ..., -4.2102e-27,\n",
      "         -1.2756e-28, -9.1854e-30],\n",
      "        [-4.6761e-38, -7.5754e-38, -1.0843e-37,  ..., -9.8576e-38,\n",
      "         -1.1355e-37, -1.3581e-37]], requires_grad=True)\n",
      "tensor([[ 2.3404e-08, -3.8657e-01,  3.4524e-02,  4.1539e-01, -1.0091e-03,\n",
      "         -1.4198e-03, -2.5669e-01,  6.1395e-01, -2.4268e-01,  1.2998e-01,\n",
      "         -1.7414e-03, -8.2364e-38,  2.1254e-03, -9.9768e-04,  2.5486e-03,\n",
      "          3.7031e-01, -2.0894e-01,  8.9007e-06, -5.0227e-01, -1.0892e-04,\n",
      "         -1.4691e-03, -1.2988e-01,  1.0274e-04, -1.9644e-03, -1.6745e-03],\n",
      "        [-2.2534e-38, -4.9889e-02,  2.5455e-01,  1.0391e-02,  3.0956e-05,\n",
      "         -3.1106e-03, -2.8111e-01,  4.5809e-01,  3.2507e-01, -5.8233e-02,\n",
      "         -1.5865e-04, -2.4282e-38, -2.9488e-03,  7.5318e-03,  1.2093e-03,\n",
      "         -1.8329e-01, -4.5349e-02,  7.8137e-04,  2.5436e-02,  3.0994e-03,\n",
      "          1.1009e-03,  1.3259e-01,  2.8767e-03, -2.2357e-03,  1.2509e-04],\n",
      "        [ 5.4138e-38, -7.1253e-01,  1.7557e-01, -1.3115e-01, -4.7595e-03,\n",
      "          2.6802e-03,  3.5568e-01, -4.4945e-01, -4.7204e-01,  1.1313e-01,\n",
      "          3.2170e-04,  1.0588e-37,  3.0790e-03,  2.0994e-03, -6.3128e-04,\n",
      "          5.7930e-02, -1.3552e-01, -1.3516e-03,  4.0902e-01, -5.2637e-04,\n",
      "         -1.0558e-03, -5.1003e-01,  1.4005e-03, -3.7269e-03,  6.4388e-04],\n",
      "        [ 1.0426e-37,  7.6738e-05, -1.3700e-03, -1.6080e-03,  3.2684e-38,\n",
      "         -2.3491e-38, -9.2319e-04, -2.1566e-04, -1.4089e-04, -8.7890e-04,\n",
      "          2.0674e-38,  2.6616e-38, -2.5053e-38, -4.2019e-38, -7.7361e-38,\n",
      "         -1.5189e-03, -8.6440e-04,  5.5392e-38, -6.5935e-04, -2.0998e-38,\n",
      "         -5.0590e-38, -2.5579e-04, -2.2633e-38, -1.1874e-39,  3.2551e-38],\n",
      "        [-6.1179e-20, -5.5510e-01,  2.4948e-01,  9.3028e-02,  3.7941e-03,\n",
      "         -1.7287e-03, -1.5484e-01,  2.5894e-01, -4.9675e-01,  4.1078e-01,\n",
      "         -3.2145e-03,  1.3373e-38,  2.8765e-03, -1.8925e-03,  1.2655e-03,\n",
      "         -6.6619e-02,  2.6928e-01, -4.8875e-04,  5.0320e-02,  2.4436e-03,\n",
      "         -3.2608e-04,  5.7444e-01, -7.1363e-04,  4.1421e-03, -2.7101e-04],\n",
      "        [-7.3235e-38,  6.2634e-38, -1.4003e-09, -1.7490e-07, -2.7398e-38,\n",
      "         -2.4287e-38, -1.9119e-26,  5.9367e-38,  3.5325e-38, -6.7993e-10,\n",
      "         -2.4657e-38,  2.5398e-38, -2.8368e-38,  3.1446e-38,  2.7270e-38,\n",
      "         -5.0501e-09,  6.3488e-10, -3.5437e-38, -5.4187e-20,  2.7817e-38,\n",
      "         -2.0641e-38, -2.0710e-20,  2.1645e-38,  1.2838e-38, -2.9374e-38],\n",
      "        [ 4.1280e-14, -3.9366e-01,  2.4576e-01, -1.0988e-01,  1.3765e-03,\n",
      "          5.6145e-04, -2.8530e-01,  3.1258e-02,  2.9416e-01,  4.3939e-01,\n",
      "         -1.4256e-03,  1.4899e-38, -2.6660e-03,  2.5905e-03, -3.0799e-03,\n",
      "         -3.1524e-01, -5.0414e-01,  3.5580e-05,  3.6777e-01,  1.8780e-03,\n",
      "         -1.0497e-03,  1.7818e-01,  2.4019e-03, -1.4825e-04,  1.4328e-03],\n",
      "        [ 8.8245e-10,  2.3985e-01,  2.8748e-01,  2.6191e-01,  6.5418e-04,\n",
      "         -1.9540e-03,  6.8087e-02, -2.9632e-01,  4.9983e-01,  2.1954e-01,\n",
      "          3.3385e-03, -6.7443e-38, -2.5203e-03,  1.5249e-04, -2.0408e-03,\n",
      "          2.2403e-01, -2.4647e-02,  2.5080e-04,  4.4802e-01, -2.0897e-03,\n",
      "          1.1575e-03, -2.4910e-01, -6.4660e-04,  8.3641e-04,  1.0191e-03],\n",
      "        [-1.8077e-38,  4.3846e-38,  2.0660e-14, -6.3771e-11,  1.9517e-38,\n",
      "         -1.8173e-38,  2.2257e-38,  2.6422e-38, -3.0481e-38,  1.9417e-13,\n",
      "         -2.1794e-38, -2.5473e-38,  3.1866e-38,  2.0279e-38,  4.5519e-38,\n",
      "         -3.9193e-11,  1.3784e-26,  3.9145e-38,  4.0645e-38,  2.2774e-38,\n",
      "         -7.2796e-38,  6.7921e-38,  2.9329e-38, -2.2962e-38, -2.7259e-38],\n",
      "        [-3.3345e-38,  1.2153e-01, -9.9868e-02,  1.4866e-02,  1.9571e-03,\n",
      "         -1.6144e-03, -1.0451e-01, -7.5322e-02, -2.0119e-01, -1.5741e-01,\n",
      "         -1.1140e-03, -2.9043e-38, -3.6994e-05, -2.3492e-03,  2.1241e-05,\n",
      "          2.2870e-01,  4.2423e-01, -2.9402e-05, -4.3584e-02, -2.2750e-03,\n",
      "          7.7661e-04,  6.0775e-02, -2.3616e-03,  3.9897e-03, -4.6053e-12],\n",
      "        [-1.4254e-17, -7.1902e-01,  3.0972e-01,  4.0615e-01, -5.8627e-04,\n",
      "          8.1865e-03, -4.6094e-01, -3.8206e-01,  2.2102e-01, -2.5814e-01,\n",
      "         -1.0538e-03, -2.0240e-38,  9.9694e-04, -3.6379e-03,  1.1747e-03,\n",
      "         -2.6919e-01, -8.1859e-02, -6.8998e-06, -2.9742e-02,  1.1641e-03,\n",
      "          9.9124e-04,  1.4705e-01,  3.8951e-03,  3.9995e-03, -6.2524e-04],\n",
      "        [-6.3484e-38,  1.2623e-01, -2.3939e-01, -2.1278e-02, -7.2278e-04,\n",
      "         -1.8360e-03,  1.0429e-01,  6.1588e-01, -2.2565e-01,  1.0680e-02,\n",
      "         -4.4235e-03, -4.6932e-40, -2.9427e-03,  3.9931e-03,  1.6892e-03,\n",
      "          3.2360e-01, -2.0798e-01,  1.3695e-03, -3.1318e-02,  4.3261e-04,\n",
      "          1.2498e-03, -2.4874e-01,  1.8518e-04, -3.2304e-03, -2.9336e-04]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 3.7048e-01,  2.5932e-01,  2.1833e-01, -9.0039e-04, -7.7660e-01,\n",
      "         -2.5066e-38, -7.2340e-01,  9.4419e-02, -2.2619e-38, -1.6989e-01,\n",
      "          3.0363e-01,  2.5632e-01],\n",
      "        [-7.7313e-02,  1.4119e-01, -9.8763e-01,  1.0046e-03,  4.0264e-03,\n",
      "         -2.0815e-38,  6.2559e-01, -1.0553e-01,  3.1890e-38, -6.4008e-02,\n",
      "          7.7973e-01, -2.3186e-01],\n",
      "        [ 1.5731e-01, -7.8082e-01,  3.7980e-01,  1.2641e-04,  1.0768e-01,\n",
      "         -2.1464e-38, -4.9198e-01, -1.7270e-01,  2.5826e-38,  2.0724e-01,\n",
      "          2.6541e-01, -9.5663e-01],\n",
      "        [ 3.8580e-01, -2.0782e-01, -7.3320e-01,  2.2386e-03,  5.2250e-01,\n",
      "         -4.7171e-38, -9.2626e-01, -3.7725e-01, -2.5825e-38,  2.1558e-01,\n",
      "         -9.9499e-01,  4.2615e-02],\n",
      "        [-6.6527e-01, -3.9208e-01,  7.3452e-02,  1.4474e-03, -9.7161e-02,\n",
      "          3.9226e-38,  4.3117e-01,  3.9277e-01, -1.8227e-38, -1.6693e-01,\n",
      "         -4.7386e-01, -3.7298e-01],\n",
      "        [ 2.3567e-01,  5.6243e-03,  4.1643e-01, -3.5063e-04,  4.1302e-01,\n",
      "          9.2844e-39,  8.5655e-01, -5.2073e-01, -2.6439e-38, -4.2351e-01,\n",
      "         -2.5325e-01, -3.0067e-03]], requires_grad=True)\n",
      "Train:  tensor(0.9972)\n",
      "Test:  tensor(0.8833)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 800: nan\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[ 3.2760e-39, -8.0536e-39, -1.3736e-38,  ..., -3.4298e-38,\n",
      "         -3.3268e-38, -2.3765e-38],\n",
      "        [        nan,         nan,         nan,  ...,         nan,\n",
      "                 nan,         nan],\n",
      "        [        nan,         nan,         nan,  ...,         nan,\n",
      "                 nan,         nan],\n",
      "        ...,\n",
      "        [ 2.5974e-39, -9.5866e-39, -1.6834e-38,  ..., -1.2619e-37,\n",
      "         -8.7567e-38, -6.2748e-38],\n",
      "        [-3.6191e-06, -2.9915e-06, -2.7295e-06,  ..., -6.5866e-07,\n",
      "         -4.2651e-07, -2.9385e-07],\n",
      "        [-4.6761e-38, -7.5754e-38, -1.0843e-37,  ..., -9.8576e-38,\n",
      "         -1.1355e-37, -1.3581e-37]], requires_grad=True)\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan]], requires_grad=True)\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       requires_grad=True)\n",
      "Train:  tensor(0.1667)\n",
      "Test:  tensor(0.1667)\n",
      "Cost after epoch 900: nan\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[ 3.2760e-39, -8.0536e-39, -1.3736e-38,  ..., -3.4298e-38,\n",
      "         -3.3268e-38, -2.3765e-38],\n",
      "        [        nan,         nan,         nan,  ...,         nan,\n",
      "                 nan,         nan],\n",
      "        [        nan,         nan,         nan,  ...,         nan,\n",
      "                 nan,         nan],\n",
      "        ...,\n",
      "        [ 2.5974e-39, -9.5866e-39, -1.6834e-38,  ..., -1.2619e-37,\n",
      "         -8.7567e-38, -6.2748e-38],\n",
      "        [-5.1756e-25, -5.2422e-26, -3.3801e-27,  ..., -7.3917e-32,\n",
      "         -4.7948e-35,  6.8660e-37],\n",
      "        [-4.6761e-38, -7.5754e-38, -1.0843e-37,  ..., -9.8576e-38,\n",
      "         -1.1355e-37, -1.3581e-37]], requires_grad=True)\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan]], requires_grad=True)\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       requires_grad=True)\n",
      "Train:  tensor(0.1667)\n",
      "Test:  tensor(0.1667)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Test:  tensor(0.1667)\n",
      "Test:  tensor(0.1667)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, X_test, Y_test, num_epochs=1000, learning_rate=0.0001, weight_decay=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12288, 1080)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(forward_propagation(X_test, parameters), dim = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SKxhuoN2T12_"
   },
   "source": [
    "### 2.6 - Building the model\n",
    "\n",
    "Now, you will bring it all together! \n",
    "\n",
    "**Exercise:** Implement the model. You will be calling the functions you had previously implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ogOoTX2CT13E"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table> \n",
    "    <tr> \n",
    "        <td>\n",
    "            **Train Accuracy**\n",
    "        </td>\n",
    "        <td>\n",
    "        0.999074\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr> \n",
    "        <td>\n",
    "            **Test Accuracy**\n",
    "        </td>\n",
    "        <td>\n",
    "        0.716667\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "Amazing, your algorithm can recognize a sign representing a figure between 0 and 5 with 71.7% accuracy.\n",
    "\n",
    "**Insights**:\n",
    "- Your model seems big enough to fit the training set well. However, given the difference between train and test accuracy, you could try to add L2 or dropout regularization to reduce overfitting. \n",
    "- Think about the session as a block of code to train the model. Each time you run the session on a minibatch, it trains the parameters. In total you have run the session a large number of times (1500 epochs) until you obtained well trained parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cka8pF8BT13E"
   },
   "source": [
    "### 2.7 - Test with your own image (optional / ungraded exercise)\n",
    "\n",
    "Congratulations on finishing this assignment. You can now take a picture of your hand and see the output of your model. To do that:\n",
    "    1. Click on \"File\" in the upper bar of this notebook, then click \"Open\" to go on your Coursera Hub.\n",
    "    2. Add your image to this Jupyter Notebook's directory, in the \"images\" folder\n",
    "    3. Write your image's name in the following code\n",
    "    4. Run the code and check if the algorithm is right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EJ8Aft1CT13F",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "## START CODE HERE ## (PUT YOUR IMAGE NAME) \n",
    "my_image = \"thumbs_up.jpg\"\n",
    "## END CODE HERE ##\n",
    "\n",
    "# We preprocess your image to fit your algorithm.\n",
    "fname = \"images/\" + my_image\n",
    "image = np.array(ndimage.imread(fname, flatten=False))\n",
    "image = image/255.\n",
    "my_image = scipy.misc.imresize(image, size=(64,64)).reshape((1, 64*64*3)).T\n",
    "my_image_prediction = predict(my_image, parameters)\n",
    "\n",
    "plt.imshow(image)\n",
    "print(\"Your algorithm predicts: y = \" + str(np.squeeze(my_image_prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Q5jJuAqT13G"
   },
   "source": [
    "You indeed deserved a \"thumbs-up\" although as you can see the algorithm seems to classify it incorrectly. The reason is that the training set doesn't contain any \"thumbs-up\", so the model doesn't know how to deal with it! We call that a \"mismatched data distribution\" and it is one of the various of the next course on \"Structuring Machine Learning Projects\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMY1FYvOT13H"
   },
   "source": [
    "<font color='blue'>\n",
    "**What you should remember**:\n",
    "- Tensorflow is a programming framework used in deep learning\n",
    "- The two main object classes in tensorflow are Tensors and Operators. \n",
    "- When you code in tensorflow you have to take the following steps:\n",
    "    - Create a graph containing Tensors (Variables, Placeholders ...) and Operations (tf.matmul, tf.add, ...)\n",
    "    - Create a session\n",
    "    - Initialize the session\n",
    "    - Run the session to execute the graph\n",
    "- You can execute the graph multiple times as you've seen in model()\n",
    "- The backpropagation and optimization is automatically done when running the session on the \"optimizer\" object."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "BFd89",
   "launcher_item_id": "AH2rK"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
